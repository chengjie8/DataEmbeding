[
  {
    "title": "Home ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/",
    "html": "Virtual Regional Office (VRO) Overview\nVRO Architecture Diagram\nVRO Team working docs\nCulture and Norms\nTeam Processes\nVRO Engineer Onboarding\nActive efforts\nBIE Kafka Client\nBIE Contention Events User Guide\nVA.gov Data Visibility Initiative\nProblem Overview\nWelcome VRO Developers!\nSoftware Conventions\nVRO RabbitMQ Strategy\nLHDI's Boilerplate Instructions\nLocal Setup\nJetbrains SpringBoot Run Configuration Setup\nCode structure\nRouting API requests\nApache Camel defines processing workflows\nConfiguration settings\nDomain Applications in VRO\nDocker Compose\nDocker containers\nDevelopment process\nGradle\nPull Requests guidelines\nChange Management Plan\nCI CD Workflows\nDeploying VRO\nContainer Image Versions\nAPI Gateway\nExternal APIs to interact with other systems\nBIP APIs\nLighthouse APIs\nBGS API\nBIE Kafka Event Stream\nVRO Database\nTesting\nTesting using Swagger UI\nDevelopment environments\nEnd to End Tests\nMock Services\nPartner Teams\nNew Domain Setup\nPartner Team Deploy Process\nWelcome Contention Classification Developers!\n\nUpdating Contention Classification DC Lookup Table üìã\n\nWelcome Employee Experience Developers!\nWelcome DevOps!\nDeploying VRO\nLightkeeper tool\nKubernetes clusters\nHelm Charts\nDeploy to Prod\nQuick Deploy Instructions\nGithub Actions\nMachine User Account\nTokens and Secrets\nVRO Secrets\nSecrets Vault\nMaintenance\nDataDog monitoring\nDependabot\nWelcome Q/A!\nVRO Test Cases\nDeeper topics\nLighthouse DI Documentation repo - including diagrams\nDive into RabbitMQ/Microservice reliability\nSupport Model\nSupport Model ( Draft )"
  },
  {
    "title": "VRO v1 Roadmap ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-v1-Roadmap",
    "html": "Task Dependency diagram for Version 1.0 of VRO \n\nLegend:\n\ngray box: completed task ‚úÖ\nred-outlined box: in-progress task üèÉ\nlight-green box: low priority task that could be removed from version 1.0 scope if needed\n\nTasks for Version 1.0 of VRO (only for asthma and hypertension; targeting Aug 31st to have VRO in operation, if not sooner)\n\n‚úÖ setup: Set up RabbitMQ container, Camel routes, and deploy to LHDI\n‚úÖ version 0.1\n‚úÖ ruby_pdf_generator Service: Ruby PDF generator (given all pdf contents as request parameters)\n‚úÖ generate_summary_pdf_Endpoint: Add generate_summary_pdf endpoint to API\n‚úÖ ruby_assess_data Service: Ruby assess fast-tracking eligibility given all health data\n‚úÖ assess_data_Endpoint: Add assess-data-for-fast-track endpoint to API (given all health data)\nüèÉ get_ATO: Get ATO for deployment into prod\n‚úÖ add_authentication: Add authentication to API endpoints; using simple API-key for now\ndb\n‚úÖ save_claims_to_db: Save claims processing to DB. See LHDI's persistent volumes.\nclaim_stats_Endpoint (low priority): Add claim_stats endpoint to API for reporting and monitoring. Other metrics (from Zach):\nhow many claims are we evaluating programmatically for sufficient evidence?\nhow many of those claims are deemed to have sufficient evidence?\nhow many claims deemed to have insufficient evidence lead to an auto-exam?\nhow many claims that are deemed to have sufficient evidence lead to: a) immediate rating, and b) not an immediate rating (e.g., deferral, manual exam)?\n‚úÖ devops\nurl_for_vro: Add DNS entry for a url to VRO for all relevant envs; LHDI DNS and routing\ndeploy_to_non_prod_envs: Deploy VRO to staging/preprod VA env\ntest_in_non_prod_envs: Test VRO to staging/preprod VA env\ndeploy_to_prod: Deploy to LHDI's Prod environment (via GH Action or ArgoCD)\nOBE use_ver0.1_endpoints: Have RRD (in vets-api) call 2 endpoints of VRO version 0.1\n‚úÖ assess_claim\n‚úÖ lh_health_api_client: LH Patient (Veteran) Health API (FHIR) client. See existing Ruby code VeteransHealth::Client.\n‚úÖ assess_claim_service: Assess fast-tracking eligibility given claim (VRO must query for health data). See existing Ruby code (HypertensionProcessor, AsthmaProcessor)\n‚úÖ assess_claim_Endpoint: Add assess-claim endpoint to API that doesn't require health data\n‚úÖ use_assess_claim_Endpoint: Have RRD (in vets-api) call VRO's assess-claim endpoint\n‚úÖ evidence_summary_doc (low priority - we can use the Ruby version in the meantime)\n‚úÖ generate_summary_doc Service (low priority): Improved PDF generator (given all pdf contents as request parameters)\n‚úÖ generate_summary_doc_Endpoint (low priority): Add generate_summary_doc endpoint to API\n‚úÖ use_generate_summary_Endpoint: Have RRD (in vets-api) call VRO's evidence_summary_doc endpoint\n\nImplied acceptance criteria for tasks above:\n\n‚úÖ API endpoints should have OpenAPI spec documentation with examples.\n‚úÖ Authentication is required for all service endpoints (assess_claim_Endpoint and generate_summary_doc_Endpoint) after version 0.1\n‚úÖ Integration testing between RRD (in vets-api) and VRO is needed in a non-prod env before deployment to prod\n‚úÖ VRO passes Secure Release process\nCode improvements tasks\n\nWould be ideal to get these completed as part of version 1.0:\n\nDocument how to update version numbers of VRO, container images, etc.\nMechanism to ensure microservices are idempotent and following good patterns\nImprove robustness to failures by using retry pattern -- retry logic in Camel\nüèÉ Clean up codebase; comment out unused dependencies\nAdd more automated end-to-end and integration testing\nAdditional DevOps tasks\n\nWould be ideal to get these completed as part of version 1.0:\n\nPersist RabbitMQ message queue contents for reloading in case of failures\nConnect VRO non-prod env with other VA non-prod envs\nSet up test code coverage and other PR requirement checks. Search jacoco_enforce_violations in the code.\nEnable linter and pre-commit hook\nSet up LHDI monitoring and diagnostics tools, i.e. DataDog monitoring, Prometheus, Grafana, Jaeger\nSet up DB querying (BI) tools for reporting, e.g., Metabase/Superset\nSet up DB monitoring tools to detect slow queries, e.g., NewRelic\nDocument how to debug and modify DB data in prod\nDocument how to extract microservices into Docker containers and deploy them\nAdd Python bug and vulnerability scanners (SecRel does not currently support Python code scanning)"
  },
  {
    "title": "VRO v2 Roadmap ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-v2-Roadmap",
    "html": "iMVP (Integrated MVP)\n\nWorkflow Diagram\n\nLegend:\n\nlight-green box: implemented in v1\ntrapezoid: microservice for External APIs; listens for requests added to an associated RabbitMQ queue\nrounded box: decision point (consider implementing using Camel Dynamic Routing)\n\"DB\" all refer to the same database and \"bipClaimsApi` represents the same microservice; they're drawn separately to reduce clutter\nAlso to reduce clutter, the VRO_API_error box represents returning an error code back to the VRO API client.\n\nFor details, refer to:\n\niMVP-claim-and-contention-update-scenarios\n\nAnother workflow digram from Mural\n\nTasks\n\n(only for single-issue hypertension CFI and presumptives; targeting Dec 31st deployment)\n\nOrdered by priority:\n\n#433 VRO retrieves OCR results from MAS/VBAAP\n#434 Exam-status endpoint for MAS/VBAAP to notify VRO\n#428 IncomingClaims: VRO receives hypertension CFI claims from MAS/VBAAP\n#435 Exam ordering: VRO calls MAS/VBAAP to order an exam\n#432 VRO handles PDF upload to eFolder\n#430 Mark claim \"RFD\" to enable appropriate adjudicator assignment\n#431 Remove special issues to enable claim downstream routing\n#436 Parse OCR annotation data\n(OCTO) Add feature flag (a.k.a. feature toggle) service\n(OCTO) Encrypt RabbitMQ messages\nfix Python microservices shut down when RabbitMQ shuts down\n\nHandling presumptives (NEW claims):\n\n#446 Check fast-tracking eligibility for presumptive claims\n#447 Use OCR data when assessing sufficient health evidence\n#448 Incorporate health evidence from OCR data into the evidence summary document\n#905 iMVP RFD and PDF logic changes\n#811 Relevant documents unavailable for automated review\n#1201 Support parity between MAS- and VRO-generated ARSDs\n\nOther tasks:\n\nDB metrics\nclaim_stats_Endpoint: Add claim_stats endpoint to API for reporting and monitoring. Other metrics (from Zach):\nhow many claims are we evaluating programmatically for sufficient evidence?\nhow many of those claims are deemed to have sufficient evidence?\nhow many claims deemed to have insufficient evidence lead to an auto-exam?\nhow many claims that are deemed to have sufficient evidence lead to: a) immediate rating, and b) not an immediate rating (e.g., deferral, manual exam)?\nEnsure RabbitMQ messages are processed despite microservices failures\nüèÉ (Ryan, Cameron) Connect VRO non-prod env with other VA non-prod envs\nüèÉ Clean up codebase; comment out unused dependencies\nüèÉ (Afsin) Add more automated end-to-end and integration testing\nüèÉ (OCTO) Document how to update version numbers of VRO, container images, etc.\nDONE: Persist API request contents for reloading in case of failures\nDocument how to extract microservices into Docker containers and deploy them\nDocument how to debug and modify DB data in prod\nDe-duplicate BP readings between LH and HDR (#1373)\nCode improvements tasks\nMechanism to ensure microservices are idempotent and following good patterns\nDocument conventions on how requesters handle errors, such as resubmitting the request -- see comment (OBE task: Improve robustness to failures by using retry pattern -- retry logic in Camel)\nAdditional DevOps tasks\nDONE: Set up test code coverage and other PR requirement checks. Search jacoco_enforce_violations in the code.\nDONE: Enable linter and pre-commit hook\nSet up LHDI monitoring and diagnostics tools, i.e. DataDog monitoring, Prometheus, Grafana, Jaeger\nSet up DB querying (BI) tools for reporting, e.g., Metabase/Superset\nSet up DB monitoring tools to detect slow queries, e.g., NewRelic"
  },
  {
    "title": "LHDI's Boilerplate Instructions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/LHDI%27s-Boilerplate-Instructions",
    "html": "VRO software is deployed on the Lighthouse Delivery Infrastructure (LHDI) platform, which offers tools, services, and support team.\n\nLHDI's Java Starter Kit was used to populate this codebase (see PR #8) using Java (AdoptOpenJDK) 17 and Gradle 7.4.\n\nLHDI's Boilerplate Setup Instructions\nAbout\nCI/CD Pipeline\nSecrets\nDefault Pipeline Behavior\nDependencies\nDeploying the Application\nWhat's Next\nAbout\n\nThis is a Java Spring Boot application that has been created using the Lighthouse DI Java 17 Starterkit. It is intended to be used as a starting point for building Java APIs and should be customized to deliver whatever functionality is required. If no other changes have been made, this application will have these features included by default.\n\nCI/CD Pipeline\n\nThis project comes with a skeleton Github Actions CI/CD pipeline out of the box. You can always choose to rewrite the pipeline using a different CI/CD tool; this pipeline serves as an example that you can use and run with minimal setup.\n\nSecrets\n\nIn order to run the pipeline, you will need to create a personal access token and add it to your repository's secrets in Github. The access token should have write:packages scope.\n\nThe secrets you need to configure are\n\nACCESS_TOKEN: the personal access token\nUSERNAME: the Github username of the user who owns the access token\nDefault Pipeline Behavior\n\nThe default pipeline has 3 jobs, which do the following things:\n\nRuns CIS benchmark tests against the application Docker image using docker-bench-security\nBuilds and tests application\nPublishes Docker image to VA GHCR repository\nDependencies\n\nThe pipeline runs on Github's ubuntu-latest runner, which is currently Ubuntu 20.04. The Github Actions Ubuntu 20.04 documentation lists the software installed by default. To learn more about choosing a Github runner and Github-hosted runner types, see the job.<job-id>.runs-on documentation.\n\nSoftware required for the pipeline but not installed by default, such as Java 17, hadolint, and spectral, is installed in the pipeline. The installation for app build dependencies is implemented as an action in <./.github/actions/setup-pipeline/action.yml>.\n\nDeploying the Application\n\nThe pipeline does not currently deploy the application to the DI Kubernetes clusters out of the box, although this setup will be coming in the future. To learn how to deploy your applications, see the DI ArgoCD docs.\n\nCommon Errors\n\nError: Cannot find plugin\n\nError Message:\n\n* What went wrong:\nPlugin [id: 'gov.va.starter.plugin.cookiecutter', version: '0.1.20', apply: false] was not found in any of the following sources:\n\n- Gradle Core Plugins (plugin is not in 'org.gradle' namespace)\n- Plugin Repositories (could not resolve plugin artifact 'gov.va.starter.plugin.cookiecutter:gov.va.starter.plugin.cookiecutter.gradle.plugin:0.1.20')\nSearched in the following repositories:\n    MavenLocal(file:/Users/aasare/.m2/repository/)\n    Gradle Central Plugin Repository\n    MavenRepo\n    BintrayJCenter\n    maven(https://palantir.bintray.com/releases)\n    maven2(https://dl.bintray.com/adesso/junit-insights)\n    starterBootPkgs(https://maven.pkg.github.com/department-of-veterans-affairs/lighthouse-di-starter-boot)\n    nexus(https://tools.health.dev-developer.va.gov/nexus)\n\n\nFix: Set your Github token as per the instructions in the Required Dependencies section above.\n\nError: Failed to get resource\n\nError Message:\n\nFailed to get resource: GET. [HTTP HTTP/1.1 401 Unauthorized: https://maven.pkg.github.com/department-of-veterans-affairs/lighthouse-di-starter-boot/starter/java/build-utils-property-conventions/starter.java.build-utils-property-conventions.gradle.plugin/0.1.32/starter.java.build-utils-property-conventions.gradle.plugin-0.1.32.pom)]\n\n\nFix: Set your Github token as per the instructions in the Required Dependencies section above, ensure that read:packages is true.\n\nWhat's Next\n\nOnce you have verified that you are able to run the application successfully, you can now start customizing the application to deliver the functionality you would like.\n\nBy default, this application assumes the use of a build, test, release cycle as defined in this development guide. Take a look at that guide to see how you can make changes, test them and get them deployed to a target environment.\n\nThe application itself is organized into the following three tiers of functionality:\n\nAPI\nService (business logic)\nPersistence\n\nTo see how each of these tiers is used by default, take a look at the Project Structure documentation."
  },
  {
    "title": "Virtual Regional Office (VRO) Overview ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Virtual-Regional-Office-%28VRO%29-Overview",
    "html": "Document Purpose: To provide a catalog of VRO offerings along with links to more detailed documentation. This page will be updated regularly to ensure it contains the most current information.\n\nWelcome to the VRO software platform! The VRO team is here to support you as a new partner product team getting up and running in VRO.\n\nüëã Who is the VRO team?\n\nWe are the software platform team for the OCTO Benefits Portfolio, and we support partner teams building products that improve claim processing for digitally submitted (ie. va.gov) claims. Together, these products form the 'Virtual Regional Office' product suite.\n\nWe work to ensure that teams building in VRO have access to shared knowledge, processes, and tools, allowing you to quickly build and validate ideas that improve the VA's employee-facing claims process.\n\nChat with us and follow along in DSVA Slack: #benefits-vro-support.\n\nMeet the team:\n\nOCTO (VA) Enablement Team\nZach Goldfine, Product Owner\nJulie Strothman, Design Lead\nCory Sohrakoff, Engineering Lead\nProduct & Research\nDiana Griffin, Product Manager\nBianca Rivera Alvelo, Designer/Researcher\nSoftware Engineers\nCheng Yin\nErik Nelsestuen\nJosiah Jones\nTeja Naraparaju\nTom Greene\nDevOps Engineers\nMason Watson\nüõ† VRO Features, Tools, and Support\n\nVRO provides a software development environment for Claims Fast-Tracking Crew teams to quickly integrate with existing VA services and deploy to the VA's Lighthouse Delivery Infrastructure (LHDI) platform. In service of this goal, we offer key features relevant to claims processing products, tools to expedite your development process, and the support, guidance, and subject matter expertise of our team members.\n\nüå± VRO is evolving! VRO is a new software platform, and will continue to grow and mature alongside our partner products. We collaborate with product teams to understand your application's needs and evolve VRO services and tools to meet them while incorporating them with the holistic needs of the product suite.\n\nFeatures\nEvent-based, scalable microservice architecture\n\nVRO implements an event-driven architecture with Queue-Processor components that act like an internal microservice, modularizing functionalities so that each can be updated and maintained more easily. These components are connected together using well-tested and stable Enterprise Integration Patterns (EIP) tools (such as Apache Camel) so that we can focus on VRO functionality and less on ‚Äúglue code‚Äù. This approach promotes low software coupling and, as a result, simplifies debugging and maintenance.\n\nLearn more\nReusable software patterns\n\nTo help expedite time-to-deployment and maintain consistency between products, VRO provides reusable software patterns that implement VRO Software Conventions. VRO offers libraries and encourages reuse of software patterns to minimize onboarding, diagnosing, and development time.\n\nVA system integrations\n\nClaims Fast-Tracking products need to access and update claim data in order to deliver desired outcomes, which requires integrating with other VA systems. VRO offers a growing number of integrations with VA systems (such as Lighthouse APIs, BIP APIs, and BGS) and other services (such as Slack). Leveraging VRO's existing integration services could save many weeks (if not months) of work per integration.\n\nLearn more\nDatabase and Redis cache\n\nVRO provides a dedicated Postgres database for persisting (non-PII and non-PHI) data, as well as a Redis cache to temporarily store and track data.\n\nAPI gateway\n\nVRO's API Gateway provides a single location to access all APIs provided within VRO, regardless of implementation language. To expose APIs for each domain, the API Gateway offers a Swagger UI to inspect API offerings, retrieving the OpenAPI spec for the selected API from domain-specific containers and presenting it in a Swagger UI.\n\nLearn more\nTools\nEstablished development process\n\nVRO partner teams can get up and running quickly using our established development process with Gradle utilities and Github Actions workflows to automate code testing, Docker image creation, and housekeeping tasks.\n\nLearn more about:\n\nVRO development process\nGradle utilities\nGithub Actions\nFlexible deployment configurations\n\nFor teams building in VRO, flexible deployment configurations and processes ease deployment under the VA's cATO (continuous authority to operate). The build and deployment pipeline incorporates the required Secure Release process and scanning, which minimizes software vulnerabilities and keeps the software up-to-date.\n\nLearn more about:\n\nHelm Charts for deployment configuration\nDeployment process\nCI CD Workflows\nSecure Release (SecRel) process\nDependabot\nSupport\nSoftware conventions\n\nThe VRO team develops, maintains, and expands our Software Conventions to help partner teams jump start their development, reduce time-to-deployment, and maintain consistency across products in the VRO product suite.\n\nInherited Continuous Authority to Operate (cATO)\n\nTraditional ATO processes can add weeks or months of coordination and overhead before an initial product launch, but VRO benefits from Lighthouse Delivery Infrastructure's continuous ATO (cATO) process to stay in compliance while being able to deploy frequently to production. Products within the VRO product suite benefit further by inheriting VRO's ATO -- we incorporate your product scope into our cATO process so that you don't have to start from scratch.\n\nHands-on support and collaboration\n\nThe VRO team is here to support our partner product teams, from initial idea validation through development and launch, and with ongoing support and maintenance of a stable production environment where your product can grow and scale. We work iteratively to support new requirements and improve the maintainability and sustainability of the ABD-VRO codebase, with built-in DevOps to ensure VRO is operational and scaling as needed and team members available to help onboard partner teams, review Pull Requests, and offer technical support, product guidance, and design standards."
  },
  {
    "title": "VRO Test Cases ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Test-Cases",
    "html": "Latest Test Cases for VRO v1.0\n\namida vro test cases -17 aug 2022.xlsx"
  },
  {
    "title": "VRO RabbitMQ Strategy ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-RabbitMQ-Strategy",
    "html": "Scope\n\nThe goal of this page is to identify the strategy that VRO implements for Partner Teams' integrations with VRO microservices via RabbitMQ. This document includes:\n\nVRO & Partner Team Responsibilities\nExchange / Queue Declarations and Bindings\nRequests / Response Models\nPartner Team Recommendations\n\nFor more information on RabbitMQ specifics or AMQP protocol concepts, consult the documentation on the RabbitMQ website.\n\nVRO Microservice Responsibilities\ndeclaring exchanges and their type (direct exchange, fanout exchange, topic exchange, or headers exchange)\ndeclaring request queues\nbinding request queues to the exchanges\nThe following settings are expected for exchange declarations:\nmust be declared durable¬†(exchange will survive a broker restart)\nmust be auto-deleted (exchange is deleted when the last queue is unbound from it)\nThe following settings are expected for request queue declarations:\nmust be declared durable (queue will survive a broker restart)\nmust be auto-deleted (queue that has had at least one consumer will be deleted automatically when the last consumer has unsubscribed (disconnected))\nmust not be exclusive (queue is able to have more than a single connection besides declaring service)\nPartner Team Responsibilities\n\nIn order for partner teams to communicate via RabbitMQ to VRO's microservices, queues and exchanges must be declared identically to the VRO microservice that will be processing requests and supplying responses. Failure to do so will result in the applications' inability to consume or publish to an exchange/queue, and depending on the RabbitMQ library client, a runtime exception.\n\nThe following settings are expected for exchange declarations:\nmust match VRO declarations for exchanges\nmust not publish messages to the default exchange\nThe following settings are expected for request queue declarations:\nmust match VRO declarations for queues\nThe following settings are expected for response queue declarations:\nmust declare response queues as necessary\nmust bind response queues to the appropriate exchanges\nPublishing request requirements:\nmust provide reply_to property in request message\nmust correlate requests made to responses received by using correlation_id property in request message\nmust also keep track of those correlation_ids\nmust implement time-out or retries policies for requests where expected response was never received\nmust provide any other message level properties such as delivery_mode, see here for more info on message level properties in request message\nmust implement any sort of publisher acknowledgements if desired\nMessage consumption\nmust provide message consumer acknowledgements, rejections, or negative acknowledgements to the server upon receiving a response message\nmust validate the correlation_id if consuming a message as a response to a request\nRequest / Response Model\n\nIn order to allow multiple partner teams to utilize downstream VRO microservices, the request and response model for each of the VRO's microservices through RabbitMQ shall be consistent. Keep in mind, publishing with the correct message properties and payload is the partner team application's responsibility. It is also the responsibility of each VRO microservice to validate requests before processing, or, in the case of a pass-through service like svc-bip-api, pass the request as-is to a downstream service and report the response as reported by the downstream service.\n\nRequests\n\nThe following structure shall be used for publishing requests to VRO microservices via RabbitMQ:\n\nRequired Message Properties:\ncontent_type=\"application/json\"\napp_id - name of calling application\nreply_to - name of response queue if a response is desired\ncorrelation_id - the ID assigned to a request that will be returned with a response to correlate the response to the request made if a response is desired, it will need to be correlated to a request\nRequired Payload:\n\nThe required payload is determined by the VRO microservice to which the requests are routed.\n\nResponses\nRequired Message Properties:\ncontent_type=\"application/json\"\ncorrelation_id - id used to correlate the request for which this response was made, if the request contained a correlation_id\napp_id - id of application returning the response\nother message properties set by RabbitMQ (see here)\nRequired Payload (aka. response body)\n\nVRO microservices are responsible for determining the majority of the body of the response. The only required fields are the integer value statusCode and string value statusMessage for any type of request. The statusCode should represent a typical rest response code or the VRO microservice's defined values. The statusMessage should be a string representation of the statusCode for quick readability of the response. If additional information regarding the status is required, the VRO microservices is responsible for defining that field to be added with statusCode and statusMessage.\n\nExample payloads:\nResponse for a request that does not have any additional fields in response body:\n{\n   \"statusCode\":201,\n   \"statusMessage\":\"CREATED\"\n}\n\nResponse for a request that resulted in a 500\n{\n   \"statusCode\":500,\n   \"statusMessage\":\"INTERNAL_SERVER_ERROR\"\n}\n\nResponse for a request to a service that also returns an array of results:\n{\n   \"statusCode\":200,\n   \"statusMessage\":\"OK\"\n   \"results\":[\n     {\n        \"fieldName\":\"thing1\",\n        \"intVal\":1\n     },\n     {\n        \"fieldName\":\"thing1\",\n        \"intVal\":1\n     }\n   ]\n}\n\nPartner Team Recommendations\n\nPartner teams are welcome to implement their own solutions, provided that those solutions adhere to the information presented above. If there are any questions or concerns, contact the VRO team.\n\nFor parter teams implementing their application in python, they can use a solution that follows this strategy by utilizing the hoppy library for asynchronous request/response patterns with a client that has configurable RabbitMQ connection parameters, retries and timeout policies, queue and exchange declarations, and more!"
  },
  {
    "title": "VRO Engineer Onboarding ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Engineer-Onboarding",
    "html": "Editors: Any new additions to onboarding that require the reverse process for offboarding should be added to VRO Engineer Offboarding.\nScope:\n\nThis document is specifically designed for Engineers onboarding to the VRO (Virtual Regional Office) software project with the intent of streamlining and reducing the startup time for ramping up on VRO.\n\nFor offboarding process, see VRO Engineer Offboarding.\n\nTechnical Intro:\n\nVRO Software uses the following:\n\nMostly Java - heavy use of the Spring Framework\nMany microservices are written in Python (under the service-python folder)\nApache Camel - important to learn\nRabbitMQ - used to communicate with microservices\nGradle - heavily used; also see the buildSrc folder\nDocker - heavily used locally and for deployments\nNote that for most use cases, this contract does not supply a Docker Desktop license, meaning that you will need a different container runtime to use Docker locally. Colima is one option that is emerging as a good alternative.\nPostgres - learn as needed\nRedis - learn as needed\nKubernetes (for DevOps) - LHDI provides the infrastructure\nCurrent state (as of May 2nd, 2023): VRO is being updated to support partner teams, such as the Contention Classification team.\nEngineer Checklist:\n\nBelow are actions you should take or links with resources to review. As you are going through onboarding, feel free to add other pages to this list if you think they'd be useful.\n\nGet added to recurring team meetings such as¬†Weekly VBA Stakeholder Sync, Daily VRO Team Sprint Standups, Biweekly VRO Sprint Planning / Review / Retrospective, etc.,\nJoin relevant slack channels -- you don't have to join ALL these channels; start with the #benefits-vro-* and #benefits-cft-* and you can expand from there as you get more involved in the work. Feel free to ask the team in #benefits-vro if you're wondering about whether to join a particular channel.\nMost relevant channels for you: #benefits-vro (our team channel for coordination across the VRO team), #benefits-vro-engineering (engineering-specific collaboration for the VRO team), #benefits-cft (cross-team collaboration channel for the 3 teams in the Claims Fast-Tracking Crew)\nWatch \"The Way We Work\" presentation\nGo through Benefits Portfolio onboarding materials and onboarding buddy meetings (see #benefits-onboarding channel in Slack; you'll be tagged there with your onboarding buddy assignments and materials to review)\nSet up team 1:1s (See their slack profiles for a calendar URL where you can find available times):\nVA Product Owner, Premal Shah\nOCTO engineering leads, Steve Albers and Cory Sohrakoff\nVA / VRO onboarding buddy meetings (likely initiated by them)\nGet added to the VA GitHub organization (ask someone who is already in the org)\nGet added to the OCTO-VRO GitHub team to have access to VRO's GitHub repos\nGet added to the OCTO Benefits Portfolio roster\nIf you need access to VRO's LHDI deployment environments, get added to the VA-ABD-RRD GitHub team\nIf you need access to potentially \"dangerous\" actions (such as deploying to prod or deleting packages), get added to VRO-RESTRICTED.\nWhen you have your VA.gov email, connect it to your GitHub account by adding as an email in your settings. You must also make sure the name on your profile matches your name. This will allow you to perform SSO to various apps through GitHub on the VA's network, e.g. Aqua.\nWhen you have your PIV card, set up an Okta account following these instructions\nAdd your name and skills to POCs\nRead VRO's wiki, in particular the following (some content may need updating):\nProblems being solved and Roadmap provides context\nRouting-API-requests, Code structure, and Development process (from Slack thread)\n(Optional) If interested in the initial incarnation of VRO (before Amida touched it), watch old recording '2022-06 VRO code walk-through.mp4' in Shared Meeting Notes\nGradle projects\nCurrent Software State is a good intro to the codebase history\nLHDI's Boilerplate Instructions and if needed Setup on Windows. Amida wrote up some setup instructions for Windows in Software Design Document (linked from Roadmap#references)\nExternal APIs to interact with other systems\nReview the materials on this wiki page for walkthroughs of the code\nWatch this demo from the Lighthouse Delivery Infrastructure (LHDI) team on the SecRel pipeline and SD Elements\nRemember to periodically add what you're working on (or responsible for) to the Engineering POCs spreadsheet (pinned at the top of the #benefits-rrd-engineering Slack channel)\nFor background, context, and motivation, read VA Automated Benefits Delivery Team - Team Onboarding as a lower priority (whenever you have downtime between meetings or waiting on blockers to be resolved). Important topics from that document include:\nTeams and vision: various responsibilities of various teams (requires sharepoint)\nSlack usage\n\n(Unlikely) If you'll be working in va.gov's codebase (a.k.a. RRD), you should review:\n\nTechnical onboarding\nRRD technical overview\nVA DevOps Release Process (may be out of date)\nCurrently, Yang and Yoom are familiar with va.gov's RRD codebase, which should be sufficient."
  },
  {
    "title": "VRO Engineer Offboarding ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Engineer-Offboarding",
    "html": "Editors: Any new additions to offboarding that require the reverse process for onboarding should be added to VRO Engineer Onboarding.\nScope:\n\nThis document is specifically designed for Engineers offboarding from the VRO (Virtual Regional Office) software project.\n\nFor onboarding process, see VRO Engineer Onboarding.\n\nOffboarding checklist\nRemove from recurring team meetings\nRemove from VA-ABD-RRD team and VA-ABD-RRD GitHub team\nUpdate status as deactivated on OCTO Benefits Portfolio roster\nIf onboarded to VFS Platform:\nOpen a Platform ticket to offboard user. This includes stuff like Slack, SOCKS, AWS, team roster spreadsheet ‚Äì pick and choose as needed.\nIf leaving vets-api codebase:\nOpen a PR to remove email from flipper.admin_user_emails in config/settings.yml\nIf staying at VA, ask them to leave Sidekiq team (otherwise, removal from GH org takes care of this)\nIf not onboarded to VFS Platform, the Platform offboard ticket template can still be used. But we can also open a separate ticket to remove user from GitHub org."
  },
  {
    "title": "VRO Database for RRD ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Database-for-RRD",
    "html": "OBSOLETE\nIntroduction\n\nVRO Database is primarily used for audit purposes. It does not store any Personal Identifiable Information (PII) or Personal Health Information (PHI). The following information is available\n\nClaim ID (VBMS ID), disability action type, presumptive and RFD (Ready for Decision) flags\nClaim Submission data associated with the attempt to process each Claim Submission Request through the workflow\nany external vendor ID (Reference ID ‚Äî used by external vendors to track the claim in their systems)\nalong with the associated type (representing the specific vendor associated with the Reference ID)\nNon PII Ids for the Veteran who owns the claim\nContentions in the claim\nExam Order state as issued by VRO and updated from MAS\nNon PHI summary information for the assessment for each of the contention\nNon PHI summary information for each evidence PDF generated for the contention\nVarious event audit logs as a claim goes through VRO system\nDatabase version information to support future changes through database migrations\n\nAll tables and fields are available in the Entity Relationship Diagram (ERD). This version of ERD is manually generated using DBeaver on 3/12/2023.\n\nNote that the database is designed for multi issue claims although iMVP will not support such claims.\n\nTechnologies and Code Walkthrough\nDatabase\n\nVRO uses PostgreSQL. Since RDS is currently not available in LHDI, a PostgresSQL Docker based container serves as the database. The subproject postgres is used to build the container.\n\nThe Dockerfile in the postgres subproject is a simple wrapper around the Docker Hub PostgresSQL image. The primary reason to use the Dockerfile instead of using the Docker Hub image directly is to run the initialization script init_db.sh. This script creates the application database and a database user to run database migrations that are different from the default database postgres and the super user for security purposes.\n\nThe version of the database can be found in the subproject Dockerfile.\n\nVRO achieves persistance through persistent Docker Volumes. The setup the volumes can be found in the application docker-compose file for local development and in the Helm's chart templates for deployment.\n\nDatabase Migrations\n\nVRO uses Flyway to initialize and change the database. The subproject db-init containts all database migration related artifacts. In particular all versioned migrations are SQL based and in the directory here. These migrations create all the schemas and tables. In addition an additional user with limited privileges is created. This user is used to access the database within all non migration VRO functionality.\n\nThe subproject db-init also contains a Dockerfile. The container based on this Dockerfile is used to run the migrations both in the local development docker-compose file and in application deployment.\n\nThe strategy for creating migration files is simple: Each work branch and subsequent Pull Request should be its own contained version number. Furthermore, you should create one migration file per proposed table change. Smaller, incremental changes on a per-file basis allows for better maintainability, troubleshooting, and testability.\n\nSpring Data JPA\n\nVRO uses Spring Data JPA to access and manage data in the database from the Java code. The subproject persistance/model contains the Object-relational mapping (ORM). In particular entity files that map to the database tables are in here.\n\nTo access and manage entity data , VRO uses JPA Repositories. All the JPA repositories are in here. These JPA Repositories inherit basic CRUD methods and also contains explicit definition of more complex methods needed by the VRO applications. In either case implementation of the methods are provided by Spring Data JPA automatically.\n\nService Interfaces and Implementations\n\nVRO defines 3 service interfaces (Java Interfaces) in the service/spi subproject to populate and access the database. These interfaces are\n\nSave to Db service: This service is used within the Camel routes to store information about claims, assessments, and evidence pdf generations.\nClaim Metrics service: This service serves as the service later for the REST interface that exposes information in the database.\nAudit Event service: This is also used within the Camel routes and logs various information about events that occur as claims are processed and external systems are called.\n\nThese services are implemented (Java Implementations of Interfaces) in the service/db subproject as Spring Boot services and are autowired in the rest of the projects. Implementations use JPA Repositories in the subproject persistance/model.\n\nUsage within Camel Routes\n\nThe Save to Db service and Audit Event service are primarily used in Camel routes which are defined in the service/provider.\n\nUsage within REST Interface\n\nThe Claim Metrics service is used in the implementation of claim-metrics and claim-info REST end points. These end points are defined in Claim Metrics resource and implemented in Claim Metrics controller.\n\nDatabase Tables and Fields\nGeneral\n\nThe Entity Relationship Diagram (ERD) shows all the tables and fields used. All the tables reside in the claims schema. These are the tables in the database\n\nclaim: This table stores audit information about the incoming Claims themselves (as unique entities). The corresponding entity class is ClaimEntity.\nclaim_submission: This table stores audit information concerning each attempt to submit and process a Claim Submission through VRO. The corresponding entity class is ClaimSubmissionEntity.\nveteran: This table stores audit information about the veterans in the claims. The corresponding entity class is VeteranEntity.\ncontention: This table stores audit information about the contentions in the claims. The corresponding entity class is ContentionEntity.\nexam_order: This table stores audit information about the contentions in the Exam Order status. The corresponding entity class is ExamOrderEntity.\nassessment_result: This table stores audit information about the assessmen results for the claims. The corresponding entity class is AssessmentResultEntity.\nevidence_summary_document: This table stores audit information about the evidence documents created for the claims. The corresponding entity class is EvidenceSummaryDocumentEntity.\naudit_event: This table stores log information as claim processing progresses through the Camel routes. The corresponding entity class is AuditEventEntity.\nschema_history: This table stores migration version information and is used by Flyway database migrations. Database migration are described in Database Migrations\n\nThe tables claim, claim_submission, veteran, contention, exam_order, assessment_result and evidence_summary_document all have created_at and updated_at columns. These columns are inherited by the corresponding Entities from a BaseEntity. BaseEntity uses Spring Data JPA @CreatedAt and @LastModifiedDate annotations to implement the functionality. With these annotations Spring Data JPA automatically populates the fields without additional code in VRO.\n\nFor the same tables id column implementation is also shared in BaseEntity.\n\nDetails for Claim Table\n\nThe following are the column descriptions for the claim table.\n\nvbms_id: This represents the VBMS system ID for the Claim. This is not intended as the source of truth and should be paired with submission_date to determine when the last known valid associated timestamp was.\nveteran_id: Foreign key to the veteran table and identifies the Veteran the claim belongs to.\ndisability_action_type: i.e. INCREASE\npresumptive_flag: Represents the Claim's presumptive status for fast-tracking\nrfd_flag: Represents a Ready For Decision (RFD) state\nDetails for Claim Submission Table\n\nThe following are the column descriptions for the claim_submission table.\n\nclaim_id: Foreign-key to the corresponding ID in the claim table.\nreference_id: Represents an external vendor's internal system ID for tracking the Claim. Used in addition to id_type to identify the source of the claim.\nid_type: Represents the external source, or vendor, of the Claim. Used in addition to reference_id to identify the source of the claim. This was the constant va.gov-Form526Submission for V1. For V2 this constant is 'mas-Form526Submission'\nincoming_status: Status of the incoming claim. This was the constant submission for V1.\nsubmission_source: Taken from the claimSubmission.claimDetails.claimSubmissionDateTime initially sent by MAS\nsubmission_date: Taken from the claimSubmission.claimDetails.claimSubmissionSource initially sent by MAS\noff_ramp_reason: Explanation for why the claim was off-ramped.\nin_scope: Boolean flag representing the claim is in scope of being processed. Set by VRO. Defined in #428 but potential duplicate of off_ramp_reason=outOfScope ‚Äî we might not need this anymore\nDetails for Veteran Table\n\nThe following are the column descriptions for the veteran table.\n\nicn: The Internal Control Number (ICN) for the Veteran. It uniquely identifies the Veteran in VHA systems.\nparticipant_id: The Participant Id for the Veteran. It uniquely identifies the Veteran in VBA systems and is actually the database ID in the CorpDb.\nicn_timestamp: Since it is possible for ICNs to change, you can tell when the ICN was last updated with this timestamp. In theory, you could also use updated_at, but that column could possibly apply to other pieces of data here, so icn_timestamp provides a targeted \"last known good time\".\nDetails for Contention Table\n\nThe following are the column descriptions for the contention table.\n\nclaim_id: Foreign key that links the contention to the claim it is submitted with.\ndiagnostic_code: The diagnostic code for the contention. It links the contention to the VASRD codes.\ncondition_name: Name of the condition to be assessed\nclassification_code: Taken from the claimSubmission.claimDetails.classificationCode initially sent by MAS\n\nNote that this design assumes multiple contentions per claim for future developments although iMVP will support only one contention (Hypertension) in single issue claims.\n\nDetails for Exam Order\n\nThe following are the column descriptions for the exam_order table.\n\nclaim_submission_id: Foreign key that links the Exam Order to the Claim Submission that issued it.\ncollection_id: Collection ID associated with the Claim Submission.\nstatus: The current status of the Exam Order\nORDER_SUBMITTED: initial status from VRO, which creates a record when it issues a new Exam Order\nVRO_NOTIFIED: TODO: Seek clarification on if this will continue to remain the initial submitted status follow-up from IBM/MAS\nDRAFT: This was the initially assumed initial submitted status follow-up from IBM/MAS indicating the Exam Order is in Draft State\nFINAL: status from IBM/MAS indicating the Exam Order has been completed\nordered_at: TODO: Change to status_at to represent the various statuses\n\nA Claim Submission can have multiple Exam Orders.\n\nDetails for Assessment Result Table\n\nThe following are the column descriptions for the assessment_result table.\n\ncontention_id: Foreign key that links assessment result to contention.\nevidence_count_summary: Summary of evidence counts for the assessments. This is a JSON objects that summarizes assessment and is provided by the assessment microservice.\nsufficient_evidence_flag: Originally from #447. Represents that the Assessment Result has determined there is sufficient evidence to mark the claim as RFD.\nDetails for Evidence Summary Document Table\n\nThe following are the column descriptions for the evidence_summary_document table.\n\ncontention_id: Foreign key that links evidence summary document to contention.\nevidence_count: Summary of evidence counts for the document. This is a JSON objects that summarizes the information shown in the document.\ndocument_name: Name of the document generated.\nfolder_id: Represents the UUID of the folderId returned by BIP on PDF upload, in order to facilitate easier tracking down of the file in eFolder.\nDetails for Audit Event Table\n\nThis is the table structure for audit events:\n\nevent_id: A unique id identifying the request\nroute_id: The id of the camel route from which the event is issued. Example: \"mas-order-exam\"\npayload_type: The type of payload being processed. Example: \"Automated Claim\"\ndetails: Other details pertinent to the event, but specific to the type of processing. For Example, collectionId, offRampReason, presumptiveFlag, et cetera.\nmessage: A descriptive message explaining the action. Example: \"Collecting evidence\"\nthrowable: The stacktrace of an exception, if the even indicates an error.\nevent_time: Date and time the event was issued.\n\nSimplified ER (detailed ERD can be found here: Entity Relationship Diagram (ERD)): "
  },
  {
    "title": "VRO Database ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Database",
    "html": "VRO's database does not store any Personal Identifiable Information (PII) or Personal Health Information (PHI).\n\nUse non-PII identifiers for the Veteran\nUse non-PHI summary information for contentions\n\nThe VRO platform only supports Postgres as a DB engine at this time.\n\nLocal Development\n\nThe postgres subproject builds the Docker container to serve as the database, which is useful for local testing. The postgres/Dockerfile uses the PostgresSQL image as a base image and runs the initialization script init_db.sh. This script creates a database user to run Flyway database migrations -- this user must be separate from the super user for security purposes.\n\nData is retained between container restarts through persistent Docker Volumes -- see the volumes configuration in docker-compose.yml.\n\nConnect to DB\n\nTo connect to the local Postgres DB:\n\nStart the VRO Platform Base: ./gradlew :dockerComposeUp (see Docker Compose)\nOr start only the postgres container: docker compose up -d postgres-service db-init\nGet the connection URI: echo postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:5432/vro\nConnect using psql (if needed, install psql)\npsql postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:5432/vro -c \"\\dt claims.*\"\n                   List of relations\n Schema |         Name         | Type  |     Owner\n--------+----------------------+-------+----------------\n claims | bie_contention_event | table | vro_admin_user\n claims | schema_history       | table | vro_admin_user\n(2 rows)\n\nTo connect using a GUI, try DBeaver -- it provides an Entity Relationship Diagram (ERD) of the tables and columns.\nRemote Environments\n\nVRO uses Postgres on AWS RDS in all LHDI remote environments. The URLs for accessing RDS DB Instances are made available to applications as K8s secrets which are passed along as environment variables to applications through Helm.\n\nA useful set of these environment variables can be referenced here. For any application seeking to use the DB, developers should ensure they include these definitions via symlink in their Helm chart deployment definition definition under a subdirectory named named_templates. Then in their deployment definitions they should set environment variables for the db client like in the xample-workflows deployment definition\n\nDevelopers may use their PostgresSQL client of choice to connect to the database by querying these environment variables at application startup or runtime.\n\nVRO Engineers\n\nSee the internal wiki for documentation on AWS entities created for the RDS integration.\n\nThese AWS entities are most easily managed by interacting with the LHDI kubernetes clusters (e.g. with the kubectl command or through the Lens GUI tool). They are specified using AWS controllers for Kubernetes.\n\nDatabase Migrations\n\nVRO uses Flyway to initialize and change the database. The db-init subproject contains all database migrations and is used to initialize and update the database schema.\n\nAll versioned migrations are SQL based and in the migrations directory. These migrations create all the schemas and tables.\nAn additional user with limited privileges is created. This user is used to access the database within all non-migration VRO functionality. A separate user can be created for each new domain.\n\nThe db-init/Dockerfile will run the migrations both in the local development and in LHDI deployments.\n\nThe strategy for creating migration files is simple:\n\nEach work branch and subsequent Pull Request should be its own contained version number.\nCreate one migration file per proposed table change.\nSmaller, incremental changes on a per-file basis allows for better maintainability, troubleshooting, and testability.\n\nNOTE: At this time, migrations are only applied in remote environments on deployment of the VRO Application.\n\nDatabase Tables and Fields\n\nDB tables have created_at and updated_at columns. These columns are inherited by the corresponding Entities from a BaseEntity. BaseEntity uses Spring Data JPA @CreatedAt and @LastModifiedDate annotations to implement the functionality. With these annotations Spring Data JPA automatically populates the fields without additional code in VRO.\n\nSimilarly, the id UUID column is also included in BaseEntity.\n\nAll the tables reside in the claims schema. These are the tables in the database:\n\nschema_history: stores DB migration version information and is used by Flyway for database migrations.\nbie_contention_event: stores BIE Kafka events related to contentions\nAccessing the DB via Java\n\nVRO uses Spring Data JPA to access and manage data in the database from the Java code. The subproject shared/persistence-model contains the Object-relational mapping (ORM).\n\nTo access and manage entity data, VRO uses JPA Repositories. All the JPA repositories are in shared/persistence-model. These JPA Repositories inherit basic CRUD methods and also contains explicit definition of more complex methods needed by the VRO applications."
  },
  {
    "title": "VRO Console ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Console",
    "html": "To investigate and recover from errors particularly in the production environment, a VRO Console container was implemented and took inspiration from Rails Console.\n\nThe VRO Console container (or simply Console) facilitates diagnostics, such as examining the processing state of the claim by looking at Camel routes and the DB contents, and realtime updates to VRO's state in Redis and VRO's DB.\n\nEnable Console Locally\nexport COMPOSE_PROFILES=debug -- the console container only starts if the docker-compose debug profile is enabled.\nStart VRO. Note the vro-console-1 container; check out the logs: docker logs vro-console-1.\nAttach to the container: docker attach vro-console-1\n\nIf you need to restart the console container:\n\ncd app/src/docker\ndocker-compose up -d console\ndocker attach vro-console-1\n\nConnect to deployed Console container\n\nIn order to connect to Console container deployed in LHDI, set up kubectl using the Lightkeeper tool.\n\nNext to connect to the DEV deployment:\n\n# For convenience\n‚ùØ alias kc='kubectl -n va-abd-rrd-dev'\n\n‚ùØ kc get pods\nNAME                             READY   STATUS     RESTARTS   AGE\nvro-api-645dc44c64-w95mw             0/6     Init:1/3   0          7m52s\nvro-api-postgres-559c5bddbb-7rm2r    1/1     Running    0          7m53s\nvro-api-rabbit-mq-74bd4c5bfc-lxb7v   1/1     Running    0          7m53s\nvro-api-redis-555446854-jwfqg        1/1     Running    0          7m53s\n\n# The console container is in the pod with several containers\n‚ùØ kc exec -i -t vro-api-645dc44c64-w95mw -c abd-vro-console -- sh -c \"java -jar vro-console.jar\"\n\nFor other deployment environment, adjust the kubectl namespace in the alias.\n\nMore details in PR #695.\n\nConsole Usage\nInspect DB contents\n\nAdded in PR #531.\n\nOn the groovy:000> prompt, try the following:\n?  # display help\n// Note the printJson (alias pj) custom command\n\n:show variables\n// Note the `claimsT` variable, which can be used to query the claims DB table\n\nclaimsT.findAll().collect{ it.claimSubmissionId }\nc = getAnyClaim()\n// Different ways to do the same thing:\nprintJson c\nprintJson getAnyClaim()\npj c\n// exit   # This will stop the container\n// Instead, press Ctrl-p Ctrl-q to detach from the container without stopping it\nSee https://groovy-lang.org/groovysh.html for other built-in console commands\nInspect Redis\n\nAdded in PR #614.\n\ngroovy:000> :show variables\n// Note the redis and redisT variables\n\ngroovy:000> redis.keys \"*\"\n===> [claim-1234]\ngroovy:000> redis.hlen(\"claim-1234\")\n===> 2\ngroovy:000> redis.hkeys(\"claim-1234\")\n===> [\"type\", \"pdf\"]\ngroovy:000> redis.hget(\"claim-1234\", \"type\")\n===> hypertension\ngroovy:000> redis.hget(\"claim-1234\", \"pdf\")\n===> JVBERi0xLjQKMSAwIG9iago8PAovVGl0bGUgKP7/KQovQ3JlYX ... (truncated base64 encoding of the generated pdf)\n\n// Using RedisTemplate redisT\ngroovy:000> ops=redisT.opsForValue()\ngroovy:000> ops.hget(\"claim-1234\", \"type\")\n===> hypertension\nWiretap Camel routes\n\nListen to messages at certain predefined wireTap Camel endpoints. Added in PR #597\n\ngroovy:000> :show variables\n// Note the camel variable\n\n// Check out http://localhost:15672/#/queues for current queues and see how `console-*` queues are added as the following commands are run.\n\n// Initially no routes in the CamelContext of the Console container\ngroovy:000> camel.routes\n===> []\n\ngroovy:000> wireTap claim-submitted\n// Now, submit a claim using Swagger\n// Expect to see a log message\n\ngroovy:000> wireTap generate-pdf\n// Now, generate a pdf using Swagger\n// Expect to see a log message\n\ngroovy:000> camel.routes\n===> [\n  Route[rabbitmq://tap-generate-pdf?exchangeType=topic&queue=console-generate-pdf -> null], \n  Route[rabbitmq://tap-claim-submitted?exchangeType=topic&queue=console-claim-submitted -> null]\n]\nInject message into workflow\n\nTo submit a message from the VRO Console into a Camel Route endpoint:\n\n// Create the request as a JSON String\nreq=\"\"\"{\n  \"resourceId\": \"123444\",\n  \"diagnosticCode\": \"A\"\n}\"\"\"\n\n// Create the Camel endpoint URI\nexchangeName=\"v3\"\nroutingKey=\"postResource\"\nuri=\"rabbitmq:\" + exchangeName + \"?skipQueueBind=true&routingKey=\" + routingKey\n\n// Inject the message -- see CamelEntry for alternatives ways to inject\nresp=pt.requestBody(uri, req, String)\n===> {\"resourceId\":\"123444\",\"diagnosticCode\":\"A\",\"status\":\"PROCESSING\",\"reason\":null}\n\nThis requires that the Camel Route endpoint be exposed outside of the JVM, e.g., the endpoint uses rabbitmq: and not direct: or seda:. The VRO Console has access to RabbitMQ queues, not the internal JVM queues or endpoints.\n\nCustomizations\nAdd custom console commands\n\nSee the PrintJson and Wiretap classes.\n\nAdd custom function\n\nAdd to the console/.../groovysh.rc file."
  },
  {
    "title": "VRO Architecture Diagram ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Architecture-Diagram",
    "html": "VRO is deployed to Kubernetes clusters provided by LHDI.\n\nLegend\n\ncircle: external service or API not controlled by us\nLighthouse APIs\nBIP APIs\nBGS API\nBIE Kafka Event Stream\n(oblique) parallelogram: Docker container\nAPI Gateway\nDocker containers\ncylinder: data storage, such as a database, persistent volume, or temporary cache\nVRO Database\nHelm Charts\nHow to ...\nExport diagram as image\n\nTo export the diagram as an image:\n\nClick on the diagram. This opens the Mermaid diagram editor (https://mermaid.live/edit#...).\nClick the \"Actions\" link near the bottom of the page, then click the PNG or SVG button.\nUpdate diagram\nClick on the diagram. This opens the Mermaid diagram editor (https://mermaid.live/edit#...), where you will:\nEdit the markup code as desired -- see Mermaid tutorial.\nWhen done, click the \"Actions\" link near the bottom of the page, then the \"Copy Markdown\" button.\nOpen this wiki page in a new browser window and click the \"Edit\" link.\nDelete the diagram markdown text (which starts with [![](https://mermaid.ink/img/pako: and ends with )).\nPaste over the new diagram markdown text, which you copied earlier. Optionally remove the ?type=png from the markdown text to make it render in GitHub's dark theme better.\nClick the \"Preview\" tab.\nIf it looks correct, click the \"Save page\" button."
  },
  {
    "title": "Virtual Regional Office (VRO) Platform Outline ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Virtual-Regional-Office-(VRO)-Platform-Outline",
    "html": "Overview\n\nThe Virtual Regional Office (VRO) is a suite of products that contribute to making disability benefits claims submitted on va.gov maximally actionable and minimally burdensome for claims adjudicators, resulting in faster and more accurate claim processing and shorter wait times for Veterans to get a decision on their claims.\n\nWithin the VRO ecosystem, the VRO platform supports the teams building these products, by providing them with the data, systems access, authority, and infrastructure needed to quickly and effectively develop new product hypotheses.\n\nOutcome Summary\n\nOur Mission: The VRO platform exists to provide an environment where it's easy to build software to improve the VA's internal claims process.\n\nOur Vision: Teams on the platform have access to shared knowledge, processes, and tools, allowing them to quickly build and validate ideas that improve the VA's internal claims process. As a platform team, our success is measured by time-to-value for our partners, ie. the product teams building and iterating on applications in VRO.\n\nRelated/Associated product(s)\n\nThe VRO platform team (#benefits-vro) currently supports these application teams:\n\nContention Classification (#benefits-cft-classification)\nEmployee Experience (#benefits-cft-employee-exp)\nProblem\n\nNo place to build and test ideas to improve VA's internal claims process\n\nAs the OCTO Benefits Portfolio increases its efforts to impact the end-to-end disability benefits lifecycle beyond the Veteran-facing services provided on va.gov, teams with ideas to improve aspects of the VA's internal (employee-facing) claims process have not had a go-to place to quickly build and test those ideas. Prototypes could be shoe-horned into existing codebases, but would not have a chance to scale there.\n\nVRO provides a dedicated space for these product opportunities, tailored to the needs of claims processing tools\n\nClaims data visibility\n\nGetting claims-related data often requires formalized data requests that can take weeks and still return the wrong information. In addition, teams need deep knowledge of the domain to frame a request properly, which dissuades teams from utilizing data for crucial decisions.\n\nVRO aims to provide visibility into real time claim lifecycle events and va.gov claim submission data. (See VA.gov Data Visibility Initiative)\n\nCoordination cost of integrating with VA systems\n\nClaims processing tools rely heavily on interactions with multiple VA systems to retrieve and update relevant claims data. Integrating with these systems often requires extensive coordination across different parts of the organization with different ways of working, as well as navigating the technical and operational complexity of the given system ‚Äì draining time and energy that could otherwise be put into accelerated product development.\n\nProducts within the VRO product suite benefit from system integrations implemented and maintained by the VRO platform team, and made available to the product suite as microservices, including fully documented helper libraries to reduce developer discovery burden\n\nSupport for rapid, iterative product experimentation\n\nMost OIT teams aren‚Äôt currently set up to support rapid software delivery to production in weeks, due to prescheduled six-week cycles. Even when the software is developed, it takes months to get approval to go to production.\n\nThe VRO platform provides a software development environment for partner product teams to deploy to the VA's Lighthouse Delivery Infrastructure (LHDI) platform, including build utilities, DevOps support, and ATO support.\nThe platform team provides processes and tools for releasing software in VRO, documentation & code from past experiments, best practices, and libraries for development, design, and research. Teams will be able to learn from & leverage the work others have done in the benefits space.\nDesired Outcomes\nTeams building applications in the VRO product suite can deploy an MVP to production in weeks, not months\nTeams testing product ideas in VRO have the data they need to validate or invalidate their hypotheses in days or weeks, not months\nUndesired Outcomes\nTeams on the VRO platform take longer to validate ideas than if they were to use another platform.\nTeams building on VRO independently work on redundant or duplicative efforts\nMeasuring Success\nCurrent Objectives\n\nObjective: Continue to mature the platform to support partner teams\n\nKey Result: By December 31, 2023, VRO partner teams will have the ability to perform daily automated deployments to production, independent of the platform team.\n\nObjective: Build Claims Data Visibility\n\nKey Results:\nBy Oct 2023, VRO will have visibility to real time and historical contention history for all claims submitted.\nBy December 31, 2023, VRO will have visibility to all disability benefit form data submitted on VA.gov\n\nObjective: Support the priorities of all teams on the claims fast tracking crew.\n\nKey Result: By December 2023 at least 40% of partners indicate that they would be very disappointed if VRO were no longer available to them."
  },
  {
    "title": "Virtual Regional Office (VRO) Overview ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Virtual-Regional-Office-(VRO)-Overview",
    "html": "Document Purpose: To provide a catalog of VRO offerings along with links to more detailed documentation. This page will be updated regularly to ensure it contains the most current information.\n\nWelcome to the VRO software platform! The VRO team is here to support you as a new partner product team getting up and running in VRO.\n\nüëã Who is the VRO team?\n\nWe are the software platform team for the OCTO Benefits Portfolio, and we support partner teams building products that improve claim processing for digitally submitted (ie. va.gov) claims. Together, these products form the 'Virtual Regional Office' product suite.\n\nWe work to ensure that teams building in VRO have access to shared knowledge, processes, and tools, allowing you to quickly build and validate ideas that improve the VA's employee-facing claims process.\n\nChat with us and follow along in DSVA Slack: #benefits-vro-support.\n\nMeet the team:\n\nOCTO (VA) Enablement Team\nZach Goldfine, Product Owner\nJulie Strothman, Design Lead\nCory Sohrakoff, Engineering Lead\nProduct & Research\nDiana Griffin, Product Manager\nBianca Rivera Alvelo, Designer/Researcher\nSoftware Engineers\nCheng Yin\nErik Nelsestuen\nJosiah Jones\nTeja Naraparaju\nTom Greene\nDevOps Engineers\nMason Watson\nüõ† VRO Features, Tools, and Support\n\nVRO provides a software development environment for Claims Fast-Tracking Crew teams to quickly integrate with existing VA services and deploy to the VA's Lighthouse Delivery Infrastructure (LHDI) platform. In service of this goal, we offer key features relevant to claims processing products, tools to expedite your development process, and the support, guidance, and subject matter expertise of our team members.\n\nüå± VRO is evolving! VRO is a new software platform, and will continue to grow and mature alongside our partner products. We collaborate with product teams to understand your application's needs and evolve VRO services and tools to meet them while incorporating them with the holistic needs of the product suite.\n\nFeatures\nEvent-based, scalable microservice architecture\n\nVRO implements an event-driven architecture with Queue-Processor components that act like an internal microservice, modularizing functionalities so that each can be updated and maintained more easily. These components are connected together using well-tested and stable Enterprise Integration Patterns (EIP) tools (such as Apache Camel) so that we can focus on VRO functionality and less on ‚Äúglue code‚Äù. This approach promotes low software coupling and, as a result, simplifies debugging and maintenance.\n\nLearn more\nReusable software patterns\n\nTo help expedite time-to-deployment and maintain consistency between products, VRO provides reusable software patterns that implement VRO Software Conventions. VRO offers libraries and encourages reuse of software patterns to minimize onboarding, diagnosing, and development time.\n\nVA system integrations\n\nClaims Fast-Tracking products need to access and update claim data in order to deliver desired outcomes, which requires integrating with other VA systems. VRO offers a growing number of integrations with VA systems (such as Lighthouse APIs, BIP APIs, and BGS) and other services (such as Slack). Leveraging VRO's existing integration services could save many weeks (if not months) of work per integration.\n\nLearn more\nDatabase and Redis cache\n\nVRO provides a dedicated Postgres database for persisting (non-PII and non-PHI) data, as well as a Redis cache to temporarily store and track data.\n\nAPI gateway\n\nVRO's API Gateway provides a single location to access all APIs provided within VRO, regardless of implementation language. To expose APIs for each domain, the API Gateway offers a Swagger UI to inspect API offerings, retrieving the OpenAPI spec for the selected API from domain-specific containers and presenting it in a Swagger UI.\n\nLearn more\nTools\nEstablished development process\n\nVRO partner teams can get up and running quickly using our established development process with Gradle utilities and Github Actions workflows to automate code testing, Docker image creation, and housekeeping tasks.\n\nLearn more about:\n\nVRO development process\nGradle utilities\nGithub Actions\nFlexible deployment configurations\n\nFor teams building in VRO, flexible deployment configurations and processes ease deployment under the VA's cATO (continuous authority to operate). The build and deployment pipeline incorporates the required Secure Release process and scanning, which minimizes software vulnerabilities and keeps the software up-to-date.\n\nLearn more about:\n\nHelm Charts for deployment configuration\nDeployment process\nCI CD Workflows\nSecure Release (SecRel) process\nDependabot\nSupport\nSoftware conventions\n\nThe VRO team develops, maintains, and expands our Software Conventions to help partner teams jump start their development, reduce time-to-deployment, and maintain consistency across products in the VRO product suite.\n\nInherited Continuous Authority to Operate (cATO)\n\nTraditional ATO processes can add weeks or months of coordination and overhead before an initial product launch, but VRO benefits from Lighthouse Delivery Infrastructure's continuous ATO (cATO) process to stay in compliance while being able to deploy frequently to production. Products within the VRO product suite benefit further by inheriting VRO's ATO -- we incorporate your product scope into our cATO process so that you don't have to start from scratch.\n\nHands-on support and collaboration\n\nThe VRO team is here to support our partner product teams, from initial idea validation through development and launch, and with ongoing support and maintenance of a stable production environment where your product can grow and scale. We work iteratively to support new requirements and improve the maintainability and sustainability of the ABD-VRO codebase, with built-in DevOps to ensure VRO is operational and scaling as needed and team members available to help onboard partner teams, review Pull Requests, and offer technical support, product guidance, and design standards."
  },
  {
    "title": "VA.gov Data Visibility Initiative ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/VA.gov-Data-Visibility-Initiative",
    "html": "Problem Overview\nExecutive Summary\nProblem\nConsequence\nOpportunity\nGoal\nVRO's role\nEnabling team Q&A\nExecutive Summary\nVA.gov activity data, including disability benefits claim submission data, is functionally inaccessible to Benefits Portfolio product teams, with the exception of a handful of engineers with command line access to query the production database in vets-api.\nOCTO wants to develop a safer, more accessible, and more user-friendly way for teams to access this data.\nThe VRO team is responsible for coordinating this effort via collaboration across the Benefits Portfolio, in particular with the Disability Benefits Experience team(s) who are familiar with the va.gov Postgres database and the needs of engineers working on va.gov benefits products.\nProblem\nVA.gov activity data is currently trapped in a Postgres database in vets-api\n\nTeams working to improve the end-to-end experience of digitally submitted disability benefit claims need access to va.gov activity data (including claim submission data) in order to learn about problems, validate ideas, troubleshoot issues, measure experiments, and iterate on solutions. However, teams can't easily access this data from the database where it's currently stored.\n\nHow is va.gov claim submission data trapped in this Postgres DB?\n\nVA.gov stores all activity in a Postgres database that can only be queried via prod Rails console, and the data can‚Äôt be used by BI tools/etc. to interact with the data. Due to the nature of some of the information (PII data) it is often not able to be logged.\nOnly a handful of Benefits portfolio team members have access to query the production database via the prod Rails console.\n\nWhy not just get the data from the VBA side?\n\nWhile va.gov claim submissions end up in the Enterprise Data Warehouse (EDW, which sits one layer above the source-of-truth Corporate Data Warehouse, CDW) after claim establishment, all of EDW's claims that come from va.gov are slightly mislabeled when it comes to identifying va.gov as their source. While this mislabeling problem is currently being investigated in hopes of resolving it point forward (outside the scope of this effort), the problem will still apply to historical claims data (since Nov 2021) in EDW.\nEven disregarding that EDW data is slightly mislabeled, no one in OCTO has access to EDW; queries must go through VBA's PA&I team\n\nWhat about getting the data via Kafka streams? The current thinking is that in the long-term, the ideal would be for the VES Event Bus kafka service to act as the source for this data, however, OCTO would like to implement an intermediary solution without waiting for this option to solidify.\n\nConsequences\nVa.gov activity data in its current state is functionally inaccessible for the majority of Benefits Portfolio teams' needs.\n\nWhat's wrong with getting this data from the Postgres DB?\n\nHaving engineers going into the prod Rails console and querying for data risks potentially impacting actual production and end-users, or altering real production data\nIt's hard to look at large amounts of this data, because of the risk of overloading the actual production system (due to size of the queries)\nWhile only a small number of Benefits Portfolio engineers can access this data (Yang, Luke, Steve, and Kyle Soskin are the ones we're aware of), increasing that number would violate the principle of least privilege\nThe folks who have access to query the database are not in support roles tasked with fielding requests for data, so there's no official way to ask for a data pull\n\nWhat's wrong with requesting this data from PA&I?\n\nGiven that PA&I fields requests from all across VBA and OCTO, it can take weeks or months for data requests to be fulfilled.\nFurthermore, because all of VBA's reporting is slightly mislabeled when it comes to va.gov as a claim source, it's not possible for PA&I to report on claims from va.gov with full confidence of accuracy.\nOpportunity\nOCTO needs a way for Benefits Portfolio teams to retrieve va.gov activity data in a safer, more accessible, and more user-friendly way:\nwithout using a prod command line to access the va.gov Postgres database,\nwithout lengthy turnaround times on the scale of weeks or months,\nand with confidence that the data we're seeing covers all va.gov 526 claims.\n\nIdeally, non-engineers who need visibility into the data will be able to retrieve it for themselves without having to go through an engineer. Building upon that ideal scenario, we can imagine enabling the configuration of data dashboards to meet teams' specific and ever-present needs for data analysis and insights. And in a perfect world, this va.gov activity data would be matched up to \"downstream\" claim lifecycle data from EDW (available via kafka event topics) so that teams could follow claims from submission on va.gov through to claim completion in VBMS.\n\nNote that the VA has a larger effort underway related to reducing/eliminating va.gov engineers' dependencies on interacting with the prod Rails console (as shared by Bill Chapman in the July Benefits Portfolio engineering all-hands) ‚Äì our focus on data visibility represents just one aspect of this overall effort.\n\nGoal\nBy December 31, 2023, Benefits Portfolio teams will have visibility to all disability benefit form data submitted on VA.gov.\n\nMore context:\n\nThe whole benefits portfolio should be part of discovery of needs even if we choose to implement early solutions that focus on a particular team or crew\nAt minimum, \"visibility\" = Benefits portfolio engineers can pull data via a secure solution (e.g. read-only credentials, scoped only to claim data) that doesn't require them having prod console access to the va.gov Postgres DB.\nCurrently, \"all disability benefit form data\" is a hypothesis about what will be valuable to the portfolio teams. For now, we can assume that we're talking about 526EZ form submission data (including historical submission data), but we will refine expectations of what data to include based on further discovery that defines and prioritizes data visibility needs across Benefits Portfolio teams. Other types of data that may be prioritized include va.gov activity and error data.\nVRO's Role\nGiven VRO's mission to make it easy to build software to improve the VA's internal claims process, with particular emphasis on our vision of allowing teams to quickly build and validate product ideas, the VRO team is well positioned to lead the effort of identifying a pathway to deliver value in this problem space.\n\nOur VA partners are asking the VRO team to:\n\nBe responsible for coordinating this work. If VRO's research or roadmap requires work or input from other teams (such as DBEx), that's totally fine.\n\nExpectations:\nVRO should make it as easy as possible for other teams to stay informed and complete relevant tasks.\nThis work should (as always!) follow OCTO's principle of working in the open. All chatter about this project should be in open channels for folks across the portfolio (i.e. #benefits-vro-support #benefits-portfolio #benefits-cft or in a place that‚Äôs new / opt-in / not 1:1 DMs).\n\nCollaborate with teams across the Benefits Portfolio to define and prioritize the needs related to visibility of va.gov data. There are a variety of needs related to claim data across the portfolio, both at submission and beyond. Some are related to monitoring in the moment, and some are more driven by product/design, researching historical data. The Disability Benefits Experience (DBEx) teams, given their knowledge of the va.gov database, should be primary collaborators on this effort.\n\nRequired output:\nRecommend a prioritized set of needs as initial and subsequent areas of focus for the teams' efforts between now and the end of the year\nExpectations:\nSet up a touchpoint/meeting between VRO and DBEx team (or teams, but likely starting with DBEx Team 2) by first week of August\nBuild out a comprehensive list of needs (and their relative priority/frequency)** through end of August and use that list to define our roadmap.\n\nCollaborate with DBEx on shaping and scoping solutions to the portfolio's prioritized needs.\n\nRequired output:\nRecommend a roadmap to deliver on the priority needs, including determining which team will implement which portions of which solutions (assuming the solutions include elements that span va.gov and VRO).\nExpectations:\nVRO should aim to implement the solution as far as possible, reducing dependency on DBex or any other teams if possible.\n\nImplement against the agreed upon roadmap.\n\nExpectations:\nAn MVP solution should be started by ~ end of August\nA note about the long-term\n\nOur product owners recognize that a \"productized\" version of available data via dashboards and other tooling is a product in itself, somewhat separate from VRO as a platform. We are empowered to recommend the best structure for long-term maintenance and expansion of this work stream, however, the expectation is that VRO will lead the initial shaping and roadmapping of solutions to this set of problems, identify a path to quick value, and deliver on it.\n\nEnabling Team Q&A\n\nQ: Who are the ultimate decision-makers about what can/can't be done with the va.gov data and what can/can't be built on the va.gov side? (We know it's all OCTO; is there anyone we don't know yet who's a key decision-maker?)\n\nA: Need to loop in/keep aligned with the ATO team for Lighthouse and va.gov (Jesse House has been the person Steve has talked to; #platform-security-review is the team's slack channel). Do the same with VRO's cATO contacts.\n\nQ: Is it accurate to think of this effort as addressing one slice of the overall \"Steve and Bill idea\" (ie. Steve Albers & Bill Chapman's exploration of internal APIs and/or other solutions to reduce/eliminate dependencies on accessing the va.gov database via prod console)?\n\nA: Short answer, no. They're related problems but we don't want to create dependencies between these efforts.\n\nQ: Technical question: is there an existing backup of the va.gov database?\n\nA: No. It's hard. System written in almost NoSQL fashion... it's not trivial to extract data from the DB; have to decrypt. Would be potentially a performance impact because it needs to run in the same space as production.\n\nQ: For Steve & Cory: How baked are your ideas on where to start (e.g. replicate data to an S3 bucket)? We don't want to go back to square one if you're already feeling confident that there's an obvious first step we should take.\n\nA: Kyle Soskin is writing up a best practices guide on using Sidekiq for backend queries and we should wait for that before making decisions. There's a backlog ticket for CE teams (DBEx 2) to do this monthly data extraction -- but maybe this is a short-term thing, we might only want it once! It makes sense for them to own it, but in terms of capacity it might make sense for VRO to build it, if we can get to it sooner.\n\nQ: When we talk about access as a problem, is [technical] skill part of the problem? For example, could we assume that everyone who needs access to this data can use SQL?\n\nA: Maybe for an MVP but since part of the problem is making the data accessible beyond engineers, probably can't assume e.g. all PMs are SQL fluent.\n\nQ: What would overthinking this look like?\n\nA: Don't over-engineer in the beginning. For example, if we felt like a JSON file or something would be insufficient and assumed we need complex data viz to solve the need. We CAN start small!\nWe don't need all data in real-time. Figuring out which data is part of the question for us! Picturing a list that lays out, \"We need this, this often, and this is the person who needs it\" -- then prioritize this list.\nThere's also underthinking it! Doing a data pull, dropping it on Sharepoint and calling it done is not taking holistic enough view of the problem. There are many needs!\nA good deliverable would be laying out what we can/should do now with sidekiq and which things should wait until data is available from EventBus.\nWe're excited about the potential linkage between claim submission data and downstream claim lifecycle data (but don't start there!)"
  },
  {
    "title": "Updating Contention Classification DC Lookup Table ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Updating-Contention-Classification-DC-Lookup-Table",
    "html": "Getting the lookup table updated\nSubmitting Updates\nPlease make updates to the lookup table in a separate PR without any additional changes to application logic!\nEnsure the title of the PR indicates that the lookup table is being updated and include the version number\nthis will make sure that users can see the lookup table updates from the releases section on github!\nSpreadsheet Versioning\nCurrently we store the mappings in a spreadsheet HERE.\nThere are different sheets available for different versions.\nLocking\n\nMake sure the latest version intended for release has been \"locked\" so no further edits can be made to the sheet. \n\nExport the desired sheet as .csv\n\nGithub\n\nMake sure the .csv filename follows the file format\n\n\"Contention Classification Diagnostic Codes Lookup table master sheet - DC Lookup {{ TABLE_VERSION }}.csv\"\n\nAdd the new .csv file to domain-cc/cc-app/src/python_src/util/data\n\nUpdate table_version.py to include the updated version number\n\nIf the CI passes, you should be good-to-go! Have a great day üí´"
  },
  {
    "title": "Update vets api parameters for VRO ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Update-vets-api-parameters-for-VRO",
    "html": "For VRO's MVP (where the va.gov backend calls the VRO API), vets-api proxies requests through to VRO. There are entries in AWS Parameter Store in the dsvagovcloud AWS account that influence how that integration works.\n\nThe relevant params are:\n\n/dsva-vagov/vets-api/dev/vro/api_key\n/dsva-vagov/vets-api/sandbox/vro/api_key\n/dsva-vagov/vets-api/sandbox/vro/url\n/dsva-vagov/vets-api/staging/vro/api_key\n/dsva-vagov/vets-api/staging/vro/url\n/dsva-vagov/vets-api/prod/vro/api_key\n/dsva-vagov/vets-api/prod/vro/url\nAccess\n\nOn our team, currently @cweagans has access to update those params. To get access to make these updates yourself:\n\nAttend a platform onboarding session.\nFill out this form. For \"Desired AWS Access\", it should be something along the lines of \"I need to create/update/delete the following parameter store values\" (and paste in the params from above)."
  },
  {
    "title": "Ubuntu VM Setup ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Ubuntu-VM-Setup",
    "html": "This guide is a supplement to Local Setup for getting a development environment running on a Ubuntu Linux VM.\n\nRather than getting a development environment working on Windows or WSL2, you may find it easier to work within a virtual machine. The following steps will guide you through requesting and setting up VirtualBox on you VA laptop.\n\nNote: While connected to the VA VPN, some HTTPS requests are intercepted, and the network presents its own certificate. This causes connection failures within the virtual machine. A simple solution is to disconnect from VPN when network issues arise.\n\nRequesting VirtualBox\nNavigate to yourIT and select the \"Need Something New? Submit a Request\" option.\nChoose \"IT Software or Hardware Request\" and fill the request form.\nFor \"Non-Medical Software Details\" enter the following as the request for VirtualBox 7.xx (or the latest version): Please install VirtualBox 7.xx software. https://trm.oit.va.gov/ToolPage.aspx?tid=6679, https://www.virtualbox.org/wiki/Downloads\nFor \"Justification\" enter the following: VirtualBox is required to conduct official VA business, specifically it facilitates software development for VA.gov, VRO, and other OCTO efforts.\nAfter filling the request form, click the \"Order Now\" button.\nIT will contact you in order to remote into your laptop and install the software. It may take several days for them to service your request.\nInstalling Ubuntu 22.04\nDownload the Ubuntu 22.04 ISO from here\nFrom VirtualBox, select the \"New\" button.\nName your VM and select the ISO Image before moving to the next screen.\nVirtualBox has an unattended installation feature that will install and configure the guest OS for you. On the screen, you may setup a username, password, and hostname for your VM. You may also want to install Guest Additions if given the option.\nSelect a reasonable amount of memory for the VM. (For example, I selected 8192 MB for a host laptop with 16 GB of memory.)\nSelect a reasonable amount of processors for the VM. (For example, I selected 4 for a host laptop with 8 CPUs.)\nOn the disk screen, allocate a virtual hard disk with enough space for development. (I created an 80 GB disk.)\nOn the final screen select the Finish button. VirtualBox will install and configure the guest OS for you.\nAfter the installation is finished, you will want to ensure that Enable Nested VT-x/AMD-V is selected in your VM. Go to Settings > System > Processor and select the approprate checkbox.\nStart the VM and log in.\nYour user cannot use sudo, but the root account is conifigured with the same password as your username. To add your user to the sudo group, from a terminal type su and enter your password. Then, usermod -aG sudo <your username here>. Type exit to leave the root shell. Afterward, you may need to log out and log in again for sudo to work.\nDevelopment Prerequisites (adapted from LHDI Developing on Non-Mac OS)\nGit\n\nInstall git:\n\nsudo apt install git\n\n\nConfigure your name and email for git:\n\ngit config --global user.name \"FIRST_NAME LAST_NAME\"\ngit config --global user.email \"MY_NAME@example.com\"\n\nVS Code (optional)\n\nDon't install VS Code from the Ubuntu Software app. There are some issues with it that cause problems later with Gradle. Download the Linux x64 .deb file from here instead. Install it with:\n\nsudo apt install <VS Code .deb file>\n\nJava\nsudo apt install openjdk-17-jdk\n\nPython Setuptools\npip3 install --upgrade pip setuptools\n\nDocker\n\nFollow the official instructions from Docker:\n\nInstall Docker Engine on Ubuntu | Install using the apt repository\nLinux post-installation steps for Docker Engine\n\nDocker plugins (such as docker-compose) are not put into the PATH by default, you can link docker-compose to /usr/local/bin to make it easier. You can do similar for other plugins as needed.\n\nsudo ln -s /usr/libexec/docker/cli-plugins/docker-compose /usr/local/bin/\n\nHadolint\nDownload the latest hadolint-Linux-x86_64 release.\nRename the file hadolint.\nchmod +x hadolint\nsudo mv hadolint /usr/local/bin\nSpectral\n\nInstall the binary following the official instructions with this command:\n\ncurl -L https://raw.github.com/stoplightio/spectral/master/scripts/install.sh | sudo sh\n\nShellcheck\nsudo apt install shellcheck\n\nPre-commit\nsudo apt install pre-commit\nRun pre-commit install from your abd-vro project to install the git hook.\nGo\n\nGo is required by some pre-commit hooks. Install it following the Linux instructions here. However, you will need to prefix both commands in step 1 with sudo for it to work properly.\n\nAdr-tools\nsudo apt install adr-tools\n\nKubectl\n\nFollow the offical Install using native package management instructions.\n\nHelm\n\nFollow the From Script offical installation instructions.\n\nIstioctl\n\nFollow the download instructions here. Though in addition to step 2, you can permanently add istioctl to your path with the following command:\n\necho 'export PATH=$HOME/.istioctl/bin:$PATH' >> ~/.profile\n\nMinikube\n\nFollow the offical installtion steps for Linux here\n\nCodeclimate\n\nFollow the Anywhere installation steps. (This requires Docker to have already been installed.\n\nMkdocs\nsudo apt install mkdocs\n\nScala\n\nFollow the Linux x86-64 instructions here.\n\nLHDI Lightkeeper\n\nFrom the offical installation instructions:\n\nInstall jq, which is a dependency of the installer script: sudo apt install jq\ncurl https://$GITHUB_ACCESS_TOKEN@raw.githubusercontent.com/department-of-veterans-affairs/lighthouse-di-lightkeeper/main/install.sh | bash -s $GITHUB_ACCESS_TOKEN\n\nNote: For this step to work, GITHUB_ACCESS_TOKEN must be configured as described in Local Setup."
  },
  {
    "title": "Troubleshooting Local Environment ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Troubleshooting-Local-Environment",
    "html": "This troubleshooting page is created to address some common issues for local environment build\nFailing at build without any issues present: In order to build successfully make sure you have the latest code as well as abd-vro-dev-secrets and run scripts/setenv. Since some of the environment variables need to be overridden, it's advised to open a new terminal and run the build in the new terminal (setnev.sh does not override)."
  },
  {
    "title": "Software Conventions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Software-Conventions",
    "html": "VRO Software Conventions\n\nVRO is opinionated about the technical architecture (i.e., event-driven and Apache Camel-based).\n\nThis page describes software conventions that should guide new development in VRO to help maintain consistency and expedite time-to-deployment, not to mention to evoke a great developer experience.\n\nBackground\n\nVRO has developed an MVP (in Dec 2022) and iMVP (in Feb 2023). The keyword is \"MVP\". VRO needs some TLC:\n\nThe code works but should not necessarily be used as the basis for new code.\nNew teams/engineers will inevitably copy-and-paste to develop new functionality.\nVRO code should follow best practice; follow conventions and be consistent (which improves maintainability); be modular with minimal coupling (to help ensure components don't inadvertently affect each other as new features are added); and be scalable to handle high volumes.\n\nSo I propose:\n\n(Mostly done) We document instructions, set coding conventions, and/or incorporate working example code into the VRO codebase.\n(In progress) We refactor code for reuse, maintenance, and modularity while keeping existing VRO capabilities functional. We can leverage Camel routes to fork a secondary path that runs refactored code in parallel with existing code, and then compare the results. Once results are the same for many claims, then we can be confident that our refactored code can replace the existing code.\n\nOther context:\n\nVRO is one of the first to use the LHDI platform, LH SecRel pipeline, and LH cATO process, which are gradually being developed, expanded, and matured.\nContributions to and management of the VRO codebase will change hands over time.\nConsistent code implies easier debugging and maintenance across domains. Coding conventions help to ensure consistent code.\nTerminology\n\"domain\" = area of functionality to be added to VRO, such as va.gov integration (VRO v1), MAS integration (VRO v2), or Contention Classification (CC)\n\"MQ\" = message queue, such as RabbitMQ, provides a message bus, enabling communication across all containers\n\"container\" = a Docker container is used to encapsulate functionality into modular, scalable, stateless components\nVRO's container categories:\nApp container: there's a default App container that exposes an API using Java Spring Web and combines Java-based domain-specific API endpoints into a single API\nA domain can expose its own API but some manual updates will be needed to expose the API via the API Gateway\nWorkflows container: defines domain-specific workflows using Camel Routes (in Java) or Prefect in Python; a domain should have its own Workflows container; typically a single Workflows container is needed per domain regardless of the number of workflows.\nThe workflow library should be able to send requests to Service containers using the message queue.\nService container: holds one or more microservices that implement (in any language) step(s) in a workflow, providing some basic, stateless functionality. A multi-step workflow is expected to use multiple service containers. These services are typically domain-specific but they can be generalized to be domain-independent and reusable by multiple domains.\nPlatform container: offers a domain-independent resource or service, such as containers that run the RabbitMQ, Postgres DB, and Redis Cache services.\nA container may be composed of several VRO Gradle modules (or subprojects).\nA Gradle module is a folder with a build.gradle file. A module is used as a library or for a container (which may import libraries). A container module uses a container.*-conventions Gradle plugin (search for id .*container.*-conventions in build.gradle files to identify all the container modules).\n\nExample of VRO Container Categories:\n\nFolder Structure\n\nTop-level folders are for domain-independent code, except for domain-... folders where domain-specific code resides. VRO software resides in these folders:\n\napp: VRO entrypoint; pulls in domain-specific api-controller modules\nconsole: VRO shell console for diagnostics (inspired by Rails Console)\ndb-init: initializes and updates the database schema\npostgres: defines a customized database container\nshared: utility and library code shared across domains (Shared libraries)\nsvc-*: domain-independent microservices, typically for integrations (Shared microservices)\nmocks: mock services for development and testing, typically mocking external APIs\ndomain-ABC: domain-specific code with subfolders:\nABC-api-controller: module that defines domain API endpoints and controller\nABC-workflows: defines domain workflows (i.e., Camel Routes)\nsvc-*: domain microservices supporting the workflows\n\nOther top-level folders contain configurations for building and deploying VRO.\n\nShared libraries\n\nVRO offers utilities and DB classes as shared libraries for use by domain-specific classes.\n\nshared/api folder: (Java) general API models\nshared/controller folder: (Java) Controller utilities, including InputSanitizerAdvice\nshared/lib-camel-connector folder: (Java) Camel utilities, including a Spring Configuration for Camel and CamelEntry\nshared/persistence-model folder: DB utilities, including Java Entities\nPython MQ utilities to facilitate interaction with the MQ\n\nThese libraries should have minimal dependencies to help avoid dependency conflicts (aka \"dependency hell\").\n\nBe cautious when updating these files since they can inadvertently affect multiple domains that use the libraries. (Versioning the libraries can help avoid this scenario but adds significant complexity and maintenance overhead.)\n\nShared microservices\n\nVRO offers integrations with external services (such as Lighthouse API and BIP API) via shared microservices, which are deployed in separate containers (for independent updating, down-time handling, and scalability). These domain-independent shared microservices are in the top-level svc-... folders. Other (non-integration) cross-domain microservices can be added as long as they are domain-independent.\n\nNote that external services can go down, so the domain's workflow should incorporate error-handling and retry mechanisms for workflow robustness. Do not rely on RabbitMQ-specific retry mechanisms (RabbitMQ Microservice reliability or any MQ-specific features) in case VRO moves to using some other MQ; instead, use Camel EIPs to do retries as part of the workflow.\n\nAdding a New Domain\n\nTo enable capabilities for a new domain in VRO, the partner team will typically implement a Workflows container and several Service containers, as well as adding API endpoints by creating an API-Controller module.\n\nCode for one domain should not reference code in another domain. Keep domains decoupled, including keeping containers for different domains independent of each other. This allows containers for one domain to be restarted without affecting unrelated containers.\nThe API-Controller module should rarely need to be updated once created, whereas the Workflows and Service containers are restarted more frequently.\nAdd a domain-... folder at the top level of VRO's codebase. All code for the domain should reside in subfolders under this domain folder. For an example, refer to the domain-xample folder.\nDomains should include automated tests that validate the workflow; end2end tests. These tests can be called manually in the local developer environment, and automatically during CI by GitHub actions.\nAPI Endpoints and Controller library module\n\nUnder the domain folder, add a ...-api-controller subfolders and populate them with Java code. Because the folder name is used as the jar artifact file name, the folder name should be unique, regardless of where it is in the codebase.\n\nThe app container module (in the top-level app folder) pulls in each domain's api-controller module to present a single API spec.\nNote that API endpoints should rarely be deprecated. Instead, use version numbers in the URL. https://www.mnot.net/blog/2012/12/04/api-evolution\nThe controllers should be very minimal in terms of logic. It should check the request payload and immediately inject it into a workflow. This enables error-recovery and testing, where the payload can be injected directly into the workflow, bypassing the need for an API and Controller.\n\nAPIs implemented in non-Java languages are served by VRO without having to implement a ...-api-controller subfolder -- see API Gateway.\n\nShared library module\n\nDomain-specific shared libraries can be implemented in a single subfolder or multiple subfolders. You can use any folder name, such as constants, util, or dto. These subfolders define library modules that can be imported by other modules in the domain.\n\nIn the domain-xample example, the shared module illustrates defining shared domain-specific constants, DTO (data transfer object) classes, and utility classes.\nNote that DAO (data access objects) belong in the top-level persistence folder because all domains share a DB ‚Äì see the Add DB Entities section.\nOnce the code is stable, domain-independent classes can be moved to one of the shared libraries for use by other domains. Ensure these classes are very well tested with unit tests and clearly documented.\nWorkflows container module\n\nWorkflows are implemented within a single ...-workflows subfolder. Because the folder name is used as the Docker image name, the folder name for the container should be unique, regardless of where it is in the codebase.\n\nFor Java, workflows are defined using Camel Routes and Camel Components, which are building blocks for implementing EIPs.\nThe Workflows container should implement only Camel Routes and basic processing (workflow logic or data object conversions), not the microservices themselves.\nHowever for rapid prototyping, early implementations can include microservices in the single container, with the understanding that the microservices will later be extracted into their own containers (as described in the next section).\nA workflow step can call a method on a Java Bean, a custom Camel Processor, etc. to perform basic logic to determine the next processing step. Nothing (except maybe a rejection from a manual code review) prevents that component from being big and complex. To avoid this situation, split the workflow into simple steps, which will facilitate extracting a step into an appropriate service container later. In the long-term, this will make the workflow easier to understand (workflow logic and transmitted data is encoded at the workflow level), comprehensive (because hidden substeps become an explicit step), and easier to modify (by modifying routes instead of service internals).\nWorkflows should split the processing steps into logical stages, where each stage is one or more Camel Routes. The benefit of segmenting the workflow is to facilitate testing and error recovery, where for example a payload (a claim) can be injected into any stage of the workflow.\nPrefer to design workflows for asynchronous (event-based) processing, leveraging MQ queues to trigger the next step in the workflow. In addition to decoupling workflow steps, this facilitates being able to (manually or as part of another workflow) inject a payload in strategic intermediate steps within the workflow.\nFor Python, Prefect library can be used instead of Camel. To send requests to Service containers, Prefect tasks should send the request over the MQ.\nService (microservice) container module\n\nA microservice is implemented within a svc-... subfolder. There should be a separate folder for each microservice. Because the folder name is used as the Docker image name, the folder name for the service should be unique, regardless of where it is in the codebase.\n\nFor scalability, a microservice is encapsulated in a container so they can be replicated as needed.\nMicroservices can be easily scaled by having replicated microservices listen to the same MQ queue; more effort is required to scale using a REST API\nqueue-based asynchronous model vs REST API\nA microservice should be stateless, idempotent, and basic (e.g., implementing one step in a workflow).\nA microservice listens for JSON requests on the MQ and always sends a JSON response back to the client via the MQ. Benefits:\nNo REST API to set up and manage; less DevOps maintenance (e.g., exposing ports for REST API). Fewer libraries to include implies fewer libraries to maintain for SecRel.\nMakes services easier to test since the input and output are always JSON strings.\nJSON String are easily parsed in any programming language. There is no JSON data structure enforcement unless a JSON Schema is specified and applied.\nA microservice response is always expected by the client to ensure the request was received and processed (or errored). The client can handle the response synchronously (blocks the workflow while waiting for a response) or asynchronously (workflow continues and reacts whenever a response is received).\nhttps://developer.ibm.com/articles/how-messaging-simplifies-strengthens-microservice-applications/: decoupled communication, pull instead of push workload management, simplified error handling, security is configured in central MQ instead of potentially inconsistently in each microservice\nThe expected JSON structure for a microservice response is very flexible. The only requirement is a header field:\n{\n  \"someRequestCorrelationId\": 987654,\n  \"anyField\": { \"someSubField\": \"anyValue\" },\n  \"anotherField\": 1234.5,\n  \"header\": {\n    \"statusCode\": 200, // use HTTP status codes\n    \"statusMessage\": \"error msg or optional msg useful for debugging/observability\",\n    \"jsonSchemaName\": \"optional domain-specific JSON Schema name for msg body\",\n  }\n}\nTo convey an error or exception to the microservice client, the response JSON String should contain the fields statusCode and statusMessage.\nThe statusCode integer value should correspond to an HTTP status code. If this key is not present, then the client can assume status code = 200.\nThe statusMessage String value should convey some diagnostics and preferably actionable information. This should be present for a non-success statusCode to provide details for diagnostics.\nImplement business logic in the Workflow container.\nDo not rely on RabbitMQ-specific retry mechanisms (or any MQ-specific features) in case VRO moves to using some other MQ. Handle retries as part of the Workflow, especially since the retry strategy (e.g., constant-time retry 3 times, exponential backoff, send Slack notification for each retry) will likely depend on the domain. Check the shared libraries or domain-specific code for implemented retry strategies that can be reused.\nA microservice should not call another microservice ‚Äì implement that as steps in the workflow when possible.\nExcept for communication via the MQ container, a microservice should avoid interacting directly with Platform containers. If DB values are needed, have the Workflow load it from the DB and feed that as input to the microservice. If the service needs to write to the DB, have the service output the data back to the Workflow container, which would write to the DB. This facilitates unit testing (so that a mock DB is not needed) and flexibility (e.g., output data can be sent to logs in addition to the DB).\nIntegrations with external APIs (VA services) should be implemented as a microservice in order to better isolate and handle external API problems. Additionally it should be implemented in a general manner in order to promote it to a shared microservice.\nThe microservice should indefinitely retry connecting to external services rather than exiting and causing the container to fail. Otherwise, when the microservice is deployed to LHDI (where mock services don't exist), the microservice will fail and Kubernetes will keep retrying to start the container. If the microservice loops indefinitely retrying to connect to the external service, this would avoid Kubernetes from unnecessarily restarting the container since the problem is the connection, not the microservice container itself.\nIf there is a temporary network or external service issue, the microservice container should not be restarted and interrupt/cancel other activities/processing occurring in the microservice.\nAnother reason to keep the container running is to enable diagnostics within the running container in case there is some credential or login issue when deployed to LHDI. If the container keeps restarting, it's challenging to log into the container to diagnose the problem.\nIf using Java Spring to connect to the external service, you get this for free because Spring will automatically retry connecting.\nIdeally there is one microservice per container, but if several microservice are very basic, then those microservice can be in a single container to conserve resources.\n\nWhen implementing the microservice in Java, Groovy, and other Java-based languages:\n\nTo listen to the queue(s), use a RabbitMQ client with the help of Spring ‚Äì see module svc-xample-j.\nOr use a Camel Route to listen to the queue(s), like the Workflow container module. Use classes in the :shared:lib-camel-connector module to facilitate this.\n\nWhen implementing the microservice in other languages, such as Python or Ruby :\n\nUse a RabbitMQ client to listen to the queue(s). There's likely common code to use the client, so set up some mechanism (like a common folder) to share that code across services for easier maintenance and updating. (TODO: create a common library for Python)\nAll together now\n\nOnce the above pieces are implemented, a call to the endpoint flows as follows:\n\nVRO client sends a request to API endpoint.\nController parses the request and sends it to the MQ via the CamelEntrance utility class using the queue named derived from the endpoint URL string. (This convention avoids having to share queue name constants across containers.)\nThe controller can choose to send a quick response to the client or wait for some result from the triggered workflow, depending on the semantics of the endpoint.\nThe Camel Route (that is listening on the queue) initiates the associated workflow given the request. At the termination of the workflow, the last response message is sent back to the controller.\nOne step at a time, the workflow sends JSON-string messages to specific queues (or topics). (Again, use consistent queue naming conventions to avoid synchronizing these constants across containers.)\nA service (that is listening on the specific queue or topic) performs its function and returns an output, which is typically used as input to the next step in the workflow.\n\nFigure: Interfacing with MQ from various components \n\nRunning the xample domain microservices\n\nTo run the xample microservices, add xample to the COMPOSE_PROFILES environment variable so that the xample microservices are started in order for the xample-resource POST endpoint to succeed.\n\nexport COMPOSE_PROFILES=\"$COMPOSE_PROFILES,xample\"\n\nAdd DB Entities\n\nSince the DB is shared across domains, the associated DB code resides at the top-level persistence folder, rather than the domain-specific folder.\n\nUpdates to the DB schema require adding a Flyway migration (in the db-init folder)\nDB models should be consistent across domains and clearly documented with column comments. Great care should be taken to prevent the logic in one domain incorrectly modify data values in another domain.\nDB ORM models should reside in the persistence folder in the appropriate subfolder. These classes do not have to be in sync but they should be consistent with each other across languages. Note that these classes can be used by any domain.\n\nNote: A microservice should avoid interacting with the DB directly ‚Äì a workflow should act as the intermediary (see the DB-related bullet in the Service (microservice) container section). With that said, if direct service-to-DB interaction is desired, use the following guidance.\n\nFor interacting with the database via Java :\n\nAdd associated Java Entity classes in the model subfolder for use by workflows or some Repository class\n\nFor interacting with the database via Python :\n\nAdd associated SQLAlchemy ORM classes in the sqlalchemy subfolder\n\nFor interacting with the database via Ruby on Rails :\n\nAdd associated Rails ActiveRecord classes in the rails subfolder\nOther Details\nMessage Queue (RabbitMQ)\nThe payload or message body is a JSON string.\nRequests/calls/messages can be synchronous or asynchronous.\nMQ queue names are constants that must be shared or synchronized across containers. Using naming conventions reduces the number of constants to be shared (via a library or environment variables).\nIn order for Camel to automatically marshal/unmarshal DTO classes for transmission in the MQ, either define the class in the gov.va.vro.model package (or nested package) or add to the vro.camel.dto-classes (in conf-camel.yml).\nConfiguration Settings and Environment Variables\nConfiguration settings for software settings within a container\nscripts/setenv.sh for environment variables\nPrefer to add them to application*.yml (for Java) or settings*.py (for Python). Those files allow different setting values per deployment env. Adding environment variables incurs the cost of keeping docker-compose.yml (for local development) and helmcharts (for each LHDI deployment environment) updated.\nValid reasons why a setting should be in the setenv.sh file:\nA setting to support local development (and automated end2end testing)\nSecret credentials like username, password, private token. Only fake secret values belong in this file. Sensitive non-production values are in the abd-vro-dev-secrets repo. Critical production values are set within the Kubernetes deployment environment -- see Secrets Vault.\nA setting that must be the same or shared across containers\nFeature flags: TBD\nSpringBoot Profiles\nSee https://www.baeldung.com/spring-profiles\nAnd PR Use @ActiveProfiles consistently #831\nMerging Workflows Containers\n\nTo conserve infrastructure resources, lightweight workflows from a domain can be combined with workflows from other domains to reside in a single container. A lightweight workflow is defined as one that uses very little memory and cpu. The workflow code should be stable and expect no changes between major VRO deployments (e.g., when the VRO API is updated). Combining workflows across domains is easily done without any code change by treating the workflow module as a library (instead of a container) and importing it into a multi-domain workflows container, which gets deployed instead of the domain-specific container ‚Äì similar to how the app container imports api-controller modules from several domains.\n\nVersioning\n\nSemantic versioning is used for VRO releases.\n\nAll code (all Gradle modules) have the same version when a code release is created.\nA deployed container uses a Docker image tagged with a specific release version. Since containers may be independently updated, deployed containers may refer to different versions.\nPort numbers\n\nVRO platform services use the typical port numbers for the particular service (usually less than port 10,000)\n\n5432: Postgres DB service\n5672: Rabbit MQ service\n15672: RabbitMQ Management UI\n6379: Redis cache service\n\nThe API Gateway uses the following:\n\n8060: API Gateway\n8061: health check port for the API Gateway\nPorts for APIs within VRO\n\nVRO domains offering APIs use this port numbering convention:\n\n81Nx = VRO microservice ports, where N is an index\n\nThe VRO (Java) App uses the following:\n\n8110: VRO App API\n8110: health check port for the VRO App\n\nThe (Python) Contention Classification uses the following:\n\n8120: Contention Classification API\n8121: health check port for Contention Classification\nPorts for VRO microservices\n\nVRO microservices use this port numbering convention:\n\n10NNx = VRO microservice ports, where NN is an index\n\nexample: 1010x = ports used for svc-lighthouse-api microservice\nexample: 1020x = ports used for svc-bgs-api microservice\n\n10NN1 = health check port for microservice NN\n\nexample: 10101 = health check port for svc-lighthouse-api microservice\nexample: 10201 = health check port for svc-bgs-api microservice\nexample: 10301 = health check port used for svc-bie-kafka microservice\n\nVRO microservices only need to connect to RabbitMQ and do not typically need to expose any service ports, except for health checks.\n\nMock services use this port numbering convention:\n\n20NNx = mock service ports, where NN is an index\n20NN1 = health check port for mock service NN\n\nSo the following mock services would use these ports:\n\n20100: mock Slack\n20101: for health check\n20200: mock Lighthouse API\n20201: for health check\n20300: mock BIP Claims\n20301: for health check\n20310: mock BIP Claim Evidence\n20311: for health check\n20500: mock BGS API\n20501: for health check\n\n(Note that the 2 BIP mocks use 2030x and 2031x to denote high similarity.)\n\nTo see if a port is already being used search the code base for usages of that port.\n\nRunning Xample Domain containers\n\nA xample-integration-test GH Action workflow demonstrates an end-to-end test, from VRO API request to microservice.\n\nTo manually test svc-workflow and svc-xample-j locally, use Docker Compose:\n\nsource scripts/setenv.sh\n# Build all containers\n./gradlew docker\n\n# Start the relevant containers\n./gradlew :dockerComposeUp\n./gradlew :domain-xample:dockerComposeUp\nCOMPOSE_PROFILES=\"\" ./gradlew :app:dockerComposeUp\n\nOpen a browser to http://localhost:8110/swagger and go to the POST ‚Äã/v3‚Äã/xample-resource section and open it. Click \"Try it out\". In the Request body, replace null with:\n\n{\n  \"resourceId\":\"1234\",\n  \"diagnosticCode\":\"J\"\n}\n\nNote: diagnosticCode must be J in order for the request to be routed to the svc-xample-j microservice, as coded in Xample's Camel route.\n\nClick Execute. The response code should be 201 with response body:\n\n{\n  \"resourceId\": \"1234\",\n  \"diagnosticCode\": \"J\",\n  \"status\": \"PROCESSING\",\n  \"statusCode\": 0\n}\n\nThe above can also be done with curl:\n\ncurl -X POST \"http://localhost:8110/v3/xample-resource\" -H \"accept: application/json\" -H \"Content-Type: application/json\" \\\n  -d '{\"resourceId\":\"1234\",\"diagnosticCode\":\"J\"}'\n\n# The API response:\n{\"resourceId\":\"1234\",\"diagnosticCode\":\"J\",\"status\":\"PROCESSING\",\"statusCode\":0}%\n\nCheck the logs:\n\ndocker logs vro-xample-workflows-1\ndocker logs vro-svc-xample-j-1\nXample code highlights\nAn HTTP API endpoint is implemented by XampleController\nWhen an API request is received, the controller sends a one-way MQ message to the Xample Workflow for processing\nIn the meantime, the controller sends an API response (with status=\"PROCESSING\")\nThe Xample Workflow uses Camel to route a MQ message to the svc-xample-j microservice\nAfter an artificial delay to simulate processing, the microservice sends a MQ response back to the Xample Workflow\nWhen the Xample Workflow receives the MQ response, it logs the microservice's response (with status=\"DONE\")\nRun microservice outside of Docker\n\nTo run any VRO component outside of Docker, you'll need to configure your environment so that the component can communicate with containers inside of Docker -- some steps for running vro-app in IntelliJ.\n\nFor the svc-xample-j microservice, no additional setup is necessary since the defaults to connect to the MQ should work.\n\nsource scripts/setenv.sh\n./gradlew :dockerComposeUp\n./gradlew :domain-xample:dockerComposeUp\nCOMPOSE_PROFILES=\"\" ./gradlew :app:dockerComposeUp\n\n# Stop the Docker instance of svc-xample-j so you can run svc-xample-j's bootRun outside of Docker\ndocker stop vro-svc-xample-j-1\n\n./gradlew :domain-xample:svc-xample-j:bootRun\n\nTo test, run in a new console:\n\ncurl -X POST \"http://localhost:8110/v3/xample-resource\" -H \"accept: application/json\" -H \"Content-Type: application/json\" \\\n  -d '{\"resourceId\":\"1234\",\"diagnosticCode\":\"J\"}'\nTo build and restart specific containers\nTo build the svc-xample-j Docker image: ./gradlew :domain-xample:svc-xample-j:docker\nTo build all images under the domain-xample folder: ./gradlew -p domain-xample build\nTo run this the updated image: ./gradlew :domain-xample:dockerComposeUp\nAlternatively, you can use Docker Compose directly: docker compose -f domain-xample/docker-compose.yml up svc-xample-j\n\nFor more see Docker Compose."
  },
  {
    "title": "Testing using Swagger UI ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Testing-using-Swagger-UI",
    "html": "VRO's Swagger UI is available for testing VRO's API endpoints\n\nLocal testing\nRun VRO: ./gradlew :dockerComposeUp :app:dockerComposeUp -- see Docker Compose and Development process for details and alternatives\nOpen a browser to http://localhost:8110/swagger or http://localhost:8111/swagger\n\nTesting MVP endpoints:\n\nVerify VRO is running by testing an endpoint (without authenticating) by expanding an endpoint (e.g., /v1/health-data-assessment) and clicking the \"Try it out\" button, then \"Execute\". The response should be code 403 with \"error\": \"Forbidden\".\nAuthenticate by clicking the \"Authorize\" button (near the top of the page), fill in the X-API-Key (at the bottom of the popup window) with one of these values (original PR), then click the \"Authorize\" button (NOTE: use just test-key-01 or test-key-02 in the Authorize dialog). A \"Logout\" button should appear. Click \"Close\".\nRetry the endpoint (now with the X-API-Key header).\n\nTesting iMVP endpoints:\n\nVerify VRO is running by testing an endpoint (without authenticating) by expanding an endpoint (e.g., /v2/automatedClaim) and clicking the \"Try it out\" button, then \"Execute\". The response should be code 401.\nAuthenticate by clicking the \"Authorize\" button (near the top of the page), fill in the \"Bearer Authentication (http, Bearer)\" (at the bottom of the popup window) with a valid JWT token, then click the \"Authorize\" button. A \"Logout\" button should appear. Click \"Close\". Use an example JWT, or use python to create a mock JWT:\nInstall PyJWT\npython3 -m pip install PyJWT\n\nCheckout generate-mock-jwt.py\nRun the script\npython3 jwt-maker.py\n\nRetry the endpoint (now with the \"Bearer Authentication (http, Bearer)\" header)\n\nTo connect to Lighthouse, a service-data-access/src/main/resources/private.pem file is needed. If it doesn't exist, create it using this.\n\nTesting on Non-prod LHDI deployment\n\nSwagger UI can be accessed on the VA network at https://dev.lighthouse.va.gov/abd-vro/swagger."
  },
  {
    "title": "Team Processes ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Team-Processes",
    "html": "Dependabot tickets\nWe will rotate who is taking the lead on Depandabots each sprint.\nA ticket will be created at the start of each sprint during planning and assigned to one team member.\nAny Dependabots for the sprint should be linked to in the sprints Dependabot ticket so that we can track our work.\nThe Dependabot Lead for the sprint can post in the Benefits-VRO-DevOps channel for any Dependabot tickets they may need help with.\nDependabot PRs need 2 reviewers: the Dependabot Lead + another teammate. The assignee is the Dependabot Lead but they also act as a reviewer since Dependabot is the author of the PR.\nFor more information see Dependabot.\nSecrel issues\nAll of the teams engineers should monitor the Benefits-VRO-DevOps Slack channel for Secrel issues.\nIf a Secrel alert is for something that may have been caused by an engineers work, that engineer should indicate in the Slack alert that they are looking into the issue.\nThe person assigned to the Secrel monitoring ticket for a given sprint will indicate in the alert thread in Slack if they are looking into the alert or they will tag the engineer whose work may have caused the alert in the Slack thread.\nCreating tickets\nWhen someone other than the Product Manager creates a ticket for the backlog they should post a slack message in the team channel with a link to the ticket tagging the Product Manager. The ticket creator should indicate their opinion on the priority level of the ticket in the slack message. If the ticket creator feels the ticket should be worked on in the current sprint, they should state this in the slack message and seek agreement from the Product Manager before starting. Tickets should be created for most work that a team member performs that takes more than a half day of time so the team can get credit for the work completed.\nPull Requests (PRs)\nPRs must be approved by a minimum of two reviewers.\nDependabot PRs are authored by Dependabot. This means that the two reviewers are the Dependabot Lead for the sprint, and another teammate.\nFor more information on PRs see Pull Requests"
  },
  {
    "title": "Support Model ( Draft ) ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Support-Model-(-Draft-)",
    "html": "Support Model\nIntroduction\n\nObjective of this page is to define software process to support abd-vro application in production environment\n\nPoint of Contacts\n\n#\tService Name\tMode of Contact\tPoint of Contact\t: Escalation\n1.\tData Access Service\t\t\t\n2.\tHealth Assessment Service\t\t\t\n3.\tCompile Evidence PDF Service\t\t\t\n4.\tRetrieve Evidence PDF Service\t\t\t\n5.\tClaim Notification and Order\t\t\t\n6.\tPersistence Layer\t\t\t\nInfrastructure Engagement Process\nHow to Engage MAS team for MAS failures\nHow to Engage LHDI team\nSupport Tools ( ServiceNow, PagerDuty )\nEnvironment Access Guide\n\nLink steps to access given environment\n\nStandard SOP/Troubleshooting Guide\nLHDI Point of Contact\nMAS Point of Contact\nEscalation Policy\nOncall Schedule and Rotation Policy"
  },
  {
    "title": "Secrets Vault ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Secrets-Vault",
    "html": "Secrets are stored in a LHDI's HashiCorp Vault, which resides in the VA network. Secrets include credentials, tokens, and certificates for all deployment environments. Scripts and Helm configurations have been created to formalize and reproduce deployment of secrets to all LHDI environments.\n\nHashiCorp Vault\n\nSecrets for all LHDI deployment environments are stored in a single vault at https://ldx-mapi.lighthouse.va.gov/vault, which requires VA network access. Following the security principle of least privilege, only members of the VRO Admins GitHub Team can log in using their GitHub credentials. Log in to the web UI using these instructions using vro-admins (which corresponds to the new VRO Admins GitHub Team) as the \"Role\".\n\n(Context: A separate VRO Admins GitHub Team was created to limit access to secrets. By default, LHDI allows all members of VA-ABD-RRD GitHub Team to have access to a vault store, which is contrary to the principle of least privilege. There's a vault store for va-abd-rrd but it is unused.)\n\nIn the Vault, secrets are organized under the deploy/ folder. Subfolders for each environment are used as follows:\n\ndefault: provides default secrets for all environments; used for the LHDI dev environment\nqa, sandbox, prod-test, prod: used for the respective LHDI environment and overrides any default secrets\nOnly differences from default secrets need to be present. As a result, there are few secrets in the qa environment and there is no dev subfolder.\n\nWithin each environment subfolder are other subfolders, which will be referred to as \"groups\". Each group contains key-value pairs. Typically the key is an environment variable name that is mapped verbatim for use by containers. Occasionally, Helm configurations map the secret to a different environment variable name as expected by different container -- for an example, search for DB_CLIENTUSER_NAME. In summary, the full Vault path to a group is $TEAM_NAME/deploy/$ENV/$GROUP. There are no subfolders deeper than the group level.\n\nThe groups are as follows:\n\ndb: secrets for VRO's database; maps to Kubernetes secret named vro-db\nmq: secrets for the message queue; maps to Kubernetes secret named vro-mq\nredis: secrets for the Redis cache; maps to Kubernetes secret named vro-redis\nVRO_SECRETS_API, VRO_SECRETS_LH, VRO_SECRETS_MAS, ...: secrets used by VRO components; these VRO_SECRETS_* groups map to a Kubernetes secrets named vro-secrets-....\nThese VRO_SECRETS_* groups are treated differently than the above groups to allow new secrets to be added without having to update Helm configurations, thereby minimizing maintenance. Most new secrets will be added in these groups.\nUnlike the other groups, each VRO_SECRETS_* group is passed as a single aggregate environment variable to VRO containers that use them, as specified in Helm configurations. For example, the VRO_SECRETS_API group maps to the VRO_SECRETS_API environment variable for the app container. The aggregate environment variable contains multiple export commands like export APIAUTH_KEY01=.... Upon startup, the container runs set-env-secrets.src to execute the export commands in the aggregate environment variable, resulting in exported environment variables (such as APIAUTH_KEY01) being available for the application.\nTo handle multiline strings and special characters, secret values can be base64-encoded. These secrets use a key name that ends with _BASE64 so that the set-k8s-secrets.sh script will decode the value properly and sets an environment variable without the _BASE64 suffix.\nUnique key names to avoid collisions\n\nWhile key-value pairs are organized in separate subfolders, the key names (which are typically used as environment variable names) should be unique within each LHDI environment to avoid any collisions when they are mapped to environment variables for Docker containers. For example, if there was a MY_SECRET key name in both the redis and VRO_SECRETS_API group subfolders AND a container uses both groups, then the container will only have one environment variable rather than the desired two. Note that this collision can also occur between MY_SECRET and MY_SECRET_BASE64 key names because the _BASE64 suffix is elided from the container's environment variable name.\n\nAdding/Modifying a secret\n\nAsk a VRO Admin to add, remove, or update the secret in Vault. Securely provide the secret for each LHDI environment -- minimally, one secret value for dev and another for prod.\n\nIf the secret is added to an existing VRO_SECRETS_* group, no Helm configuration changes are needed.\nIf the secret is added to another group, Helm configurations should be updated to use the new secret.\n\nRun Deploy secrets from Vault for each LHDI environment to update the Kubernetes secrets. The Docker containers will not use the secrets until they are redeployed.\n\nUpdates to secrets not being propagated?\n\nThere are circumstances where Kubernetes logs \"Error: couldn't find key VRO_SECRETS_... in Secret va-abd-rrd-.../vro-secrets\" -- see Slack thread for screenshots. This occurred because a single aggregate vro-secrets secret was used for all VRO_SECRETS_* groups, as that introduces issues with propagation of secret updates because containers still references the old aggregate secret:\n\nSymptom: Sometimes redeploying the pod works and sometimes it fails with this error.\nCurrent hypothesis for this inconsistent error: If other running pods reference the vro-secrets secret, then old versions of it may be available and is being used by new pods. This article prompted the hypothesis.\nWorkaround: Restart all old pods that reference the vro-secrets secret. Then start everything back up. If a restart isn't sufficient, a complete shutdown of all pods may not be necessary to remove all references to the old secret.\nAdditionally, marking the secret immutable may be contributing to the use of old secrets because immutable secrets aren't expected to change, so any changes (included a destroy and re-create) are not propagated. As a result, the vro-secrets secret is marked mutable in set-k8-secrets.sh.\n\nNow the vro-secrets-* secrets are individual secrets, where an individual secret is used by one or a very small number of containers. This reduces the number of containers that need to be shut down simultaneously to release all references to the old secret. This improvement should mitigate this probably of this problem.\n\nAdding a non-secret environment variable\n\nTo set a non-secret environment variable for a container in an LHDI environment, add it to the relevant Helm chart(s) under helm/. If the variable value is different for each environment, also add it to helm/values-for-*.yaml files.\n\nWith that said, before adding an environment variable, please read the next section.\n\nConfiguration setting vs. Environment variable\n\nIt is preferred to use a configuration file scoped to only the application/microservice/container (e.g., Configuration Settings#springs-applicationyml).\n\nAn environment variable is needed when any of the following are true:\n\nit is a secret (username, password, token, private certificate, ...) -- use Hashicorp Vault (as described on this page)\nused by multiple containers -- set it in helm/values*.yaml files and reference it in Helm charts (under helm/)\nneeds to be manually changed in deployment environments -- let's discuss\n\nWe should minimize the number of unnecessary Helm configurations, which will reduce DevOps maintenance and overhead, and reduce the number of factors that can cause VRO deployments to fail.\n\nSetting the Vault-token secret\n\nA Vault token is needed to access vault. The automation (a self-hosted GitHub Runner) expects a the Vault token to be a Kubernetes secret named vro-vault in the LHDI dev environment. The token expires monthly. Run scripts/set-secret-vault-token.sh \"$VAULT_TOKEN\" to set the token, where $VAULT_TOKEN equals the string copied from the Vault web UI (click \"Copy token\" in the upper-right corner drop-down menu).\n\nSetting Kubernetes access tokens\n\nKubernetes access tokens for each cluster (i.e., non-prod and prod) are needed to be able to deploy the secrets to the LHDI environments. The access tokens expire in 90 days. Run scripts/set-secret-kube-config.sh to set the devops-kubeconfig secret.\n\nSetting GHCR secret\n\nA GHCR secret in Kubernetes named devops-ghcr needs to be set for LHDI to pull images. Run scripts/set-secret-ghcr.sh \"$ENV\" \"$PAT\" for each LHDI environment, where $ENV is dev, qa, etc. The $PAT is a GitHub personal access token -- generate one using the abd-vro-machine account. This only needs to be run once (or every time the PAT expires).\n\nFAQ\nWhy use Vault?\n\nIt is a centralized, secure location in the VA's network designed to hold secrets. From the Vault, secrets can be quickly and consistently redeployed to various LHDI environments in case they need to be reset or rotated.\n\nWhy use a self-hosted GitHub Runner?\n\nOur GitHub Action workflow starts a self-host runner that runs within our LHDI dev environment to pull Vault secrets and set Kubernetes secrets, all within the VA's network. This is more secure than using HashiCorp's GitHub Action which would pull Vault secrets outside the VA network and into the GitHub Action workflow environment. The runner is a container in the vro-set-secrets-... Kubernetes pod and can deploy secrets to any LHDI environment when initiated by the Deploy secrets from Vault GitHub Action workflow. The pod is deployed to the dev LHDI environment (because that environment doesn't require SecRel-signed images) and can deploy secrets to other environments.\n\nWhy are some Kubernetes secrets immutable?\n\nThere has been unexplained occurrences where Kubernetes secrets have changed and caused problems. Making them immutable aims to reduce (but not entirely prevents) this problem."
  },
  {
    "title": "Setup on Windows ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Setup-on-Windows",
    "html": "There are two ways to set up the application on Windows:\n\nInstall directly on the Windows OS, using a unix-like command line to execute the scripts\nInstall on Ubuntu using WSL\nInstalling Directly on Windows\n\nGit Bash shell: https://git-scm.com/downloads Once install, you need to run the following to make line endings on windows compatible: git config --global core.autocrlf true\n\nDocker Desktop: https://docs.docker.com/get-docker/\n\nTo install hadolint, first install Scoop (https://github.com/ScoopInstaller/Scoop) and follow the instructions here: https://github.com/hadolint/hadolint#install\n\nYou will also need scoop to install shellcheck: https://github.com/koalaman/shellcheck#readme\n\nTo install Spectral, find the windows executable and put it on the PATH\n\nFor the remaining setup, just follow the instructions on the README https://github.com/department-of-veterans-affairs/abd-vro#readme, executing all commands from within the Git Bash shell (not DOS or Powershell).\n\nI should note that, although this way is simpler, not all services started successfully on docker. Postgres and Rabbit come up successfully but the vro services do not start.\n\nInstalling via Ubuntu\nMake Sure wsl2 is enabled\nDownload Ubuntu from https://digitaltransfusion.net/ubuntu2004.zip\nextract it to c:\\Tools\\Ubuntu2004 and start Ubuntu.exe. This will install Ubuntu\nOpen a windows command shell (cmd) and type the following:\nwsl --set-default Ubuntu-20.04\nwsl --set-version Ubuntu-20.04 2\nwsl --set-default-version 2\n\nEnable Ubuntu on Docker: \n\nRestart wsl\nwsl --shutdown\n\n\nRestart docker.\n\nOpen a new command window (cmd)\n\nType wsl. This should now switch to an Ubuntu shell.\n\ntype \"sudo docker ps\" to make sure it works\n\nupdate linux\n\nsudo apt update && sudo apt upgrade\n\nInstall prerequisites\nd ~/\nsudo apt install openjdk-17-jdk shellcheck\n\ncurl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | bash\n\nsudo wget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v1.16.3/hadolint-Linux-x86_64 && sudo chmod +x /bin/hadolint\n\n\ntype \"exit\"\ntype \"wsl\"\nnvm install 17.0.0\n\nnpm install -g @stoplight/spectral-cli\n\nexport GITHUB_ACCESS_TOKEN=<replace-with-token-from-github>\n\ncd ~/\n\nmkdir repos, cd repos\n\ngit clone git@github.com:department-of-veterans-affairs/abd-vro.git\n\n\n\nFor the remaining setup, just follow the instructions on Local Setup, executing all commands from within Ubuntu."
  },
  {
    "title": "Routing API requests ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Routing-API-requests",
    "html": "This page describes how API requests are routed to a microservice and an API response is sent back to the client, leveraging Apache Camel to provide most of the glue code and common Enterprise Integration Pattern (EIP) implementations.\n\nSynchronous API request\n\nReferring to the code version after PR #71 was merged. To run the code, see Development process and open http://localhost:8110/swagger in a browser to see the API spec, including the \"Demo API\" provided by the DemoController mentioned below.\n\nIn the app Docker container, when a Controller class receives an API request, it converts the requests into a model, feeds that model as a message into a Camel route, and converts the result of the last route endpoint into an API response object for sending back to the client -- see DemoController for an example.\nThe DemoController uses CamelEntrance to inject messages into a Camel endpoint for Camel routing.\nCamel routes are defined by RouteBuilder subclasses, like PrimaryRoutes and ClaimProcessorRoute. For example,\nDemoController.assess_health_data calls CamelEntrance.assess_health_data_demo\nto inject an AssessHealthData message into the direct:assess_health_data_demo endpoint,\nwhich is defined in PrimaryRoutes as a route that goes to rabbitmq:assess_health_data?routingKey=health_data_assessor (the health_data_assessor queue in RabbitMQ's assess_health_data direct exchange -- RabbitMQ/AMQP concepts).\nIn a separate service-ruby Docker container, a Ruby microservice is subscribed to RabbitMQ's health_data_assessor queue (using RabbitSubscriber service-ruby/src/lib/rabbit_subscriber.rb). As configured in microservices.rb, when a message is popped out of that queue, HealthDataAssessor will process the message and return a result, which is sent as a reply RabbitMQ message.\nIt is intended that communication with microservices use RabbitMQ, typically via a direct exchange and routing key to the queue. The message should be a JSON string (automatically encoded as byte[]) -- this will facilitate replacing RabbitMQ with other messaging systems (like AWS SQS) if needed.\nThe reply message is received back on the Camel route in the app Docker container. When the last endpoint on the route is completed, the last message is converted to the return object of CamelEntrance.assess_health_data_demo, which will be mapped to the API response object in DemoController.\n\"Asynchronous\" API request\n\nWhen an API request takes more than 30 seconds to complete, consider sending an immediate API response that includes a URL for the client to check on the status and/or to poll for the result. For an example, check out the POST claim API in the CamelApp, where a claim is processed asynchronously in the background and the claim's status is updated to reflect processing completion.\n\nTo achieve this, we leverage the recipientList EIP to send the POSTed claim to multiple endpoints. Since all the endpoints are configured to be asynchronous, we do not wait for any of the results and move on to the next endpoint of the Camel route (using the same message that was sent to the recipientList).\n\nDynamic Routing\n\nWhen the route of a claim (a.k.a., a Camel message) is determined dynamically (e.g., based on the claim characteristics or the processing logic), we use a dynamicRouter EIP. For example the DynamicClaimRouter class routes a claim to an endpoint depending on claim's contentionType attribute. A more complete example is provided in the CamelApp."
  },
  {
    "title": "RabbitMQ Microservice reliability ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/RabbitMQ-Microservice-reliability",
    "html": "This is a condensation of a short tech talk discussing findings and recommendations from issue #565.\n\nRabbitMQ uses acknowledgments to ensure that queue consumers reliably receive messages. A consumer can listen for messages on a queue using one of two settings:\n\nAuto-ack (no-ack) - Once RabbitMQ has successfully written a message to the TCP socket, its job is done and it forgets about the message. From RabbitMQ's perspective, it does not expect any acknowledgment back from the consumer. From the consumer's perspective, the receipt of the message comes with an implication of acknowledgment (courtesy of TCP's syn-ack handshake); no further action is needed to tell RabbitMQ to forget about the message.\nExplicit ack - The consumer is responsible for explicitly sending an acknowledgment back to RabbitMQ. If it doesn't (due to buggy code, or a system failure that the consumer can't handle), RabbitMQ will wait a certain amount of time (30 minutes by default), then re-queue the message for delivery to another consumer. Note that an ack can mean: (1) a consumer received the message (redundancy for the TCP syn-ack) or (2) consumer successfully processed the message (may not be needed if the caller is expecting a response to the request) -- the semantics is for us to decide but we should be consistent.\n\nThe remainder of this page describes a few scenarios for how VRO microservices could interact with RabbitMQ to ensure reliability (or not), starting with some obviously unworkable ones.\n\nNo acknowledgment\n\nThe microservice consumes requests with auto-ack on, and performs the work without communicating out about any success or failure.\n\nThis does not provide any safeguards in case of failure, and should only be used if the microservice is intended to perform optional \"best-effort\" work.\n\nExplicit acknowledgment\n\nThe most basic way to address the above issue is to have the microservice explicitly acknowledge that it performed its work successfully.\n\nSee the re-queuing section below for a description of RabbitMQ's behavior if it never receives the explicit ack.\n\nResponding to app\n\nCurrently, every microservice in VRO is invoked by a caller (for example the API server, labeled here as \"App\") that depends on some sort of result from the microservice. As with the request sent to the microservice, the response is published to RabbitMQ, and subsequently consumed by the caller.\n\nIn this case, the response message also serves conceptually as an acknowledgment from the microservice that it performed its work successfully. The next two scenarios describe what happens when a failure occurs.\n\nResponding to App with error\n\nIf the microservice encounters a failure and can recover (in Java and Python microservices that wrap the entire task in a try/catch, this likely accounts for nearly all failures), the microservice can send an error response back to the app.\n\nThis is nearly identical to the previous scenario, just with different content in the response message -- the error response acts like the ack.\n\nLetting the App time out\n\nOccasionally there are failures can't be handled by the microservice, e.g. if its container gets killed by OOM. In this case, the microservice isn't able to send back any response, so it's up to the App to time-out after a reasonable amount of time.\n\nThis works well when the App itself is bound by a request/response context that is expected to return within a reasonable amount of time. Other callers/requesters must ensure to set a response timeout and handle it by resubmitting the request (itself or via RabbitMQ's features) or raising an error for the originating caller to handle.\n\nRe-queuing asynchronous work\n\nIn the event that the microservice is responsible for work that finishes at some indeterminate time in the future, RabbitMQ's re-queuing of unacknowledged messages can be used. When the microservice consumes messages with explicit acks, RabbitMQ will wait a certain amount of time (30 minutes by default) for the ack, and re-queue and re-deliver the message after a time-out.\n\nThis sequence diagram shows the flow when a second microservice is available to consume the message."
  },
  {
    "title": "Release Tags ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Release-Tags",
    "html": "Review current tags and follow Semantic Versioning guidelines to choose the next version number for the release tag. For the desired branch (usually develop), use the create-release-tag GitHub Action workflow in the public repo to update version numbers in the code and create a release tag.\n\nWhen the release tag is created, the create-gh-release GitHub Action workflow will automatically create a GitHub Release (these auto-created GitHub releases with suffix -auto are candidates for deletion during cleanup).\nAdditionally, the mirror GitHub Action workflow will copy the release tag to the internal repo, where an identical GitHub Release is auto-created in the internal repo, which will trigger SecRel GitHub Action workflow to run and sign the images. The packages/images published to GHCR should be tagged with the release tag version number (v*.*.* or release-*.*.*).\nThe Secure Release process pushes packages to the GitHub Container Repository. Note that the packages are associated with the abd-vro-internal repo.\nYou can follow progress of SecRel (Secure Release) workflow run on the internal repo's Actions.\nIf the SecRel workflow is not successful, the SecRel alerts need to be addressed -- see SecRel Processing Guide.\n\nAll GitHub releases that are annotated as a Pre-release (and its associated git tag) will be automatically deleted by the Delete old Pre-Releases and tags GitHub Action workflow. To prevent deletion, remove the Pre-release annotation.\n\nDetails\n\nRefer to diagram at Deployment-Overview#process-diagram.\n\ncreate-release-tag workflow runs the following:\n\n./gradlew release creates a git tag locally (until it is pushed to the remote in the last step)\nscripts/image-version.sh pin to \"Pin unpinned image versions\" to the release version number created in the previous step -- see Container Image Versions\ngit push updated image versions to develop\ntriggers mirror workflow to push changes to internal repo\ntriggers SecRel workflow (on internal repo) but any processing is skipped due to a gate-check, which exists to avoid unnecessary processing. SecRel will be triggered in the next step.\ngit push release tag\ntriggers Auto-create GitHub release on public repo\ntriggers mirror workflow to push changes to internal repo\ntriggers Auto-create GitHub release on internal repo)\ntriggers SecRel workflow (on internal repo) on release tag\npublishes non-dev images (i.e., those without the dev_ prefix) to GHCR -- see CI-CD-Workflows#1-continuous-integration-ci\ncalls SecRel pipeline to scan images\n\nPeriodically, the Delete old Pre-Releases and tags runs to declutter pre-releases and git release tags the public repo. These deletions will be automatically propagated to the internal repo when the mirror workflow runs, at which point:\n\nGit release tags (associated with GitHub pre-releases) are deleted in the internal repo\nThese tag deletions will result in GitHub releases disappearing in the internal repo, but the number of releases (shown on the front page) is incorrect (i.e., there are orphaned GH releases) -- submitted bug report to GitHub.\n8/24/2023: Bug report closed with:\n\nOur engineering team has flagged this issue as it appears to be a bug in displaying the releases. There is no current ETA on when the fix will be available, but I recommend you keep an eye on the GitHub Blog and our social media feeds for the latest announcements about new features and fixes. Our policies prevent us from maintaining a ticket in an open state for extended amounts of time. With this I am going to resolve this ticket. I understand that this doesn't resolve the issue reported, however it will likely be some time before any changes are made."
  },
  {
    "title": "Release Process ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Release-Process",
    "html": "The release process requires access to the VA system. Information on the release process can be found on this page of the internal wiki."
  },
  {
    "title": "Quick Deploy Instructions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Quick-Deploy-Instructions",
    "html": "Spun off from the Deploy to Prod page.\n\nThis is a straight-forward walk-through, with visual aides, for cutting releases and performing deploys for VRO.\n\nThese instructions are valid as of April 20th, 2023. If this changes, please remember to update this doc and this date.\n\nJune 1, 2023: Prepended \"[THIS IS NOW AUTOMATED]\" to obsolete steps.\n\nThere are actually two (2) GitHub repositories we need to use for this:\n\nabd-vro\nabd-vro-internal\n\nYou will need the correct permissions in order to create releases for both repositories.\n\nTHIS ASSUMES YOUR main BRANCH IS IN A STATE TO CUT A RELEASE FROM ‚Äî usually by merging develop into main ‚Äî such that it has all the latest and greatest items you want in your next release.\n\n1) First, we want to be on abd-vro\n\n2) [THIS IS NOW AUTOMATED] Next, we will want to create a new release\n\nClick on either of these areas I've highlighted with red boxes:\n\n3) [THIS IS NOW AUTOMATED] Draft a new release:\n\n4) [THIS IS NOW AUTOMATED] Click on \"Choose a tag\":\n\n5) [THIS IS NOW AUTOMATED] Our release hasn't been created yet, so we need to manually type it in\n\nIn this example, we'll be bumping the patch iteration of our semantic version number up by one ‚Äî going from release-1.0.94 to release-1.0.95. Then we will need to click on \"Create new tag\".\n\n6) [THIS IS NOW AUTOMATED] Select the branch you want to cut the release from\n\nIn this example, we're doing so from main (this is most likely going to be the only branch we cut releases off of from now on)\n\n6) [THIS IS NOW AUTOMATED] Type in the name of the release in the \"Release title\" text field. We've been using a format like so: v1.0.95\n\nYou don't need to type in a description. When everything looks correct, click \"Publish release\".\n\n7) Once published, this is what you should see:\n8) Go to abd-vro's Actions tab and make sure the \"Mirroring\" job runs and goes green\n\nYou might have to wait a minute for the job to queue up, but this should happen quickly. This job does what it says in the name and tries to mirror abd-vro to abd-vro-internal. In our case, it pushes the tag over to abd-vro-internal so we don't have to create the tag over there, though we will still have to create the release.\n\nMore info on tags versus releases.\n\n9) Now we switch to the abd-vro-internal repository\n\nNote that this repo's most current release is still the old one. In this example, it's still release-1.0.94\n\n10) [THIS IS NOW AUTOMATED] REPEAT STEPS 2-7 FOR THIS REPO\n\nThe only difference will be that this time, the release-1.0.95 tag we created in abd-vro was \"mirrored\" over ‚Äî just the tag, not the actual release, which we will have to create (the same way) in this repo.\n\nNOTE: Selecting the new release created in abd-vro will default your branch to main\n\n11) Go to abd-vro-internal's Actions tab and make sure you see a job with the release tag as the job name\n\nIt may take a minute to queue up but is generally quick to show up. The workflow job itself, however, takes a while to run ‚Äî approx 20 mins. We need to wait and make sure it turns green. Once complete, if successful, you should be ready to deploy now (next step)\n\n12) In your Actions Workflow pane to the left, click on \"1. (Internal) Update Deployment\" and then \"Run workflow\"\n\n13) [OBSOLETE] Select your branch and target environment you wish to deploy from\n\nThe following is obsolete -- refer to https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deploying-VRO#deploy-images-with-the-release-tag instead.\n\nDefault settings for all else will be used in most cases. The deployment workflow itself shouldn't take anywhere near as long as the previous SecRel step. It averages 4-5 minutes.\n\nYou're done!\n\nThat's all there is to it. Provided there are no problems (usually SecRel-related), this should be a straight-forward process.\n\nIf you don't wait between manual steps, then the whole thing should usually take around 30-40 minutes."
  },
  {
    "title": "Pull Requests ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Pull-Requests",
    "html": "Procedure for submitting and merging PRs (pull requests).\n\nCreate a pull request\nThe title of the PR becomes the commit message in the main branch, so be as concise and specific as possible. Refer to How to Write a Git Commit Message.\nTry to limit to 300 lines of code changes; smaller logical chunks of code is better; https://blog.logrocket.com/using-stacked-pull-requests-in-github/\nhttps://opensource.com/article/18/6/anatomy-perfect-pull-request\nhttps://developers.google.com/blockly/guides/modify/contribute/write_a_good_pr\nAdd a minimum of 2 reviewers or post in Slack channel #benefits-vro-engineering for anyone to review\nException: Dependabot PRs are authored by Dependabot. This means that the two reviewers are the Dependabot Lead for the sprint, and another teammate. See Dependabot for more information.\nLook for successful testing steps as GitHub actions for your changes. Note that some actions such as Container Health Checks will be skipped on some PRs, skipped actions are acceptable but failed actions should be addressed.\nAdd comments to your code under the \"Files Changed\" tab to explain complex logic or code\nhttps://betterprogramming.pub/how-to-make-a-perfect-pull-request-3578fb4c112\nReviewers provide feedback, clearly marking blocking and non-blocking suggestions\nhttps://github.blog/2015-01-21-how-to-write-the-perfect-pull-request/\nAddress feedback and repeat review step until PR is approved\nSquash and merge PR, which will trigger Github-Actions#mirror among other things\nCode Reviews\n\nCode reviews are intended to help all of us grow as engineers and improve the quality of what we ship. These guidelines are meant to reinforce those two goals.\n\nCODEOWNERS\n\nPRs are currently auto assigned to Github department-of-veterans-affairs/benefits-vro-engineers team, this can be tweaked and changed when additional code based rules apply based on the documentation here: https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners\n\nFor reviewers\n\nAim to respond to code reviews within 1-2 business day.\n\nRemember to highlight things that you like and appreciate while reading through the changes, and to make any other feedback clearly actionable by indicating if it is optional preference, an important consideration, or an error.\n\nDon't be afraid to comment with a question, or to ask for clarification, or provide a suggestion, whenever you don‚Äôt understand what is going on at first glance ‚Äî or if you think an approach or decision can be improved. Code reviews give us a chance to learn from one another, and to reflect, iterate on, and document why certain decisions are made.\n\nOnce you're ready to approve or request changes, err on the side of trust. Send a vote of approval if the PR looks ready except for trivial changes. Use \"Request changes\" when there's a blocking issue or major refactors that should be done.\n\nFor authors or requesters\nDraft PRs\n\nIt is highly recommended that you create a Draft Pull Request initially, then when ready, mark it as Ready for Review.\n\nYour PR should be small enough that a reviewer can reasonably respond within 1-2 business days. For larger changes, break them down into a series of PRs. If refactors are included in your changes, try to split them out as recommended below.\n\nAs a PR writer, you should consider your description and comments as documentation; current and future team members will refer to it to understand your design decisions. Include relevant context and business requirements, and add preemptive comments (in code or PR) for sections of code that may be confusing or worth debate.\n\nRe-requesting reviews after completing changes\n\nAfter you make requested changes in response to code review feedback, please re-request reviews from the reviewers to notify them that the work is ready to be reviewed again."
  },
  {
    "title": "Proposal to simplify abd_vro ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Proposal-to-simplify-abd_vro",
    "html": "The Problem\n\nThe code collected under abd-vro serves a several purpose:\n\nTo act as a parent for all subprojects relating to claim processing\n\nTo host the Glue code (JPA, REST, and Camel) that connects the individual sub-services\n\nTo host the CI and deployment scripts\n\nIt's multi-purpose nature already makes this repository pretty complex. To simplify its structure, we should aim to concentrate the source code and, especially, the configurations, in a few files as possible.\n\nIn my opinion the fact that the project structure is spread over several sub-modules is problematic as it involves several unnecessary features:\n\nFigure out which module depends on which and try to enforce those dependencies\nRequire a build script for each module\nRequire spring configuration for every module, especially for testings\nMakes it very hard to run integration tests because spring requires a full configuration for @SpringBootTest\nThe Solution\n\nIn my opinion, the solution is to simplify the project as much as possible, and this can be accomplished by combining all the sub-modules of \"app\" into a single module. This module will include the following sub-modules:\n\napi\napp\ncontroller\nservice (and all sub-modules of service)\npersistence (and all sub-modules of persistence)\n\nThere will be one build script and one spring boot configuration (two if you count the tests) for all these modules. That's it.\n\nArchitectural structure\n\nWhereas it is important that the layering of the architecture should be preserved, the layering does not need to rely on submodules. In the current structure, persistence, services, controllers, etc reside in separate gradle module. It suffices that they reside in different packages within the same module. We can leverage archunit, which we are already using, to enforce the dependencies between layers.\n\nFinal input from Dimitri:\n\nOverall, I don‚Äôt think this application is a good candidate for breaking up into even more microservices. When refactoring into microservices, we accept the price of additional complexity in order to gain flexibility advantages. These advantages include the ability to develop parts of the application independently as well as scale different pieces independently. If no such advantage can be gained, then we just pay a complexity price for no reason.\n\nMy reasoning for not recommending microservices can be summarized by the following points:\n\nThis application is already an orchestration layer which coordinates the actions of several microservices into a cohesive workflow.\nThe same external APIs are used at every step of the workflow: Lighthouse, MAS, BIP, and the python services. This means that there is no particular component which is responsible for communicating exclusively with some type of API.\nEverytime we interface we an external service, the probability that something will go wrong increases. If we inject additional microservices, there will be two levels of indirection before calling, say, the MAS API thus augmenting the probability of error and the difficulty of debugging\nThere is actually not that much code in this application.\n\nNevertheless, if we really want to extract microservices, the key is to identify components that can act virtually independently of one another. I can think of the following components that satisfy this criterion:\n\nThe database/auditing framework can become a downstream service that receives claim updates and persists them in the database and generates alerts as needed\nThe v1 and v2 processing streams have their own controllers and their own camel routes. This makes them good candidates for independent services. The downside of separating them is that they share a lot of the same code, so some common libraries will need to be extracted as well.\nInput from Rajesh on Refactoring the MAS Integration:\n\nTo refactor the interaction with MAS into self contained microservice, it could include the following\n\nAuthorization flow support components in the \"app folder\" (props, config)\nMAS API service components(auth, service, mapper) in the \"service folder\" (mas)\nAbstract the call to get the MAS collection status and annotations into a single call with options to return the response from MAS asis and mapped to a common model(abdevidence)\nAbstract the call to MAS order exam"
  },
  {
    "title": "Partner Team Deploy Process ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Partner-Team-Deploy-Process",
    "html": "Separate Domain Branch\n\nTo support a more independent deployment process for a Partner Team, a temporary domain-... branch (diverging from the develop branch) can be used during domain development. While this may add marginal complexity in git branch management, it offers the following advantages:\n\nSeparate commit histories isolate changes so that changes in one branch doesn't interfere with the other branch.\nWhile the domain code is being developed, the domain-... branch is based on a stable/unchanging develop commit, which offers a consistent VRO Platform and minimizes any delays in non-prod deployments.\nChanges to domain-specific Docker images can deploy to non-prod environments, independent of changes to the develop branch.\nThe domain-... branch can sync up with the develop branch as frequently as desired. Any significant changes to develop that affect domain(s) are communicated to domain developers, allowing Partner Teams to make updates according to their timeline/schedule (i.e., Partner Teams don't have to address the Platform changes immediately, and the changes don't immediately block their work).\nEncourage separation of concerns, where domain-specific changes should be committed to the domain-... branch, while VRO platform changes should be committed to the develop branch. This will also mitigate future branch-merging conflicts.\n\nWhen the domain code is ready for deployment to prod, the VRO Platform Team will update the develop branch using the domain-... branch, update the main branch, and (once testing is successful) deploy to prod.\n\nOnce major domain development is complete, the domain-... branch can be deleted and minor domain-specific changes are committed to the develop branch.\n\nLHDI Deployment requires only Docker images\n\nDeployment to LHDI requires only Docker images. Git branches help to track and manage the codebase as it's being developed. Once Docker images are created, git branches are irrelevant -- the images are associated with git via the commit hash uses as the image tag.\n\nTo deploy domain images:\n\nCreate and publish domain images (created from the domain-... folder) to GHCR (as dev or non-dev images) and have SecRel sign them.\nDeploy the domain images to LHDI, leaving other images unchanged (i.e., helm --reuse-values).\n\n(When we have multiple domains being deployed, we can leverage separate \"Helm releases\" (not to be confused with GitHub releases or git release tags). For now, let's \"Keep It Simple, Silly\" and have only one Helm release currently called vro.)\n\nGit Branches and Commit History\n\nTo explain how a separate domain-... branch will be used in the context of deploying domain images, the following sections describes the basic deployment steps (see Deploying VRO for details), followed by sections that use those basic steps in relation to git branches.\n\nDomain branch\n\nThe VRO Platform Team creates a protected domain-* branch for the Partner Team to develop code and deploy to non-prod LHDI environments independently of the develop branch (and its changes). Take for example a \"CC\" domain:\n\nCreate (or reset) the domain-cc branch to the latest develop commit\nPartner Team commits to domain-cc branch (with PR reviews from the VRO Platform Team)\nNon-prod Deploy of Domain branch\n\n(Similiar steps as Non-prod deploy of Develop except they're applied to the domain-cc branch)\n\nCreate a release tag on domain-cc by selecting the domain-cc branch for \"Use workflow from\" and using tag format v*.*.*-domaincc-N where N is an integer counter. This tag format helps to distinguish domain release tags from those created from the develop branch.\nUse Update CC Deployment action to deploy the created release to the desired non-prod LHDI environment (dev, qa, sandbox, or prod-test). Deployments to prod are performed by only the VRO Platform Team.\nUpdate Domain branch with the Develop branch\n\nAfter some development, the develop and domain-cc can have different commits:\n\nTo update the domain-cc branch, rebase the domain-cc branch against the latest (or desired) develop commit:\n\nPartner teams are encouraged to rebase against develop frequently to minimize the number of accumulated merge conflicts and to stay up-to-date with the develop branch.\n\nRebasing will enable the develop branch to easily be updated (in the next section) without any merge conflicts and maintain a cleaner git commit history (i.e., without merge commits).\n\nUpdate Develop with the Domain branch\n\nPrerequisite: The Partner Team has completed Update Domain branch with the Develop branch and tested its deployment.\n\nTo update the develop branch, ask the VRO Platform Team to fast-forward merge the develop branch to the latest (or desired) domain-... commit. After which, more commits can be added to the domain-... branch:\n\nThen, Create a release tag on develop using the latest release version number. This is to ensure the latest develop commit is used as the last release version in case releases have been created on both develop and domain-cc.\n\nProd Deploy of Domain branch\nUpdate Develop with the Domain branch\nAsk the VRO Platform Team to do Prod deploy of Main"
  },
  {
    "title": "Possible Errors on Windows Machines ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Possible-Errors-on-Windows-Machines",
    "html": "Windows users could run into some common issues when trying to build or run VRO. If you are a windows user and run into any issues that are related to using windows, please feel free to update this page with the error and your solution.\n\nCRLF -> LF\n\nWhen trying to run the app on docker, I get this error message: Error creating bean with name 'bipCERestTemplate' defined in class path resource [gov/va/vro/config/BipApiConfig.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.web.client.RestTemplate]: Factory method 'getHttpsRestTemplate' threw exception; nested exception is [gov.va](http://gov.va/).vro.service.provider.bip.BipException: Failed to create SSL context. Caused by: java.io.IOException: Keystore was tampered with, or password was incorrect\n\nThis is because in your abd-vro-dev-secrets repo the file endings are CRLF, when they need to be LF to work. To fix this:\n\nOpen the abd-vro-dev-secrets project into an IDE. I use intelliJ for abd-vro, but for abd-vro-dev-secrets I use VS code to keep them separate.\nIn the bottom right corner of the screen, you will see that it says CRLF. Click on this and change it to LF. You will need to repeat this process for every file.\nMake sure you get the files under abd-vro-dev-secrets/local as these are the files causing the issue.\nMake sure to save every file that you made the change on and then open abd-vro\nNow set your env variables using source scripts/setenv.sh\nRun the app and see that the app now starts up as expected."
  },
  {
    "title": "New Domain Setup ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/New-Domain-Setup",
    "html": "To set up a new domain folder and deployment configuration for a Partner Team, this page enumerates the tasks, which are organized into the following stages:\n\nGradle domain subproject for all domain code\nDocker configurations to containerize applications\nHelm configuration for LHDI Deployment\nMiscellaneous Setup\n\nUnless otherwise stated, any team member (including those in the Partner Team) can perform the described tasks.\n\nThese tasks set up and configure the resources needed for eventual deployment. For deployment to LHDI, see Partner Team Deploy Process.\n\nReferences\n\nPRs that add a new domain:\n\ndomain-cc Feature - run contention classification api in docker #1646\nSetup a deployment process/plan for domain-ee in nonprod #1922\nBy 8/21: Setup a deployment process/plan for domain-ee in nonprod #1932\n\nPRs that add a new VRO microservices:\n\nDocker: add svc-bie-kafka container image #1794\nHelm: Add config for svc-bie-kafka #1796\n1. Gradle subproject for all domain code\n\nCreate a new domain-XYZ folder (replacing XYZ with a name or acronym associated with the Partner Team). In that folder, add build.gradle and README.md with build and test instructions for the domain. Refer to other domain-* folders.\n\nAdd a domain application project\n\nIn the domain-XYZ folder, create Gradle subproject(s) for your application. The subprojects depend on your application.\n\nPopulate the subprojects with code -- refer to Software Conventions and other domain-* folders; for example:\nFor Java projects, refer to domain-xample.\nFor non-Java projects, the XYZ-app subproject should have build.gradle and gradle.properties files in the domain-XYZ/XYZ-app folder. There can be multiple *-app subprojects in the domain folder.\nAdd each subproject to settings.gradle in the project root folder.\nAdd unit tests to the subprojects and ensure ./gradlew :domain-XYZ:test executes those tests. Then in .github/workflows/test-code.yml, update and add job steps to run unit tests in the domain by running ./gradlew :domain-XYZ:test.\n2. Docker configurations to containerize applications\n\nFor end-to-end and integration tests, add a docker-compose.yml in the domain-XYZ folder (see Docker Compose). It should set up any new containers added in the domain-XYZ folder. Not all subprojects are manifested as Docker container, i.e., some subprojects are used as libraries -- those projects don't need to be added to docker-compose.yml.\n\nAdd container to scripts/image-names.sh the list of IMAGES environment variable. Run ./scripts/image-names.sh to update scripts/image_vars.src, which is used by GH Action workflows (e.g., to build and publish the Docker image to GHCR). Commit both files.\n\nAdd container healthcheck\nFor each new container, add a healthcheck.\nRefer to the HEALTHCHECK command in Dockerfile-java or Dockerfile-python within the gradle-plugins/src/main/resources/docker/ folder.\nSpecify the healthcheck_port in domain-XYZ/XYZ-app/gradle.properties. For port numbering convention, see Software-Conventions#port-numbers.\nAdd a job step to .github/workflows/container-healthchecks.yml that exercises the healthcheck.\nAdd integration tests\nAdd end-to-end and/or integration tests for the domain.\nAdd a new GitHub Action workflow that runs those tests. For examples, refer to *-integration-test.yml and *-end2end-test.yml.\nAdd the new workflow as a job in .github/workflows/continuous-integration.yml.\nAdd mock service\n\nIf a new mock service is needed, add it to the mocks/ folder. Note that mocks/ is a Gradle composite build (not a Gradle subproject) to isolate it from the typical VRO codebase because the mocks are not built for LHDI deployment. See PR Gradle: refactor mocks into composite build #1704 for details.\n\nUpdate CI/CD GitHub Action workflows\nFor each Python subproject, add the requirements.txt files to .github/actions/setup-vro/action.yml and .github/secrel/config.yml.\nIf the domain application is ready for daily deployments to LHDI dev as part of [our CI/CD](CI CD Workflows), then add a job to .github/workflows/continuous-delivery.yml.\n3. Helm configuration for LHDI Deployment\nAdd a new helm/domain-XYZ/ folder and populate it. Refer to Helm Charts and helm/domain-*/.\nEnsure that helm/domain-XYZ/templates/named_templates is a symbolic link.\nAdd an entry to helmChartAppVersions in build.gradle in the project root folder so that Helm chart version number are updated when a release is created.\nUpdate Deployment GitHub Action workflows\nAdd the new helm chart folder as a helm_chart option in .github/workflows/update-deployment.yml.\nOptionally, create .github/workflows/update-deployment-XYZ.yml that deploys to all LHDI environments except prod for Partner Teams to use. Tip: copy and modify existing workflows, e.g. update-deployment-cc.yml and update-deployment-ee.yml.\nIn the rare situation where different versions of containers in the same helm chart needs to be deployed, create a new workflow (refer to update-deployment-app.yml) and update deploy.sh (to pass the version numbers to the helm command).\n4. Miscellaneous Setup\nOptional: domain-XYZ branch\n\nIf the Partner Team wants to work on a separate git branch (see Partner-Team-Deploy-Process#separate-domain-branch):\n\nAsk the VRO Team to create a domain-XYZ branch in the repo. It will be a protected branch and is configurable by repo admins at https://github.com/department-of-veterans-affairs/abd-vro/settings/branches.\nAdd the branch as a domain_branch option to .github/workflows/fast-forward-develop.yml.\nAdd entry to API Gateway\n\nTo have the OpenAPI spec available via Swagger UI, update configurations for the API Gateway -- refer to API-Gateway#adding-a-new-domain-api-for-lhdi-deployments-and-swagger-ui which:\n\nconfigures the VirtualService\nupdates Swagger UI configuration"
  },
  {
    "title": "Mock Slack Server ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-Slack-Server",
    "html": "Mock Slack Server is mainly developed to validate calls to Slack channels in automated end-to-end tests.\n\nImplementation\n\nMock Slack Server is a Spring Boot based REST API.\n\nIt is implemented as a Gradle subproject mocks:mock-slack in abd-vro project.\n\nSlack Message Store\n\nThe primary functionality of Mock Slack Server is to detect Slack channel posts. To facilitate end-to-end tests, this Mock Server stores all received Slack messages in a Java HashMap with collection id's as keys. The values are the content of the slack message as stored in a SlackMessage object. The collection id is extracted from the message using a regex expression. The regex can be found in the Spring Boot Application configuration.\n\nSpring Boot Application configuration initializes the HashMap as part of initialization of the SlackMessageStore bean.\n\nEnd Points\n\nEnd-points are defined in an interface and implemented in a controller.\n\nThe end-points are:\n\nPOST /slack-messages: posts a new slack message.\nGET /slack-messages/{collectionId}: retrieves a previously posted slack message for the collection.\nDELETE /slack-messages/{collectionId}: deletes any stored slack message for the collection.\n\nThe main POST end-point mock Slack API basic message POST end-point. This end-point accepts a basic json object with text property similar to the Slack end-point. The collection id is extracted from the text property using the regex available in Spring Boot Application configuration.\n\nThe GET end-point retrieves the slack message posted for a collection. It is used in automated end-to-end tests to verify a slack message is posted.\n\nThe DELETE end-point removes the slack message for a collection if it has been posted. It is used before any end-to-end test to make sure a previous run of the end-end-test does not affect the current run."
  },
  {
    "title": "Mock MAS API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-MAS-API",
    "html": "Mock MAS API is mainly developed to validate MAS related functionality in automated end-to-end tests.\n\nImplementation\n\nMock MAS API is a Spring Boot based REST API.\n\nIt is implemented as a Gradle subproject mocks:mock-mas-api in abd-vro project.\n\nMock Collections\n\nThe primary functionality of Mock Mas API is to provide mock collections. All mock collections are specified in a resource directory called annotations as json files. A naming convention collection-ddd.json is followed where ddd stands for the collection id.\n\nThese collections are modelled after actual collection data receieved from MAS before and during end-to-end testing.\n\nThese mock collections are stored in a Java HashMap with collection id's as keys. Spring Boot Application configuration initializes the HashMap as part of initialization of the Collection Store bean.\n\nExam Ordered Store\n\nTo facilitate end-to-end tests, this mock server keeps tracks of the collections for which exams are ordered. This information is stored in a Java HashMap with collection id's as keys. The values are Boolean objects. Spring Boot Application configuration initializes the HashMap as part of initialization of the Exam Order Store bean.\n\nEnd Points\n\nEnd-points are directly defined and implemented in a controller.\n\nThree end points mock MAS functionality:\n\nPOST /token: retrieves a jwt.\nPOST /pcQueryCollectionAnnots: retrieves a collection.\nPOST /pcCheckCollectionStatus: retrieves status of a collection.\nPOST /pcOrderExam: orders an exam.\n\nThe remaining end points provides information about the state of the server to facilitate automated end-to-end tests:\n\nGET /checkExamOrdered/{collectionsId}: checks if an exam ordered for a collection.\nDELETE /checkExamOrdered/{collectionsId}: resets exam ordered status for a collection.\nToken Retrieval\n\nThe end-point /token retrieves jwt. This end-point is a pass-through; it simply calls to MAS development server for token and returns the retrieved token.\n\nNote that this mock server itself does not validate the token when it is attached as the Authorization HTTP header in calls to other end-points. This has been left to future development primarily due to other priorities.\n\nCollection Retrieval\n\nThe end-point /pcQueryCollectionAnnots retrieves a collection. This end-points accepts a simple json object that specifies the collection's id.\n\nCurrently MAS development server provides only one collection id'ed 350. For this collection id, this end-point retrieves the collection from MAS development server and returns the the caller. For all collection id's that identify one of the mock collections, the mock collection is return. If the collection id is not 350 or does not identify one of the mock collections, Not Found (404) status code is returned.\n\nCollection Status Retrieval\n\nThe end-point /pcCheckCollectionStatus currently simply returns VRONOTIFIED status. Since MAS is now preparing the collection before calling to VRO, we do not expect any other status.\n\nExam Orders\n\nThe end-point /pcOrderExam mocks ordering an exam. This end-point currently always returns success. In addition the collection id for which the exam is ordered for is stored in the Exam Ordered Store.\n\nExam Ordered Status\n\nThe end-point GET /checkExamOrdered returns if an exam is ordered for a collection. This information is useful in end-to-end tests. A DELETE end point is also provided so that the mock server can be reset for a particular collection id. This mackes it possible to run end-to-end multiple times after this mock api is starred.\n\nEnvironment Variables\n\nThis mock API uses the same MAS related environment variables that VRO application uses. You can find these environment variables in the application.yml mock-mas properties."
  },
  {
    "title": "Mock Services ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-Services",
    "html": "Mock Services are primarily used to simulate external systems VRO integrates with in End to End Tests.\n\nThere are five mock services, one for each external system\n\nMock BIP Claim Evidence API\nMock BIP Claims API\nMock Lighthouse Health API\nMock MAS API\nMock Slack Server"
  },
  {
    "title": "Mock Lighthouse Health API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-Lighthouse-Health-API",
    "html": "Mock Lighthouse Health API is mainly developed to validate LightHouse Health API related functionality in the automated end-to-end tests.\n\nImplementation\n\nMock Lighthouse Health API is a Spring Boot based REST API.\n\nIt is implemented as a Gradle subproject named mocks:mock-lighthouse-api in abd-vro project.\n\nMock Fhir Bundles\n\nThe primary functionality of Mock Lighthouse Health API is to provide Mock Fhir Bundles. All Mock Bundles are in a resource directory called mock-bundles in patient specific directories that are named after the Mock ICN for the patient.\n\nEach patient specific directory contains the actual Mock Bundles named after the resource types. Currently there are three types bundles all in json format\n\nObservation.json\nMedicationRequest.json\nCondition.json\n\nThese Mock Bundles are modeled after the actual Fhir Bundles received either from Lighthouse Sandbox environment or from Lighthouse Production environment before and during end-to-end testing.\n\nThe Mock Fhir Bundles are stored in a MockBundles object which are themselves stored in a Java HashMap with the patient ICN's as the keys. Spring Boot Application configuration initializes the HashMap as part of initialization of the MockBundleStore bean.\n\nEnd Points\n\nAll end-points are defined in an interface and implemented in a controller.\n\nAll end-points mock Lighthouse Health API functionality. One is for token retrieval and others are for mock Fhir Search End-points to retrieve resources:\n\nPOST /token\nGET /Observation\nGET /Condition\nGET /MedicationRequest\n\nThe end-point /token retrieves jwt. This end-point is a pass-through; it simply calls to Lighthouse Health API Sandbox server for the token and returns the retrieved.\n\nAll other end-points identify the patient ICN from the patient query parameter. If the ICN is one of those for which Mock Fhir Bundles are available, the Mock Fhir Bundle is returned. Otherwise the Bundle is retrieved from Lighthouse Health API Sandbox server and returned.\n\nNote that this Mock Server itself does not validate the token when returning the Mock Bundles. For Sandbox Bundles, Mock Server passes the token to the Lighthouse server.\n\nEnvironment Variables\n\nThis Mock API uses the same Lighthouse Health API related environment variables that VRO application uses. You can find these environment variables in the application.yml mock-lh properties."
  },
  {
    "title": "Mock BIP Claims API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-BIP-Claims-API",
    "html": "The BIP APIs are not available outside of VA firewall for local development. There is a UAT BIP Claims API server within VA firewall but using it currently requires\n\nDeveloping on GFE's which have typically not enough resources for projects of VRO size\nInstalling and maintaining of Java requires special permissions\nOther security requirements makes it difficult to test https\nSince the server is not controlled by the VRO team creating and maintaining claims for test cases is difficult\n\nDue to these difficulties, a Mock BIP Claims API has been developed. This mock is now an integral part of the End to End Tests.\n\nHTTPS Server\n\nMock Claims API is an HTTPS enabled Spring Boot server. It uses self-signed certificates generated by the build-certificates.sh. In particular the PKCS#12 files client_truststore.p12 and server_keystore.p12 are placed in resources directory and pointed directly in application.yml server.ssl property to set up an https port 8097.\n\nNote that if you generate a new set of self-signed certificates using build-certificates.sh and change it in Mock BIP Claims API, the corresponding certificates in Application and Mock BIP Claim Evidence API subprojects must change as well.\n\nHealth Check\n\nThe standard Spring Boot Health Check is provided in http port 8080 and configured in application.yml management property.\n\nHTTP Server\n\nOne primary use of the Mock Claims API is the End to End Tests. Since these are black-box tests that do not have access to https setup within the VRO application, it is involved to have these tests to use the https port. To simplify the End to End Tests, an http port 8099 is made available. The full set of end-points are available through this port.\n\nThe implementation of opening the http port 8099 can be found here.\n\nOpen API Specification\n\nOpen API Specification of the Mock BIP Claims API is available from the Swagger page.\n\nThe primary purpose of the Mock BIP Claims API is to mock the end points that are used by the VRO application. Thus all the end-points listed in BIP Claims API API Calls section are implemented. The initial version of the api specification and model classes are generated using the Open API Generator using the Open API Specification of the BIP Claims API\n\nnpm install @openapitools/openapi-generator-cli -g \nnpx @openapitools/openapi-generator-cli generate -i bipclaim_3.1.1.json -g spring -o code\n\nThe Open API Specification bipclaim_3.1.1.json is available from the Swagger page by clicking on the small /v3/api-docs link under the title.\n\nThe initial version of the api specification has been changed very little. The model classes has been updated primarily to use Lombok instead of verbose getters and setters.\n\nFuture Work\n\nThe plan was to merge the model classes with the VRO proper model classes. But since the mock has been developed late, we abandoned the idea to reduce risk.\n\nSecurity Requirements\n\nThis mock implements a JWT based security configuration. At this time not all the claims are checked. However signature is validated in a JWT Request Filter.\n\nA JWT generator is included together with an interface to specify claims. This interface is implemented in this class whose property values is read from application.yml mock-bip-claims-api.jwt setting.\n\nFor testing purpose you can update the application-test.yml with development, prod or production JWT claim values and run a test such as Claims Test and JWT will be available in the logs. In principle you can use that JWT in curl or other tools to make API calls if you specify the certificates as well.\n\nFuture Work\n\nThe JWT generator implementation is mainly used in unit tests. It could be merged with the VRO implementation since it is a bit cleaner and more modular.\n\nTest Cases\n\nThis mock uses a simple Map store to hold all the Mock Claims. This Claim Store is initialized as a bean in Application Configuration. It reads all the Mock Claims from a json file. If you need new test cases you can add to this json file. The mock server does not watch the file updates so you will need to recompile and deploy.\n\nUpdates\n\nTo simplify to get insight into the state of claim objects an Updates Store has been created. This store is again initialized as a bean in Application Configuration and keeps track of calls to update contentions and lifecycle statuses per claim.\n\nThe controller for PUT /claims/{claimId}/contentions directly updates the state for contentions per claim and the controller for PUT /claims/{claimId}/lifecycle_status directly updates the state for lifecycle statuses per claim. The states are stored in a simple Set object that holds the updated claim ids.\n\nTo make the states accessible to End to End Tests which are black-box tests that run outside of the docker compose, this mock server provides two end-points\n\nGET /updates/{claimId}/contentions\nGET /updates/{claimId}/lifecycle_status\n\nThese end-points return a simple json objects of type ContentionUpdatesResponse and LifecycleUpdatesResponse. Both types provides a boolean property found which can be used to check if updates to contentions or lifecycle statuses per claim has happened. Contentions and Lifecycle status objects are also provided if found is true.\n\nThe updates are stored in a simple Set for claim ids. There are two consequences for this\n\nSame claim ids should not be used for multiple End to End Tests\nDuring local development if you run an End to End Test without deployment multiple times, you might get false positives\n\nTo remedy the first issue, End to End Tests include a self-check test to make sure the same claim id is not used by multiple tests. The second issue is remedied by a DELETE /updates/{claimId} end point which resets the update flags per claim id. This end point is called during the start of each End to End Test.\n\nAs with the actual mock end-points, updates end-points are available to End to End Tests and to local development through http port 8099."
  },
  {
    "title": "Mock BIP Claim Evidence API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-BIP-Claim-Evidence-API",
    "html": "The BIP APIs are not available outside of VA firewall for local development. There is a UAT BIP Claim Evidence API server within VA firewall but using it currently requires\n\nDeveloping on GFE's which have typically not enough resources for projects of VRO size\nInstalling and maintaining of Java requires special permissions\nOther security requirements makes it difficult to test https\nSince the server is not controlled by the VRO team creating and maintaining claims for test cases is difficult\n\nDue to these difficulties, a Mock BIP Claim Evidence API has been developed. This mock server was instrumental both testing of mTLS functionality and upload functionality. It is also now an integral part of the End to End Tests.\n\nHTTPS Server\n\nMock Claim Evidence API is an HTTPS enabled Spring Boot server. It uses self-signed certificates generated by the build-certificates.sh. In particular the PKCS#12 files client_truststore.p12 and server_keystore.p12 are placed in resources directory and pointed directly in the application.yml server.ssl property to set up an https port 8094.\n\nNote that if you generate a new set of self-signed certificates using build-certificates.sh and change it in Mock BIP Claim Evidence API, the corresponding certificates in Application and Mock BIP Claims API subprojects must change as well.\n\nHealth Check\n\nThe standard Spring Boot Health Check is provided in http port 8080 and configured in application.yml management property.\n\nHTTP Server\n\nOne of the primary use of the Mock Claim Evidence API server is the End to End Tests. Since these are black-box tests that do not have access to https setup within the VRO application, it is involved to have these tests to use the https port. To simplify the End to End Tests an http port 8096 is made available. The full set of end-points are available through this port.\n\nThe implementation of opening the http port 8096 can be found here.\n\nOpen API Specification\n\nMock BIP Claim Evidence API mocks the BIP Claim Evidence API end points that are used by the VRO application. These end-points are listed in BIP Claim Evidence API API Calls section and and also defined in Mock BIP Claim Evidence API specification. In fact the initial version of the api specification is generated using the Open API Generator\n\nnpm install @openapitools/openapi-generator-cli -g \nnpx @openapitools/openapi-generator-cli generate -i claimevidence_1.1.1_openapi.json -g spring -o code\n\nThe Open API Specification claimevidence_1.1.1_openapi.json is available from the BIP Claim Evidence API Swagger page by clicking on the small /v3/api-docs link under the title.\n\nThe initial version of the api specification has been changed very little for the end-points used by the VRO.\n\nSecurity Requirements\n\nThis mock implements a JWT based security configuration. At this time not all the claims are checked. However signature is validated in a JWT Request Filter.\n\nA JWT generator is included together with an interface to specify claims. This interface is implemented in this class whose property values is read from application.properties mock-bip-ce-api.jwt setting.\n\nFor testing purpose you can update the application-test.yml with development, prod or production JWT claim values and run a test such as Files Test and JWT will be available in the logs. In principle you can use that JWT in curl or other tools to make API calls if you specify the certificates as well.\n\nFuture Work\n\nThe JWT generator implementation is mainly used in unit tests. It could be merged with the VRO implementation since it is a bit cleaner and more modular.\n\nStorage\n\nThis mock uses a simple Map based store to hold the file content and meta data from request and UUID from response. This Basic Store is initialized as a bean in Application Configuration.\n\nReceived Files\n\nTo simplify to get access to the generated files from End to End Tests which are black-box tests that run outside of the docker compose, this mock server provides an additional end-point GET /received-files/{fileNumber} to download the generated files.\n\nNote that the storage Map uses Filenumber as a key so if you call the /files twice, the previous file will be overwritten."
  },
  {
    "title": "Local Setup ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Local-Setup",
    "html": "If you are on a Windows machine, take a look at Setup on Windows first.\n\nIDE\nRequired Dependencies\nDirectory setup\nSoftware Packages\nGitHub Permissions\nRunning the Application\nVerifying the Application is Running\nRunning app container locally\nIDE\n\nVRO is architected to be language agnostic though you should always confirm with maintainers that a language is supported before using it.\n\nIf a language is supported, you should feel free to use your preferred IDE for your language. However, it is recommended to use an IDE which supports an integration with Gradle (VRO's build system), like IntelliJ (Community Edition):\n\nhttps://www.jetbrains.com/help/idea/installation-guide.html\nhttps://www.jetbrains.com/products/compare/?product=idea-ce&product=idea.\nRequired Dependencies\nDirectory setup\n\nMake sure you have both abd-vro and abd-vro-dev-secrets cloned in sibling directories.\n\npath/to/code/\n         |-----abd-vro/\n         |-----abd-vro-dev-secrets/\n\nSoftware Packages\n\nSee Ubuntu VM Setup for detailed software setup instructions on Ubuntu.\n\nBefore you run this application locally, you will need to make sure you have all the following required dependencies available in your local environment:\n\nJava 17 (Mac Guide | Other OS Guide)\nGradle 8.1.1. Better yet, use ./gradlew\ndocker (ensure docker-compose version 2.X)\n\nNote for Apple Silicon Macs: export DOCKER_DEFAULT_PLATFORM=linux/amd64\n\nhadolint\nspectral\nshellcheck\n\nLHDI has additional setup documentation for Mac OS and Other Operating Systems.\n\nGitHub Permissions\n\nThis should no longer be needed:\n\nCreate a GitHub personal access token with read:packages permission.\nAdd the newly created token to your shell\n# add this to your shell profile (e.g. ~/.zprofile)\nexport GITHUB_ACCESS_TOKEN=<replace-with-token-from-github>\nThis is required to in order to download application artifacts that are published to the VA GitHub Package Registry.\n\nYou should also be added to the VA-ABD-RRD GitHub team.\n\nRunning the Application\n\nYou'll need port 5432 (default for postgres) available/not-in-use when running the code.\n\nAlso, be sure that you have a docker daemon running on your machine. If you use Docker Desktop, having the application running will run the daemon. However, if you are using a different Docker runtime like Colima, you will need to manually start the daemon process configure it to always run at boot time.\n\nOnce you have all the required dependencies, you can start the application in your local environment by navigating to the root of your application directory and running the following command:\n\n./gradlew clean\n./gradlew build check docker\n\nThis will build all the application artifacts and docker images.\n\nNote: Due to the way Gradle computes dependencies, the clean command must always be separate from build commands\n\nYou can then start the application by running (more details at Docker Compose):\n\n./gradlew :dockerComposeUp\n./gradlew :app:dockerComposeUp\n\nNote: Make sure you have completed the instructions in the abd-vro-dev-secrets README\n\nThis should bring up a docker container with the app running at http://localhost:8111\n\nVerifying the Application is Running\n\nYou can verify that the application is up and running by issuing the following commands in your terminal:\n\ncurl http://localhost:8111/actuator/health\ncurl http://localhost:8111/actuator/info\n\nYou should get back responses similar to the following:\n\ncurl http://localhost:8111/actuator/health\n\n{\n    \"status\":\"UP\",\n    \"components\":{\n        \"db\":{\n            \"status\":\"UP\",\n            \"details\":{\n                \"database\":\"PostgreSQL\",\n                \"validationQuery\":\"isValid()\"\n            }\n        },\n        \"diskSpace\":{\n            \"status\":\"UP\",\n            \"details\":{\n                \"total\":62725623808,\n                \"free\":53279326208,\n                \"threshold\":10485760,\n                \"exists\":true\n            }\n        },\n        \"livenessState\":{\n            \"status\":\"UP\"\n        },\n        \"ping\":{\n            \"status\":\"UP\"\n        },\n        \"readinessState\":{\n            \"status\":\"UP\"\n        }\n    },\n    \"groups\":[\n        \"liveness\",\n        \"readiness\"\n    ]\n}\ncurl http://localhost:8111/actuator/info\n\n{\n    \"app\": {\n        \"description\": \"Java API Starter from Template\",\n        \"name\": \"abd_vro\"\n    }\n}\nRunning app container locally\n\nThe above instructions should give you enough to get all the necessary docker containers running to test new functionality in abd-vro. However, developers may wish to run the vro-app-1 container with a run configuration to more quickly iterate on changes being made as opposed to performing a full ./gradlew dockerComposeUp every time.\n\nRefer to Docker Compose\nFind instructions for setting up a jetbrains run configuration for vro-app-1 HERE.\nTips\n\nRefer to Running Kafka Services/Tests Locally for information on how to run the bie-kafka microservice(s), specifically helpful for when there are issues with our messing system.\n\nRabbitMq occasionally segfaults during startup if using apple silicon. Restarting the container will resolve this issue.\nCheck if the svc-bie-kafka docker container is running and causing failures in mock-bie-kafka:dockerComposeUp, if not restart it.\nWIP\n\nBreakdown of RabbitMq Messaging, Exchanges, queues, and routes"
  },
  {
    "title": "Lightkeeper tool ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Lightkeeper-tool",
    "html": "Prerequisite: You must be a member of the VA-ABD-RRD GitHub team\n\nFollowed Getting Started instructions and GETTING-STARTED page to install lightkeeper.\n\nDevelopment Environment Step:\nExecute below command on a GFE (a machine within the VA network).\n  lightkeeper create clusterconfig nonprod > kube_config\nTransfer kube_config from GFE to development laptop (where kubectl is installed) and save it as ~/.kube/config.\n\nNOTE:- this config is for the `nonprod` cluster (containing `dev`, `qa`, and `sandbox` only )\n\nExecute below command for Prod cluster ( containing prod-test and prod),\nlightkeeper create clusterconfig prod > kube_config` \nsee Development environments#LHDI for cluster info.\n\nNOTE:- If you're switching between the two clusters, remember to use the right config file.\n\nRemember to specify the namespace (e.g., --namespace va-abd-rrd-dev) for all kubectl commands, e.g.:\n\n`kubectl get pods --namespace va-abd-rrd-dev`\n\n`helm list --namespace va-abd-rrd-dev`\n\nImportant\n\nKube config needs to be renewed every 90 days.\n\nBe cautious when running commands as this may affect others using VRO in these environments, particularly in the prod-test and prod environments where PII/PHI and various secrets are visible.\n\nTip: For a nice GUI to monitor the status, try the Lens app.\n\nWatch VRO logs\n\nTo follow the logs for the app container in the dev environment:\n\n# Use the namespace corresponding to the dev environment\nalias kc='kubectl -n va-abd-rrd-dev'\n\n# List the pods; we want the vro-api pod with 7 containers in it\nkc get pods\nNAME                                 READY   STATUS    RESTARTS      AGE\nvro-api-7ff6569c78-jj9zk             7/7     Running   2 (17m ago)   22m\nvro-api-postgres-7776cbd54f-wkf8r    1/1     Running   0             22m\nvro-api-rabbit-mq-7ff55bcb5f-6rg46   1/1     Running   0             22m\nvro-api-redis-555446854-ksgjt        1/1     Running   0             22m\n\n# Follow the logs for the abd-vro-api in the vro-api pod\nkc logs -f vro-api-7ff6569c78-jj9zk -c abd-vro-api\n\n  .   ____          _            __ _ _\n /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  '  |____| .__|_| |_|_| |_\\__, | / / / /\n =========|_|==============|___/=/_/_/_/\n :: Spring Boot ::                (v2.7.4)\n\n2022-11-18 18:47:11.835  INFO 1 --- [           main] gov.va.vro.VroApplication                : Starting VroApplication using Java 17.0.5 on vro-api-7ff6569c78-jj9zk with PID 1 (/project/vro-app.jar started by docker in /project)\n2022-11-18 18:47:11.839  INFO 1 --- [           main] gov.va.vro.VroApplication                : The following 2 profiles are active: \"compose\", \"dev\"\n2022-11-18 18:47:15.525  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Multiple Spring Data modules found, entering strict repository configuration mode\nMachine user login (DevOPS - Step only ):\n\nIMPORTANT:- This step is only for Automated GH workflows and GH Actions. Not for Development use.\n\nUse this step to generate kubernetes config for both prod and non-prod clusters\n\nExecute .\\lighkeeper login\nFollow the OTP Url generated by lightkeeper in a browser\nFollow instruction provided in section OTP https://github.com/department-of-veterans-affairs/abd-vro/wiki/Machine-User-Account to log in to GitHub\nGenerate *_KUBE_CONFIG secrets for GitHub Action workflows by executing\nlightkeeper create clusterconfig nonprod | base64 > DEV_KUBE_CONFIG\nlightkeeper create clusterconfig prod | base64 > PROD_KUBE_CONFIG\n\nUpdate DEV_KUBE_CONFIG and PROD_KUBE_CONFIG in GitHub secrets in git GH Action secrets\n\nThen test deploying to LHDI.\n\nIMPORTANT: Since the config expires in 90 days, the DEV_KUBE_CONFIG and PROD_KUBE_CONFIG in git GH Action secrets need to be updated regularly so that GitHub Action workflows that interact with LHDI continue to work."
  },
  {
    "title": "Machine User Account ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Machine-User-Account",
    "html": "The machine-user account for VRO is named abd-vro-machine. This machine-user account has been added to the VA's GitHub organization. (The only concern is the VA may kick this account out since there's no va.gov email address associated with it. The account is still active.)\n\nThis username will be seen in some GH Action workflow runs, e.g., for SecRel. This account is also used for generating kube configs.\n\nPATs\n\nThe account as several PATs (personal access tokens):\n\nto comment on ticket 28 to keep the machine-user account active -- ticket #28 and PR #762\nfor Dependabot PR checks so that the code can be built by Dependabot's PRs\nfor pulling and deleting GHCR images\nEmail account\n\nThe account is associated with an outlook.com email address with username abd-vro. The credentials for the email and GitHub accounts is stored in LHDI's HashiCorp Vault -- see Secrets Vault.\n\nOTP (One-Time Password)\n\nIn the github/abd-vro-machine-user folder of Secrets Vault, there is a 2FA code secret. Use that secret in an OTP Generator like https://it-tools.tech/otp-generator to generate the OTP number required to log into the GitHub abd-vro-machine-user account."
  },
  {
    "title": "Lighthouse APIs ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Lighthouse-APIs",
    "html": "We prefer to use Lighthouse APIs when possible. It usually provides the quickest path to production since approvals are minimal, it builds on existing working code, and there are fewer unknowns.\n\nSome APIs need CCG (machine-to-machine auth) to be enabled. Such features should be requested sooner rather than later.\n\nOf particular relevance to VRO:\n\nPatient Health API (FHIR) to get health evidence to support fast-tracking a claim\nBenefits Claims API to get claim data; details\nVeteran Verification APIs doesn't return veteran information like SSN, name, DOB. It will need to be expanded so VRO can get veteran information to generate a PDF; additionally, VRO needs the veteran's ICN -- see the MPI section below\n\nUseful resources\n\nAPI Backend Systems - backing VA system(s) for each Lighthouse API\nPatient Health API\n\nPertinent Secrets\n\nVRO_SECRETS_LH\n\nLH_ACCESS_CLIENT_ID\nLH_PRIVATE_KEY_BASE64"
  },
  {
    "title": "LHDI's Boilerplate Instructions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/LHDI's-Boilerplate-Instructions",
    "html": "VRO software is deployed on the Lighthouse Delivery Infrastructure (LHDI) platform, which offers tools, services, and support team.\n\nLHDI's Java Starter Kit was used to populate this codebase (see PR #8) using Java (AdoptOpenJDK) 17 and Gradle 7.4.\n\nLHDI's Boilerplate Setup Instructions\nAbout\nCI/CD Pipeline\nSecrets\nDefault Pipeline Behavior\nDependencies\nDeploying the Application\nWhat's Next\nAbout\n\nThis is a Java Spring Boot application that has been created using the Lighthouse DI Java 17 Starterkit. It is intended to be used as a starting point for building Java APIs and should be customized to deliver whatever functionality is required. If no other changes have been made, this application will have these features included by default.\n\nCI/CD Pipeline\n\nThis project comes with a skeleton Github Actions CI/CD pipeline out of the box. You can always choose to rewrite the pipeline using a different CI/CD tool; this pipeline serves as an example that you can use and run with minimal setup.\n\nSecrets\n\nIn order to run the pipeline, you will need to create a personal access token and add it to your repository's secrets in Github. The access token should have write:packages scope.\n\nThe secrets you need to configure are\n\nACCESS_TOKEN: the personal access token\nUSERNAME: the Github username of the user who owns the access token\nDefault Pipeline Behavior\n\nThe default pipeline has 3 jobs, which do the following things:\n\nRuns CIS benchmark tests against the application Docker image using docker-bench-security\nBuilds and tests application\nPublishes Docker image to VA GHCR repository\nDependencies\n\nThe pipeline runs on Github's ubuntu-latest runner, which is currently Ubuntu 20.04. The Github Actions Ubuntu 20.04 documentation lists the software installed by default. To learn more about choosing a Github runner and Github-hosted runner types, see the job.<job-id>.runs-on documentation.\n\nSoftware required for the pipeline but not installed by default, such as Java 17, hadolint, and spectral, is installed in the pipeline. The installation for app build dependencies is implemented as an action in <./.github/actions/setup-pipeline/action.yml>.\n\nDeploying the Application\n\nThe pipeline does not currently deploy the application to the DI Kubernetes clusters out of the box, although this setup will be coming in the future. To learn how to deploy your applications, see the DI ArgoCD docs.\n\nCommon Errors\n\nError: Cannot find plugin\n\nError Message:\n\n* What went wrong:\nPlugin [id: 'gov.va.starter.plugin.cookiecutter', version: '0.1.20', apply: false] was not found in any of the following sources:\n\n- Gradle Core Plugins (plugin is not in 'org.gradle' namespace)\n- Plugin Repositories (could not resolve plugin artifact 'gov.va.starter.plugin.cookiecutter:gov.va.starter.plugin.cookiecutter.gradle.plugin:0.1.20')\nSearched in the following repositories:\n    MavenLocal(file:/Users/aasare/.m2/repository/)\n    Gradle Central Plugin Repository\n    MavenRepo\n    BintrayJCenter\n    maven(https://palantir.bintray.com/releases)\n    maven2(https://dl.bintray.com/adesso/junit-insights)\n    starterBootPkgs(https://maven.pkg.github.com/department-of-veterans-affairs/lighthouse-di-starter-boot)\n    nexus(https://tools.health.dev-developer.va.gov/nexus)\n\n\nFix: Set your Github token as per the instructions in the Required Dependencies section above.\n\nError: Failed to get resource\n\nError Message:\n\nFailed to get resource: GET. [HTTP HTTP/1.1 401 Unauthorized: https://maven.pkg.github.com/department-of-veterans-affairs/lighthouse-di-starter-boot/starter/java/build-utils-property-conventions/starter.java.build-utils-property-conventions.gradle.plugin/0.1.32/starter.java.build-utils-property-conventions.gradle.plugin-0.1.32.pom)]\n\n\nFix: Set your Github token as per the instructions in the Required Dependencies section above, ensure that read:packages is true.\n\nWhat's Next\n\nOnce you have verified that you are able to run the application successfully, you can now start customizing the application to deliver the functionality you would like.\n\nBy default, this application assumes the use of a build, test, release cycle as defined in this development guide. Take a look at that guide to see how you can make changes, test them and get them deployed to a target environment.\n\nThe application itself is organized into the following three tiers of functionality:\n\nAPI\nService (business logic)\nPersistence\n\nTo see how each of these tiers is used by default, take a look at the Project Structure documentation."
  },
  {
    "title": "Kubernetes clusters ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Kubernetes-clusters",
    "html": "EKS/Kubernetes clusters\n\nFrom LHDI kubernetes docs, clusters are:\n\nldx-nonprod-1 (DEV, SANDBOX, QA)\nldx-prod-1 (PROD)\nldx-preview-1 (PREVIEW)\n\nAssociated gateways\n\nldx-nonprod (NONPROD)\nldx-nonprod-1 (NONPROD1)\nldx-dev (DEV)\nldx-prod-1 (PROD1)\nldx-preview-1 (PREVIEW1) (no gateway)\nldx-mapi-1 (MAPI-1)\n\nDNS:\n\ndev.lighthouse.va.gov\nsandbox.lighthouse.va.gov\nqa.lighthouse.va.gov\n(PROD) api.lighthouse.va.gov\n\nOutput from lightkeeper list team va-abd-rrd:\n\n{\n  \"name\": \"va-abd-rrd\",\n  \"id\": \"...\",\n  \"description\": \"va-abd-rrd description\",\n  \"namespaces\": [\n    {\n      \"name\": \"va-abd-rrd-prod\",\n      \"cluster\": \"ldx-prod-1\",\n      \"cpu\": 128,\n      \"ram\": \"256G\",\n      \"provided\": true,\n      \"imageSigningEnabled\": true\n    },\n    {\n      \"name\": \"va-abd-rrd-qa\",\n      \"cluster\": \"ldx-nonprod-1\",\n      \"cpu\": 128,\n      \"ram\": \"256G\",\n      \"provided\": true,\n      \"imageSigningEnabled\": false\n    },\n    {\n      \"name\": \"va-abd-rrd-sandbox\",\n      \"cluster\": \"ldx-nonprod-1\",\n      \"cpu\": 128,\n      \"ram\": \"256G\",\n      \"provided\": true,\n      \"imageSigningEnabled\": true\n    },\n    {\n      \"name\": \"va-abd-rrd-dev\",\n      \"cluster\": \"ldx-nonprod-1\",\n      \"cpu\": 128,\n      \"ram\": \"256G\",\n      \"provided\": true,\n      \"imageSigningEnabled\": false\n    }\n  ],\n  \"scm\": null,\n  \"artifact_store\": null,\n  \"pipeline\": null,\n  \"registry\": null,\n  \"reporting\": null,\n  \"observer\": null\n}\nS3, RDS, Gateways\n\nSee:\n\nLHDI's AWS Infrastructure as a Service\nLHDI - Managed Customer Resources"
  },
  {
    "title": "Jetbrains springBoot run configuration setup ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Jetbrains-springBoot-run-configuration-setup",
    "html": "Run Configuration\n\nWIP\n\nWhat?\n\nabd-vro system involves running numerous docker containers, which can be costly in terms of build time as well as system memory.\n\nThis guide serves as a set of instructions to run the vro-app-1 container as a run configuration for IntelliJ ide.\n\nThis allows developers to more quickly spin down and spin up new changes when testing the code.\n\nSetup\ncreate this directory on your system (via Terminal.app or in the terminal inside your IDE)\nPERSIST_TRACKING_FOLDER=/tmp/persist/tracking\nmkdir -p $PERSIST_TRACKING_FOLDER\n\nIn IntelliJ ide, navigate to top right -> edit configurations\nadd new run configuration for spring boot\nname it whatever you want\nunder environment variables set the following (comma separated)\nPERSIST_TRACKING_FOLDER=/tmp/persist/tracking\nPOSTGRES_URL=jdbc:postgresql://localhost:5432/vro\nREDIS_PLACEHOLDERS_HOST=localhost\nRABBITMQ_PLACEHOLDERS_HOST=localhost\nBIP_CLAIM_URL=localhost:8097\nBIP_EVIDENCE_URL=localhost:8094\nSLACK_EXCEPTION_WEBHOOK=http://localhost:9008/slack-messages\n\nConfiguration should look something like this\nmake sure you have all the docker containers running already (or build and run ./gradlew :app:dockerComposeUp now)\nmore details on running locally can be found HERE.\nmanually stop the vro-app-1 container, which we will run through our IntelliJ configuration\ndocker stop vro-app-1\n\nnow run it from the IDE w/ the big green play button"
  },
  {
    "title": "iMVP claim and contention update scenarios ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/iMVP-claim-and-contention-update-scenarios",
    "html": "Scope\n\nSingle issue NEW and INCREASE hypertension claims\n\nUpdates handled by MAS\n\nBefore VRO is notified of an automation eligible claim, MAS makes the following automation-specific updates to the claim/contention:\n\nClaim:\n\nTemporary Station of Jurisdiction = 398\n\nContention:\n\nSpecial Issue = Rating Decision Review - Level 1 (RDR1)\nSpecial Issue = RRD\nAutomation Indicator = False\nVRO scenarios that require updating the claim/contention\n\nHappy path\n\nIn order for an automated claim to be routed and tracked appropriately after processing has completed in VRO, the claim/contention needs to be updated:\n\nField (object)\tTemporary Station of Jurisdiction (claim)\tLifecycle Status (claim)\tAutomation Indicator (contention)\tLifecycle Status (contention)\tSpecial Issues (contention)\nField value when claim is RFD\t398\tRFD\tTrue\tRFD\tRRD (remove RDR1)\nField value when exam is ordered\t398\tOpen\tTrue\tOpen\tRRD (remove RDR1)\n\nOfframp (for any reason)\n\nIn order for an automation eligible claim to be processed manually when offramped by VRO, updates made to the claim/contention as part of automation processing need to be reverted:\n\nField (object)\tTemporary Station of Jurisdiction (claim)\tLifecycle Status (claim)\tAutomation Indicator (contention)\tLifecycle Status (contention)\tSpecial Issues (contention)\nField value for any offramped claim\t398\tOpen\tFalse\tOpen\tDo NOT include Rating Decision Review - Level 1 (RDR1) or RRD\n\n"
  },
  {
    "title": "Helm Charts ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Helm-Charts",
    "html": "VRO uses Helm Charts to specify a deployment to LHDI. The charts are located in the helm folder.\n\nGitHub Action workflows call helm to deploy to various LHDI environments -- see CI CD Workflows.\n\nConfigurable Settings\n\nThe values.yaml contains the default settings for the dev environment. Other values-for-*.yaml files override some settings in the values.yaml. For example, values-for-prod.yaml overrides the size of the persistent volumes for the prod environment.\n\nBefore adding a Helm configuration, read Configuration-settings#guidelines-for-placing-settings.\n\nThe values are referenced in charts and the _helper.tpl, which is used by charts to reduce repetition and consolidate common settings into a single location. All containers share a common set of configuration settings, so changes can be consistently and easily applied to all containers.\n\nAdditionally, each chart has it's own values.yaml file, which can be overridden by using the setting global.<chartName>.someSetting, where <chartName> corresponds to the name within the corresponding Chart.yaml file. For example, overriding the value of global.rabbitmq-chart.imageTag would change the imageTag value used in the rabbitmq subchart.\n\nSee the Update deployment GitHub Action workflow for an example of how settings are overridden. Using this mechanism, sets of containers can be enabled and disabled.\n\nPersistent Volumes\n\nLHDI offers 2 types of persistent volumes (PVs):\n\nEFS\npros: allows mounting from any container; any container user can write to it but all files are owned by the same user (as determined by LHDI when the PV is provisioned)\ncons: unmodifiable folder ownership (Slack)\nEBS\npros: allows changing folder ownership\ncons: can only be mounted by containers in the same Kubernetes node; only supports ReadWriteOnce, which implies:\n\n\"the volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access the volume when the pods are running on the same node.\"\n\nVRO creates 2 PVs for different purposes:\n\ntracking: EFS volume mounted by the rabbitmq and app containers to tracking incoming requests for diagnostics and recovery\npostgres data (pgdata): EBS volume mounted by the postgres container to retain DB data between redeploys. The postgres container requires that the data folder be owned by the postgres user, hence EFS cannot be used.\n\nThe VRO Console mounts these PVs for diagnostics.\n\nWe do not have visibility into the PVs themselves -- LHDI manages the PVs. We must specify persistent volume claims (PVCs) in order to get PVs.\n\nResources\n\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\nhttps://www.datree.io/resources/kubernetes-troubleshooting-fixing-persistentvolumeclaims-error\nStatefulSet\n\nSince the pgdata is an EBS volume, the postgres and console containers must run in the same node as the EBS. To satisfy this constraint, a StatefulSet is created for the postgres container and a podAffinity is applied to the console container.\n\nWhen updating a deployment (using helm upgrade), StatefulSets causes an error. A workaround is implemented to avoid this error.\n\nSecrets\n\nSee Secrets Vault"
  },
  {
    "title": "Health data assessment containers ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Health-data-assessment-containers",
    "html": "Health data assessment containers\n\nThe code for health data assessment and analysis is in the python-service folder. Each folder is designed to run inside its own docker container and communicate with other parts of the application using RabbitMQ. In general, the assessment services loop through a Veterans health data (medications, observation, conditions, procedures etc) and pull out anything relevant to a claimed contention (\"relevant\" in this case means anything that may help an RVSR rate a claim). The assessment services also provide summary statistics as meta-data that gets stored in the DB. Refer to Plan to Deploy for more general information on \"Queue-Processor\" architecture.\n\nAssessment of health data for hypertension\n\nThis container handles all claims with contentions for hypertension (7101 VASRD). There are two queues set up by this container, health-assess.hypertension for v1 functionality and health-sufficiency-assess.hypertension for v2. The queue is named this way because the v2 queue will return a flag to describe the sufficiency of evidence for a given claim (the logic is described in 905 ). The health-sufficiency-assess.hypertension queue leads to a function that is designed to handle combined data from MAS and Lighthouse in a robust fashion. The assessment service does formatting of dates for the PDF, since it is already parsing a date as part of its core functionality. Medications, blood pressure readings and conditions are sorted by date before being returned as an evidence object. Blood pressure readings without a parse-able date are not considered algorithmically because there is no feasible way of determining whether they meet the date requirements (without a lot of error-prone custom string parsing, and the vast majority of data that will be flowing through the VRO will have parse-able dates). Medications and conditions without a date are attached at the end of the sorted list of objects with dates.\n\nHealth Data validation requirements\n\nSome \"keys\" in the request message are validated before any algorithms are run to alleviate the need to constantly catch errors. Some data cleaning is assumed by the upstream services for data collection. To accommodate MAS and Lighthouse as data sources, most dates are not assumed to always be in the correct format. For example some primary dates from MAS could be missing from OCR data and instead a secondary, or \"partial\" date will be used. For the hypertension queues, \"bp_readings\" is required to be present and for each blood pressure reading, a diastolic and systolic value must be present. The cerberus Python package is used as a lightweight validator.\n\nSufficiency to set RRD status\n\nTo determine sufficiency a collection of algorithms analysis the patient data to determine a few decision points. The patient health data will include diagnosis information as code-able concepts in ICD or SNOMED. The ICD 10 codes {\"I10\", \"401.0\", \"401.1\", \"401.9\"} are used to filter the complete patient record down to just the relevant objects. In addition, blood pressure measurements are filtered by date and value.\n\nMore information on the logic to determine sufficiency can be found here 905 )\n\nAssessment of health data for asthma\n\nThis container handles claims with contentions for asthma (6602 VASRD). On startup it creates a queue, health-assess.asthma which is only used by v1 endpoints.\n\nAll other assessment folders\n\nThe other assessment folders are included in a gradle build if it is given a special property and the docker-compose profile is set to prototype. Use the following commands to build and run those containers.\n\n./gradlew -PenablePrototype build check docker\n\nexport COMPOSE_PROFILES=prototype\n\nTesting and Development\n\nThere is unit testing for each assessment service in service-python/tests/assessclaim.\n\nTo test additional conditions, the VASRD code needs to be added to the dpToDomains object in svc-lighthouse-api/src/main/java/gov/va/vro/abddataaccess/service/FhirClient.java. The dpToDomains map determines which resources to pull from the Lighthouse Patient Health API and later sent to the assessment services."
  },
  {
    "title": "Gradle ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Gradle",
    "html": "Gradle is used to specify Java dependencies, lint code, compile code, build Docker images, etc.\n\nThere's lots of tutorials online. Here are some tips:\n\nhttps://www.youtube.com/playlist?list=PL0UJI1nZ56yAHv9H9kZA6vat4N1kSRGis\nhttps://www.youtube.com/watch?v=5v92GbC9cqo\nBash alias\n\nSince it's used frequently, a Bash alias is useful (alias g='$PWD/gradlew'), which reduces typing g build. (The $PWD enables g to be run from any folder.)\n\nAdditional aliases can be created:\n\nalias g=\"$PWD/gradlew\"\nalias delint='g spotlessApply'\nalias gbuild='delint && g build'\nalias gdocker='g build && g docker'\nalias gcdocker='g clean && gdocker'\n\nalias gdcDown='g dockerComposeDown'\nalias gdcUp='g dockerComposeUp'\nalias gdcRestart='g dockerComposeDown dockerComposeUp'\nImportant Files to Review\nsettings.gradle: includes subdirectories as Gradle subprojects\nbuild.gradle in the project root, api, and app subdirectories\nbuildSrc subdirectory: LHDI Starter Kit provides Gradle plugins; details in the README.md.\nGradle library dependencies\n\nTo see the current dependencies between subprojects, navigate to the root of the repo and run: ./gradlew :<GRADLE_PROJECT_NAME>:dependencies --configuration compileClasspath | grep \"project :\"\n\nTo see all (including 3rd-party) of the dependencies for a gradle subproject run: ./gradlew :<GRADLE_PROJECT_NAME>:dependencies --configuration compileClasspath > deps.txt\n\nThis command will print the dependency tree for the supplied subproject to the deps.txt file.\n\nGradle tasks\n./gradlew tasks\n./gradlew tasks --all\n\nhttps://gitlab.com/barfuin/gradle-taskinfo:\n\n./gradlew tiTree assemble\n./gradlew tiOrder docker\nJacoco Test coverage report\n\nJacoco is used to report test coverage for Java -- see https://rhamedy.medium.com/how-to-setup-jacoco-code-coverage-with-maven-gradle-76e0b2fca9fb.\n\nAt the root project folder:\n\n./gradlew test jacocoTestReport\n\n\nThen open build/reports/jacoco/jacocoAggregatedReport/html/index.html in a browser.\n\nFor a subproject, for example:\n\ncd svc-bip-api/\n./gradlew test jacocoTestReport\n\n\nThen open build/jacoco/jacocoAggregatedReport/html/index.html.\n\nbuildSrc\n\nIf you're looking for a buildSrc folder, use the gradle-plugins folder instead -- it functions equivalently but is reusable and results in faster builds.\n\nComposite Builds\n\nThe codebase use Composite Builds to partition and decouple Gradle projects, resulting in faster builds.\n\nThe current composite builds are:\n\ngradle-plugins: only used for building the code; this is included by other Gradle projects\nmocks: for projects that represent external APIs or services\n(root): the main VRO codebase\n\nA composite build is built independently unless it is referenced by an includeBuild in settings.gradle. For example, since gradle-plugins is included by other Gradle projects, it is re-built automatically when it changes. However, mocks will not be rebuilt unless the Gradle build task is explicitly run against mocks folder ./gradlew -p mocks build or:\n\ncd mocks\n./gradlew build\n\n\nSince mocks rarely need to be rebuilt (and are not integral to the deployed VRO), this will reduce build time."
  },
  {
    "title": "Github Actions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Github-Actions",
    "html": "GitHub Actions are defined in the .github/workflows folder, and their executions are in the Actions tab.\n\nBuild and Publish\n\nNo longer required: GitHub actions that build VRO code or publish VRO images require credentials of someone in the VA-ABD-RRD Team.\n\nThis was replaced in #925: @yoomlam added his USERNAME and ACCESS_TOKEN secrets to the repo's Secrets settings\nWhen publishing, resulting packages (i.e., images for Docker containers) are tagged using the first 7 characters of the PR's commit hash, which are needed for deployment to LHDI's Kubernetes clusters\nCodeQL\n\nThe VA GitHub.com team requested that Advanced Security Code Scanning be enabled via codeql.yml. See PR #116 for details.\n\nThis is a compliment to any tools and security procedures your team is already performing rather than a replacement. ... this feature will identify potential security issues on any new pull requests. ... For more information, check out the GitHub Handbook.\n\n(If needed, an admin for this repo can bypass any identified issues.)\n\nIn commit ea4be65, we limit this action from running to certain PR event types (e.g., ready_for_review, review_requested) and for pushes to special branches since this action is slow. To manually run the action, do any of the following:\n\nRequest a review from someone.\nConvert it to a draft PR, then click the \"Ready for Review\" button.\nGo to the CodeQL Action and click \"Run workflow\" on the desired branch.\nMerge the PR to a special branch (develop).\nMirror\n\nThe Lighthouse SecRel (Secure Release) process operates only on non-public repos. To satisfy that requirement, a abd-vro-internal repo was created as a mirror of this repo. The internal repo will only be used by the Lighthouse SecRel team to enable deployments to prod -- no one should be committing to or creating PRs directly in that repo. The mirror.yml GitHub Action updates the internal repo whenever a branch in this repo is created, updated, or deleted.\n\n@yoomlam generated a password-less ssh key pairs (using bogus email mirror@abd-vro.va.gov) and added a SSH_PRIVATE_KEY secret to the repo's Secrets settings\nIn order for the GitHub Action to access the abd-vro-internal repo via ssh, the public key was added to Deploy keys using the abd-vro-machine account, which will attribute triggered actions (e.g., SecRel) in the abd-vro-internal repo to the machine-user account.\n\nPR #680 makes the following unnecessary, but keeping it for reference:\n\nPR #250 limits this action from running to certain PR event types (e.g., ready_for_review, review_requested) and for pushes to special branches since this action is causes extraneous checks to run in the internal repo. To manually run the action, do any of the following:\n\nRequest a review from someone.\nConvert it to a draft PR, then click the \"Ready for Review\" button.\nGo to the Mirror Action and click \"Run workflow\" on any branch (all branches will be mirrored)\nor run curl -XPOST -u \"$GITHUB_USERNAME:$GITHUB_ACCESS_TOKEN\" -H \"Accept: application/vnd.github+json\" -H \"Content-Type: application/json\" https://api.github.com/repos/department-of-veterans-affairs/abd-vro/actions/workflows/mirror.yml/dispatches --data \"{\\\"ref\\\": \\\"develop\\\"}\"\nor run gh workflow run mirror.yml --ref develop\nMerge the PR to a special branch (develop).\nSecRel\n\nPR Enable SecRel workflow #235 adds new actions (secrel.yml and aqua-checker.yml) for VRO's code to go through Lighthouse's SecRel pipeline to be deployable to production -- see Secure Release GitHub Actions for details.\n\nThe \"SecRel workflow\" action will be automatically triggered in the internal repo by pushes to develop and main.\n\nTo manually trigger the SecRel workflow on a PR, see To test PRs in the SecRel pipeline."
  },
  {
    "title": "End to End Tests ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/End-to-End-Tests",
    "html": "End-to-end Tests are black-box integration tests.\n\nExternal systems are primarily simulated using Mock Services. The main reason to use Mock Services instead of the test servers of the external systems are:\n\nData in test servers of the external systems do no line up. For example a claim id from MAS API development server cannot be found in the UAT server of BIP Claims API.\nVRO team does not have control over test data on external test servers.\nBIP Claims API and BIP Claim Evidence API do not have servers that are accessible outside of VA.\nInteractions are more easily recorded in Mock Services for verification.\nImplementation\n\nEnd-to-end Tests require almost all VRO containers specified in the VRO docker-compose file to be running. Only those marked as prototype can be ignored.\n\nAn environment setting script is provided. You must run this script before building the project.\n\nsource setenv-e2e-test.sh\n\nOnce the environment is set you can build VRO and run the containers as usual.\n\nEnd-to-end Tests are configured as Integration Tests as described here. After all the containers are run using ./gradlew :app:dockerComposeUp, you can run the end-to-end test task:\n\n./gradlew :app:end2endTest\n\nThis will run all the end-to-end tests and report success or provide an html link for errors.\n\nAction\n\nEnd-to-tests are automatically run when a PR is merged to develop or when a PR is assigned a reviewer. Automation is achieved using a Github action. This PR: End-to-end test for v2 action can also be run manually.\n\nMAS Integration (V2 - iMVP) Tests\n\nAll iMVP End-to-end Tests are in the Java file VroV2Tests.java. The tests are implemented as JUnit 5 tests.\n\nEnd-to-end Tests for iMVP aims to verify most, if not all, of Road Map for iMVP. There are two types of end-to-end tests: positive and negative. Positive tests verify the paths in the Road Map for iMVP when they are arrive at completion as opposed to negative tests that verify error cases where any of the paths are interrupted due to errors.\n\nPositive Tests\n\nEach positive End-to-end Test is driven by a JSON file in the resource directory test-mas. Each JSON file is the mocked payload to the VRO /automatedClaim end-point which is the primary interface to MAS for iMVP.\n\nCritical fields in each JSON file are\n\ncollectionId: the id of the collection for which annotations are retrieved. The collection can either exist in the MAS development server (currently 350 only) or be available from the Mock Mas API.\nveteranIdentifiers.icn: the ICN for the patient. The patient can either exist in the Lighthouse Health API Sandbox server or be available from Mock Lighthouse Health API\nclaimDetail.benefitClaimId: the id for the claim. The claim has to be available from Mock Bip Claims API.\nveteranIdentifiers.veteranFileId: the file id for the veteran. This id is used to generate and identify generated PDF's.\n\nThe tests calls to /automatedClaim end-point with the payload and verify the response and the expected events. The events that happen are queried using end-points in Mock Services or VRO Claim Metrics API.\n\nThe following are the primary verifications\n\nResponse code\nPDF Generation: Mock BIP Claim Evidence API is queried to make sure the evidence PDF is generated when necessary. The generated PDF is downloaded from Mock BIP Claim Evidence API and existence of Veteran name and last name verified using a simple parse.\nExam Ordering: Mock MAS API is queried to make sure exam is ordered for the collection when needed.\nSlack Messaging: Mock Slack Server is queried to make sure a slack message is sent when needed.\nClaim Updates: Mock BIP Claims API is queried to make sure the claim and contention records are updated when necessary.\nDatabase Updates: VRO Claim Metrics end-points are used to make sure database is populated as expected.\n\nAll positive End-to-end Tests resets the records in Mock Services for the collection under test to enable multiple running of the test during local development.\n\nNegative Tests\n\nNegative tests are for conditions that generate 4xx and 5xx status codes. All the negative tests are in VroV2Tests.java as well. The following are some of the examples:\n\nCheck if VRO is sanitizing the request by filtering disallowed characters such as those that are not printable.\nCheck if a non-existent claim id is validated.\nAPI (V1) Tests\n\nAll API (V1) End-to-end Tests are in the Java file VroV1Tests.java. The tests are implemented as JUnit 5 tests.\n\nEnd-to-end Tests for MVP aims to verify most, if not all, of Road Map for MVP. There are two types of end-to-end tests: positive and negative. Positive tests verify the paths in the Road Map for MVP when they are arrive at completion as opposed to negative tests that verify error cases where any of the paths are interrupted due to errors.\n\nPositive Tests\n\nThere are two positive End-to-end Tests.\n\nBoth tests use existing records from Lighthouse Health API Sandbox server.\n\nThe first test is driven by the JSON files in the resource directory test-7101-01 in particular assessment.json which is the expected response from /full-health-data-assessment end-point for a hypertension case. The input payload for /full-health-data-assessment is prepared from assessment.json, the end-point is exercised, and the response is compared to assessment.json. The second step of the test exercises /evidence-pdf. The input payload is formed using data retrieved from the /full-health-data-assessment call and the file veteranInfo.json. The generated PDF file is sanity checked by comparing pdf data to fields in assessment.json and veteranInfo.json.\n\nThe second test is similar but verifies responses for an Asthma case. This test case is driven by the JSON file in the resource directory test-6602-01. Otherwise the methodolody is identical to the first test case and in fact both share majority of the test code.\n\nNegative Tests\n\nNegative tests are for conditions that generate 4xx and 5xx status codes. All the negative tests are in VroV1Tests.java as well. The following are some of the examples:\n\nCheck if authorization fails without an API key."
  },
  {
    "title": "Entity Relationship Diagram (ERD) ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Entity-Relationship-Diagram-(ERD)",
    "html": ""
  },
  {
    "title": "EE ‚Äê Build and Deploy Process ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/EE-%E2%80%90-Build-and-Deploy-Process",
    "html": "Build and deploy EE to both prod and non-prod environments EE follows same CI/CD process as VRO. Detailed documentations can be found with below references\n\nReference: https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deployment-Overview https://github.com/department-of-veterans-affairs/abd-vro/wiki/CI-CD-Workflows https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deploying-VRO https://github.com/department-of-veterans-affairs/abd-vro/wiki/Container-Image-Versions\n\nDeploy to Non-Prod\n\nWhen the PR is merged to develop branch CI process gets triggered\n\nDeployment to Non-Prod\nNavigate to https://github.com/department-of-veterans-affairs/abd-vro-internal/actions/workflows/update-deployment-ee.yml\nSelect Domain-EE application, Select target env. Run the workflow\nDeploy to Prod\n- Prerequisite\nImages has to be signed before deploying to Sandbox and higher environments\nSecrel job has to succeed. If there is sec-rel issue then visit https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deploying-VRO#verify-and-fix-secrel, Follow steps listed in section Verify and Fix SecRel\nMake sure you sync secrets from vault to cluster using https://github.com/department-of-veterans-affairs/abd-vro-internal/actions/workflows/deploy-secrets.yml\nFollow Release process listed in https://github.com/department-of-veterans-affairs/abd-vro/wiki/Release-Process\nRun workflow https://github.com/department-of-veterans-affairs/abd-vro-internal/actions/workflows/update-deployment-ee.yml, Select target environment and deploy\nVerify Applications are up and running"
  },
  {
    "title": "Docker containers ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Docker-containers",
    "html": "Docker containers\n\nThe values.yaml file shows a list of container images that VRO creates and are ultimately deployed to Kubernetes.\n\nPackages\n\n(Packages in Github Container Registry)\n\nInitially, there were 2 container images (a.k.a. \"packages\" in Github's lingo) set up by LHDI's Starterkit:\n\nabd-vro/abd_vro-app\nabd-vro/abd_vro-db-init\nabd-vro/abd_vro-assessclaimdc7101\nabd-vro/abd_vro-pdfgenerator\nabd-vro/abd_vro-service-data-access\n\nCurrently, there are more packages associated with this repo. Packages are listed on the side panel of the VRO repo.\n\nUPDATE: PR #65 makes it such that packages are automatically associated with this repo, but the package needs to be manually set to \"Inherit access from source repository\" as instructed by LHDI doc.\n\nOBSOLETE: If more are created, go to the VA Organization's Packages page, search for the package, and manually connect them to this repo -- see LHDI Development Guide.\nDocker.com rate limits\n\nDue to Docker.com rate limits, PR #67 and PR #68 pulls container images (e.g., postgres and rabbitmq) from Docker.com (i.e., DockerHub, docker.io), then tags and pushes them to Github Container Registry (ghcr). LHDI has ideas to create mirrors of commonly used images, so that people can access those without getting rate limited (as well as being well-vetted images), however the timeline for that is unclear. Until then, we'll push them to ghcr so that LHDI can pull the images without any limit.\n\nThese packages (unchanged from Docker.com) need \"Inherit access from source repository\" to be set manually (instructions in LHDI Development Guide), and must be manually connected to this repo because we do not modify the Dockerfile LABEL (LABEL org.opencontainers.image.source=https://github.com/department-of-veterans-affairs/abd-vro), which would automatically associate the image with the repo.\n\nGradle Docker plugin\n\nTo build the container images, this project uses Palantir's Gradle Docker plugin -- https://github.com/palantir/gradle-docker.\n\nUses of container images\n\nThe following subsections describe uses of container images.\n\nLHDI deployments\n\nUsed in LHDI's Kubernetes clusters, the images are retrieved and deployed to dev and separately deployed to production.\n\nIn the Kubernetes clusters, the app (or abd_vro-app) docker container depends on init containers (which run and exit before the app container is started):\n\ncontainer-init: init_pg.sql\ndb-init (or abd_vro-db-init): flyway DB migrations\nopa-init: rego policies and permissions\n\nAs mentioned in the packages section above, the application-specific container images (abd_vro-app and abd_vro-db-init) are packaged, tagged, and pushed to Github Container Registry by VRO's Github Actions. The other (non-abd_vro) container images (such as pg-ready, istio-init, and istio-proxy) are provided by LHDI.\n\nPushed by Github Action\n\nUsed by publish.yml Github Action to push container images (\"packages\") to the Github Container Registry -- see Github Actions.\n\nLocal development\n\nUsed to set up a local environment for code development and testing. The dockerComposeUp task in app/build.gradle starts Docker containers locally using your local code modifications. See Development process for details.\n\nApproved images\n\nWhen deploying an image from Docker, use Docker official images.\n\nWhen defining a new image with a Dockerfile, use a base image from Docker official images.\n\nWhy? See LHDI docs. Either use a Docker official image, or have it scanned and signed in the Secure Release process by including it in .github/secrel/config.yml.\n\nAdding a container for a new microservice\nAdd a new Gradle subproject\nInclude your new service inside of settings.gradle\nInclude your new service inside of build.gradle\nFor local development, add your container to app/docker-compose.yml so that your container is started when running VRO. This file has several helpers and placeholder vars to ensure consistency between services and containers (rabbitmq, redis, etc). Note the service and image names will be prefixed with svc and abd_vro respectively, e.g.:\nsvc-foo:\n    image: va/abd_vro-foo:latest\n\n\nAfter creating and updating the files above, run these commands:\n\n./gradlew build check docker\n# to start VRO\n./gradlew :dockerComposeUp :app:dockerComposeUp\n\n\nYou should see Container docker-svc-foo-1 Started in the output of all containers starting up.\n\nAdding a container for a new python microservice\n\nTo add a new python microservice with docker container, there are a few steps & files that need to be updated.\n\nFirst, create the new microservice files inside of service-python. It's okay if these are stubbed to begin with, full functionality is not needed in order to create the container.\n\nFor a microservice named foo, you will need to add these files:\n\nservice-python/foo/__init__.py\nservice-python/foo/build.gradle\nThis Gradle file can be barebones, all that is needed is plugins { id 'local.python.container-service-conventions' } to re-use the common settings for all python services. Refer to the other service-python/*/build.gradle files.\nservice-python/foo/src/requirements.txt the dependency requirements for your service\nservice-python/foo/src/lib/ as the directory where your python code will actually live. This will have another __init__.py and likely a main.py, utils.py, settings.py, etc. as required by your services.\n(Optional) service-python/foo/docker-compose.yml with the necessary services listed (rabbitmq, redis, etc)\nservice-python/Dockerfile will be used to create the container image for the service using files in the build/docker/ subfolder (populated after running ./gradlew service-python:foo:docker). Using a custom Dockerfile is possible but will require special Gradle and SecRel config.yml configurations.\n\nOutside of your specific service, a few other files need to be updated:\n\nInclude your requirements in service-python/requirements.txt\nAdd your container to VRO configs -- see section above\nAdd your container to deployment configs -- see section below\nAdding a container for deployment\nUpdate scripts/image-names.sh:\nadd the folder's basename to IMAGES Bash array\nadd to various the bash functions's case statement if the folder is not at the project root-level\nRun scripts/image-names.sh and review the changes\nIf changes look good, update the files manually or like so:\ncp .github/secrel/config-updated.yml .github/secrel/config.yml\ncp helmchart/values-updated.yaml helmchart/values.yaml\n\nAdd changes, including scripts/image_vars.src, and commit changes to git\nUpdate helmchart/templates/api/deployment.yaml, which prescribes how VRO is to be deployed to LHDI\nCommit all changes, push, and create a PR\nRun the Deploy-Dev action on the PR's branch\nOnce the action completes:\nVerify that the image (\"package\") was pushed to GHCR: https://github.com/orgs/department-of-veterans-affairs/packages?tab=packages&q=dev_vro-. Click on your image and note there are 0 downloads.\nThe new container image needs to be manually set to \"Inherit access from source repository\" as instructed by LHDI doc.\nCheck for successful LHDI deployment to the DEV namespace by\nLooking for non-zero downloads of your package. LHDI should have downloaded it.\nOr connecting to LHDI's EKS cluster, e.g., using the Lens app"
  },
  {
    "title": "Domain Applications in VRO ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Domain-Applications-in-VRO",
    "html": "This page is a reference to pages for specific domains that are built on top of the VRO platform.\n\nAs new domains are onboarded to the VRO platform, application developers should create a new wiki page and link it here along with a short description. Code for a domain ABC can be found in the domain-ABC folder at the root of the repo.\n\nxample - an example domain to illustrate proper folder and dependency structures\ncc -"
  },
  {
    "title": "Deprecated Pages ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deprecated-Pages",
    "html": "VRO failure runbooks\n(March 2022) Plan to Deploy to LHDI\nCircleCI\nMAS api spec (IBM hosted api)\nMAS api spec (VRO hosted api)\nPDF Generator\nMAS Integration Camel Routes\nExternal APIs (Partial)\nInitial Roadmap for VRO's RRD Implementation\nAmida Hand-off Resources\nProblems being solved\nPOCs\nCurrent Software State (Oct. 2022)"
  },
  {
    "title": "Docker Compose ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Docker-Compose",
    "html": "There are various other Docker setups depending on your needs, this page provides some guidance and typical setups to use during development. It also provides additional details for you to customize your own Docker setup, depending on your current development focus.\n\nDocker Compose is used to define multi-container Docker setups for local development and integration tests. It sets up a Docker network, so that containers can readily connect using obvious hostnames rather than localhost. It also sets up shared volumes to simulate LHDI deployment environments.\n\nDocker setups:\n\nPlatform Base setup (docker-compose.yml): includes Postgres, Redis, RabbitMQ, API Gateway. This creates Docker volumes and a network required by other setups.\nApp setup (app/docker-compose.yml): includes the Java-based VRO App and platform (domain-independent) microservices\nMocks setup (mocks/docker-compose.yml): defines containers that act as substitutes for External APIs and services; only used only for development and integration testing\nDomain setups (domain-*/docker-compose.yml): each defines containers for their respective domain\n\nFor configurability, a docker-compose.yml file can use compose profiles to run a subset of defined containers. Examine the file to see what profiles are available for each Docker setup. The environment variable COMPOSE_PROFILES determines which containers are started by docker-compose (or docker compose). This allows developers to start only the containers they need for their current work.\n\nFor even further customization, create a local docker-compose.override.yml (docs) but do commit it to git.\n\nThe following instructions, assume the Docker container images have been built: ./gradlew docker\n\nThe Gradle tasks to start up and shut down the Docker setups are dockerComposeUp and dockerComposeDown. Alternatively, the equivalent docker compose command can be used instead.\n\nPlatform Base\nStart up: ./gradlew :dockerComposeUp (Note the :, which will run the dockerComposeUp task in only the root Gradle project).\nEquivalent to docker compose up -d\nCheck that all containers are healthy: docker ps -a\nManually test: visit the RabbitMQ Management UI at http://localhost:15672/\nShut down: ./gradlew :dockerComposeDown or ./gradlew :dockerComposeDown :dockerPruneVolume\nEquivalent to docker compose down or docker compose down --volume (respectively)\nPlatform Base + API-Gateway\n\nThe Platform Base defines an API Gateway container but it is not needed for typical development, so it is not started by default. To start it, set COMPOSE_PROFILES:\n\nStart up: COMPOSE_PROFILES=\"gateway\" ./gradlew :dockerComposeUp\nNote: Setting COMPOSE_PROFILES on the same command line as running ./gradlew will set the variable to the specified value for only that Gradle execution. In other words, if COMPOSE_PROFILES was set prior to running the above command, its value remains unchanged.\nTo check it out, visit http://localhost:8060/\nShut down: ./gradlew :dockerComposeDown\nTo shut down only the api-gateway container:\ndocker compose stop api-gateway\ndocker compose rm -f api-gateway\n\nPlatform Base + Java App\nPrep: Start up the Platform Base setup (using instructions in the prior section)\nStart up: ./gradlew :app:dockerComposeUp (Note the :app, which will run the dockerComposeUp task in only the app Gradle project.)\nEquivalent to cd app; docker compose up -d\nEquivalent to docker compose -f app/docker-compose.yml up -d\nAfter a couple of minutes, visit http://localhost:8110/v3/api-docs or http://localhost:8110/swagger\nShut down: ./gradlew :app:dockerComposeDown\nEquivalent to docker compose -f app/docker-compose.yml down\nOptionally, shut down the Platform Base setup (using instructions in the prior section)\nPlatform Base + Java App with Platform Microservices\nPrep: Start up the Platform Base setup (using instructions in the prior section)\nTo run only the Lighthouse microservice: COMPOSE_PROFILES=\"lh\" ./gradlew :app:dockerComposeUp\nEquivalent to cd app; COMPOSE_PROFILES=\"lh\" docker compose up -d\nEquivalent to COMPOSE_PROFILES=\"lh\" docker compose -f app/docker-compose.yml up -d\nTo run all the Platform microservices: COMPOSE_PROFILES=\"svc\" ./gradlew :app:dockerComposeUp\nEquivalent to cd app; COMPOSE_PROFILES=\"svc\" docker compose up -d\nEquivalent to COMPOSE_PROFILES=\"svc\" docker compose -f app/docker-compose.yml up -d\nOptionally, shut down the Platform Base setup (using instructions in the prior section)\nMocks\n\nContainers defined in the Mocks setup are not needed for typical development, so they are not started by default.\n\nFirst, build the mock container images: ./gradlew -p mocks docker\nTo run the Lighthouse API and Slack mocks: COMPOSE_PROFILES=\"lh,slack\" ./gradlew -p mocks :dockerComposeUp\nEquivalent to cd mocks; COMPOSE_PROFILES=\"lh,slack\" ./gradlew :dockerComposeUp\nEquivalent to cd mocks; COMPOSE_PROFILES=\"lh,slack\" docker compose up -d\nEquivalent to COMPOSE_PROFILES=\"lh,slack\" docker compose -f mocks/docker-compose.yml up -d\nTo run all the mocks: COMPOSE_PROFILES=\"all\" ./gradlew -p mocks :dockerComposeUp\n\nExamine the mocks/docker-compose.yml file to see what other profiles are available.\n\nWithout Docker Compose\n\nIf you want the mock to be available as localhost outside of the Docker Compose network, run it without Docker Compose -- for example:\n\ndocker run -d -p 20100:20100 --name mock-slack va/abd_vro-mock-slack\nRun specific containers\n\nTo run a container without the Platform Base, don't use Docker Compose. Instead run using Gradle task dockerStart, for example to run the Domain-CC App: ./gradlew domain-cc:cc-app:dockerStart. Prior to running the container, this task will automatically build the container image (./gradlew domain-cc:cc-app:docker). To shut down the container:\n\n./gradlew domain-cc:cc-app:dockerStop\n./gradlew domain-cc:cc-app:dockerRemoveContainer # Optional\n\nRun specific Platform Base container\n\nTo run only a subset of the Platform Base, specify the containers declared in the docker-compose.yml file and use the docker compose command directly -- don't use the Gradle tasks. For example:\n\nTo run only the Postgres DB and db-init containers: docker compose up -d postgres-service db-init.\nTo shut down, run docker compose down.\nStart All Containers\n\nAll containers are included in the all COMPOSE_PROFILE, so use that profile to start all containers in a given Docker setup.\n\nSet up: COMPOSE_PROFILES=\"all\" ./gradlew dockerComposeUp (Note the lack of :, which will run dockerComposeUp in all relevant Gradle projects, starting from the root project).\nShut down: COMPOSE_PROFILES=\"all\" ./gradlew dockerComposeDown\nIndividually Start All Containers\n\nTo start each setup individually, start the Platform Base first; the others can be started in any order:\n\nexport COMPOSE_PROFILES=\"all\"\n./gradlew :dockerComposeUp   # start the Platform Base\n./gradlew :app:dockerComposeUp\n./gradlew :domain-xample:dockerComposeUp\n./gradlew -p mocks :dockerComposeUp\n...\n\n\nTo stop each setup individually, shut down the Platform Base last:\n\nexport COMPOSE_PROFILES=\"all\" \n./gradlew :domain-xample:dockerComposeDown\n./gradlew :app:dockerComposeDown\n./gradlew -p mocks :dockerComposeDown\n...\n./gradlew :dockerComposeDown  # stop the Platform Base\n\nTip: set COMPOSE_PROFILES once\n\nWhen a subset of containers across several setups is consistently used, set COMPOSE_PROFILES once and export it. The variable will be used for all subsequent commands, and any unknown profiles for a particular Docker setup is ignored.\n\nFor example, if development or testing involves only the API Gateway and Lighthouse API, then running:\n\nexport COMPOSE_PROFILES=\"gateway,lh\"\n./gradlew :dockerComposeUp :app:dockerComposeUp \n./gradlew -p mocks :dockerComposeUp\n\n\nThis will start the Platform Base setup with the API Gateway (due to COMPOSE_PROFILES=\"gateway\" and :dockerComposeUp), the Java App with Lighthouse microservice (due to COMPOSE_PROFILES=\"lh\" and :app:dockerComposeUp), and the Lighthouse API mock (due to COMPOSE_PROFILES=\"lh\" and -p mocks :dockerComposeUp).\n\nPrune all\n\nBecause Docker resources (containers, images, and volumes) may be overridden and recreated during development, occasionally clean up:\n\nTo prune unreferenced Docker resources: ./gradlew :dcPruneAll\nEquivalent to:\ndocker container prune\ndocker image prune\ndocker volume prune\n"
  },
  {
    "title": "Development process ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Development-process",
    "html": "Check out LHDI's \"Building and Running\" Development Guide and LHDI's-Boilerplate-Instructions#running-the-application in addition to this page.\n\nM1 Macs\n\nFor M1 Macs, set export DOCKER_DEFAULT_PLATFORM=linux/amd64 to avoid problems downloading Docker images (e.g., ERROR: no match for platform in manifest or no matching manifest for linux/arm64/v8 in the manifest list entries). Details: https://pythonspeed.com/articles/docker-build-problems-mac/\n\nSecrets\n\nCheck out the abd-vro-dev-secrets repo and follow the instructions to set local development credentials and enable querying Lighthouse's development environment.\n\nPre-commit hook\n\nEnable VRO's pre-commit hook by doing the following:\n\nInstall pre-commit (Mac: brew install pre-commit)\nRun: pre-commit install. This creates a .git/hooks/pre-commit file.\nYou'll likely have to install go: brew install go\n\nWith this enabled, a series of checks and fixes (defined in .pre-commit-config.yaml) will be applied to changed files when you make a git commit.\n\nTo skip a specific check, see https://pre-commit.com/#temporarily-disabling-hooks\nTo disable pre-commit temporarily, pre-commit uninstall , and then pre-commit install to re-enable it\n\n(Most of the pre-commit alerts have been addressed in PR #666.)\n\nModifying existing code\n\nBefore modifying application code, it's advantageous to determine in which Docker container (and hence Code structure) the target code in being run.\n\nThe abd_vro-app container results from the following Gradle subprojects/folders:\napp\napi\ncontroller\npersistence\nservice\nwhile the abd_vro-db-init container results from the db-init folder;\nand the abd_vro-service-ruby container results from the service-ruby folder.\n\nAlso examine the docker-compose.yml files in the various subprojects, i.e., app, db-init, and service-ruby, to identify the container names and dependencies.\n\nWith that understanding, you can start to modify code.\n\nFirst start all the containers needed to run VRO: ./gradlew :dockerComposeUp :app:dockerComposeUp\nTo reset and rebuild container images: ./gradlew clean build docker\nNext determine what code needs to be modified and in which container it is run.\nThen follow one of the subsections below:\nA. Modify Microservice\n\nIf modifying code in the service folder, run using Sprint Boot. Alternatively, the slower option is to rebuild container image.\n\nIf modifying code in the abd_vro-service-ruby container, stop the container (docker stop docker-service-ruby-1), and run cd service-ruby/src; ./entrypoint.sh (which calls ruby) to quickly run the modified code. Alternatively, the slower option is to rebuild container image but is consistent with running other containers.\n\nB. Modify Camel Route\n\nThese currently reside in *Route(s) classes in the gov.va.vro.service.provider.camel package of the service/provider folder. Changes to this code will result in the abd_vro-app container, run using Sprint Boot or rebuild container image.\n\nC. Modify API, Controller, or Model\n\nCurrently, the API, controller, and model code result in the abd_vro-app container, so run using Sprint Boot or rebuild container image.\n\nRun using Spring Boot\n\nClose the abd_vro-app container (docker stop docker-abd_vro-1) and run ./gradlew :app:bootRun to quickly run the modified code using Spring Boot.\n\nRebuild container image\n\nThe alternative (and slower option) for running the modified code is to run it in an updated container: ./gradlew :app:dockerComposeUp, which requires rebuilding the container image and deploying the container locally. This command will automatically stop the running container and start an updated container with the same name.\n\nTip: Portainer UI\n\nFor a visual Docker Container management UI, try Portainer -- example UI. Run docker run -d --name portainer -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v /opt/portainer:/data portainer/portainer-ce and go to http://localhost:9000 in a browser.\n\nNew API and microservice\n\nWhen developing a new API endpoint with a new microservice, some folks like to start with the backend service; others prefer to start from the API; and still others like to start with a dead-simple end-to-end implementation and iterate on it until it satisfies the requirements.\n\nThe following sections describe the development steps used to add a new API endpoint and associated microservice. Each step can be done independently and in parallel, with some coordination to ensure consistent interfaces between Controller classes and Camel routes, and between route endpoints and microservices. For details, check out Routing API requests.\n\nBuild the microservice\n\nTo minimize software coupling and maximize single-responsibility, a microservice should be built independent of existing VRO code. It should be idempotent and testable without having to run any other VRO code. It can be placed in its own Docker container or into an existing container. See the service-ruby folder and PR #71 for an example of Ruby microservices that can be run and tested on its own, without Camel or a REST API -- standalone testing Ruby scripts are in the examples folder. The only requirement is interfacing with a message queue, specifically RabbitMQ.\n\nTo add a container in which the microservice will run, see Docker containers.\n\nTest Camel routing to microservice\n\nNew backend Camel routing and microservices can be tested without having to implement API endpoint, controller, data model, request/response, and mapper classes. CamelRestConfiguration uses Camel to create a REST API for quick development and testing in a local development environment -- it is not for production. CamelRestConfiguration is only enabled when vro.camel_rest_api.enable = true (set in conf-camel.yml). This is achieved by using Camel's REST endpoint to provide quick API for testing via curl or Postman. The sample CamelApp uses this Camel feature to provide the API, whereas VRO uses Spring's Web MVC described in the next section.\n\nAlternatively, Camel messages can be directly injected into Camel routes, which could be useful in unit tests. For examples, inspect the CamelEntrance class.\n\nBuild out the API and Controller\n\nImplement API endpoint, controller, data model, request/response, and mapper classes.\n\nGet agreement on the API endpoint specification, including examples and error codes. Check http://localhost:8080/swagger in a browser.\nImplement controller methods using Request and Response classes, and a Mapper class to convert to service.spi.*.model classes.\nHave the controller initiate a Camel route that eventually leads to a microservice, for example by calling a method in the CamelEntrance class."
  },
  {
    "title": "Development environments ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Development-environments",
    "html": "VRO Environments\n\nDevelopment (non-prod) and prod environments for the various systems, APIs, and data sources with which VRO interacts.\n\nFollowing Environments are Available for VRO\n\nDEV\nThe purpose of the development environment is to provide a controlled space for software developers to create, modify, and test new features, enhancements, or bug fixes. Developers have the freedom to experiment without affecting the stability of the production system. Developers use the DEV environment to write and test code, integrate new features, and collaborate on software development tasks.\nRuns mainly on Develop Branch and is frequently Updated.\nhttps://dev-api.va.gov/services/abd-vro/\nQA\nThe QA environment is dedicated to testing and quality assurance activities.Teams use this environment to perform various types of testing, including functional testing, regression testing, performance testing, security testing, and user acceptance testing.More stable then DEV environment and is manually updated.\nCan be used by team for manual testing\nInternal:\nhttps://qa.lighthouse.va.gov/abd-vro\nExternal:\nhttps://staging-api.va.gov/services/abd-vro\nSandbox\nA sandbox environment used for testing. It provides a safe space for trying out integrations without impacting the production or other critical systems. Previously it was used for testing v1\nInternal:\nhttps://sandbox.lighthouse.va.gov/abd-vro/\nExternal:\nhttps://sandbox-api.va.gov/services/abd-vro/\nProd-test\nUsed as staging env to test prod changes before pushing to prod.\nRead Access to Lighthouse Prod API\nProduction data, contains PII\nInternal:\nhttps://prod-test.lighthouse.va.gov/abd-vro/\nExternal\nhttps://api.va.gov/services/abd-vro-prod-test\nProd\nRelease process deploys code from main branch\nInternal\nhttps://prod-test.lighthouse.va.gov/abd-vro/\nExternal\nhttps://api.va.gov/services/abd-vro/\nSee Also:\nEnvironment Connections\nVRO should use Lighthouse API (as an intermediary) when possible -- this can simplify getting access and staying up-to-date.\nVRO Environment Crosswalk Spreadsheet\nPrior RRD Testing\n\nTesting the RRD prototype involved these environments:\n\nva.gov staging user\nLighthouse sandbox\nVBMS uat\nEVSS pint and VBMS uat interact with the linktest (fake) data.\n\nWe'll likely do something similar for testing VRO.\n\nEVSS preprod corresponds with VBMS prodtest\nVRO v1 Testing\nva.gov staging\nLighthouse sandbox\nVRO sandbox (sandbox.lighthouse.va.gov/abd-vro)\nVRO v1 connects to only Lighthouse (not VBMS or EVSS)\nVBMS uat\nEVSS pint and VBMS uat interact with the linktest (fake) data.\nVRO v2 Testing\nva.gov none - no prod-test env exists\nMAS prodtest\nLighthouse prod\nVRO prodtest\nVBMS prodtest\nLHDI\n\nOn the nonprod cluster:\n\nDEV - https://dev.lighthouse.va.gov\nexample: accessible on the VA network at https://dev.lighthouse.va.gov/abd-vro/v1/example/claimsubmissions or https://dev.lighthouse.va.gov/abd-vro/swagger\nQA - https://qa.lighthouse.va.gov\nhttps://qa.lighthouse.va.gov/abd-vro\nSANDBOX (used as pre-prod) - https://sandbox.lighthouse.va.gov\nhttps://sandbox.lighthouse.va.gov/abd-vro\nThe environment goes through the same release gates that deployments in the production environment.\n\nOn the prod cluster:\n\nPRODTEST - ?\nPROD - https://api.lighthouse.va.gov\nhttps://api.lighthouse.va.gov/abd-vro\n\nSee Kubernetes clusters for details\n\nva.gov\n\nva.gov (a.k.a. vets-api)\n\nFrom VA Platform doc and devops repo\n\nsandbox\nutility\ndev - https://dev.va.gov/\nstaging - https://staging.va.gov/\nTest User Dashboard \"helps teams to find acceptable high quality test users that are available for use on staging\" -- also mentions \"requesting a new test account\"\nprod - https://www.va.gov/\nLighthouse API\n\nFrom Lighthouse doc\n\nsandbox (https://sandbox-api.va.gov)\nhttps://sandbox-api.va.gov/oauth2/health/system/v1/token (Validate Token)\nhttps://deptva-eval.okta.com/oauth2/aus8nm1q0f7VQ0a482p7/v1/token (FHIR CCG Assertion URL)\nhttps://sandbox-api.va.gov/services/fhir/v0/r4 (FHIR URL)\nhttps://deptva-eval.okta.com/oauth2/ausj1sd1wiZE9S6BR2p7/v1/token (VRO CCG Assertion URL)\nprodtest & prod (https://api.va.gov)\nhttps://api.va.gov/oauth2/health/system/v1/token (Validate Token)\nhttps://va.okta.com/oauth2/aus8evxtl123l7Td3297/v1/token (FHIR CCG Assertion URL)\nhttps://api.va.gov/services/fhir/v0/r4 (FHIR URL)\nhttps://va.okta.com/oauth2/ausgbozn6qGzmjAFh297/v1/token (VRO CCG Assertion URL)\nLighthouse Sandbox Test Patient Data Management\nThere's 2 ways to submit requests to the Health API team:\nEmail api@va.gov (consumer support team's preferred method)\nFill out the contact form at developer.va.gov/support/contact-us\nA few reminders:\nIt's okay to send one email that includes the creation of 3 health test patients, for example, but if subsequent changes are needed to the health data for those 3 test patients, a new email should be sent\nKeep api@va.gov in the cc line of the email when responding to requests and questions\nSpecify that the request is for health test patients\nIndicate which team the request is coming from\nRequest does not need to be in a specific format, but be as specific as possible\nEstimated turn around time: ~1 week, depending on the intricacies of the change\nVBMS\n\nVBMS Overview\n\nLogins\n\nIn order to login you need:\n\nStation IDs: the 3-digit number that corresponds to your Regional Office. We usually use 283 for Developers. ARC is 397, and Florida is 317.\nUser ID: is the Active Directory account name under which you will authorize access.\nPIV card\n\nFor VBMS Prod access\n\nFill out this form, sign it, and send it to Zach securely\nOnce approved, use your PIV to log in\n\nFor VBMS ProdTest access\n\nyou must have access to VBMS Prod\nemail E_VBMS_4mopsleads@bah.com to be added to the ProdTest URL mailing list, which changes daily.\n\nLocal Testing of VBMS and BGS\n\ndev\nbep.dev.vbms.aide.oit.va.gov\nbepdev.vba.va.gov\nuat (a.k.a. devtest) - Core: https://www.uat.vbms.aide.oit.va.gov/vbmsp2\nto set up the test Veterans in VBMS UAT, see Notion page - involves logging in to VBMS Core, VBMS Rating, VBMS Awards.\npreprod (a.k.a. prodtest) - Core: https://www.pre.vbms.vba.va.gov/vbmsp2\nduplicate of production data updated twice a day\nprod\nEVSS\nINT\nPINT (aka SQA or staging)\nPreProd\nProd\nCMP/MAS\n\nCMP (a.k.a. MAS - Mail Automation Systems)\n\ndev - https://iam-dev.ibm-intelligent-automation.com\ntest - https://iam-test.ibm-intelligent-automation.com\nprod\n\nMAS tests its OCR function in VBMS prodtest\n\nOld related ticket\n\nMPI\n\nMPI (Master Person Index, a component of Master Veteran Index (MVI) or formerly known as MVI or Master Patient Index)\n\nMPI can be used to get the Integration Control Number (ICN) for a given veteran.\n\nLighthouse Veteran Verification API v2 could provide this lookup capability. API-17363 will look into the feasibility of this (Slack).\n\nResources\n\nMPI Testing refers to existing Ruby code\nVA MPI Playbook list service functions\nMPI Service Description\nContacts"
  },
  {
    "title": "Deployment Overview ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deployment-Overview",
    "html": "VRO is deployed to the LHDI platform. To deploy to LHDI, Docker images for deployment must exist in the GHCR (GitHub Container Registry).\n\nAs mentioned in Uses of container images, a GH Action pushes packages to GHCR so that the packages can be retrieved by EKS in the LHDI platform, initiated by calls to helm and kubectl.\n\nFor VRO releases, see Release Process and Deploying VRO.\n\nGHCR images\n\nGHCR images are associated with the abd-vro-internal repo\n\nabd-vro-internal packages\ndev_* images: added as part of the SecRel workflow when the dev_ prefix is selected\nmirror-* images: added when the Publish 3rd-party image workflow is manually run in the abd-vro-internal repo; these images are unmodified mirrors of the original\nvro-* images: added as part of the SecRel workflow when no prefix is selected or when a release-* is created in the abd-vro-internal repo (see Quick Deploy Instructions)\n\nThe SecRel workflow has an option to sign the images, which is needed for certain deployment environments.\n\nDeployment environments (LHDI clusters) pull images from GHCR\n\nDEV and QA do NOT require signed images\nSANDBOX, PROD-TEST, and PROD require signed images. For these namespaces:\n\nDI does not enforce any specific usage of these namespaces other than enforcing resource quotas and image signing for prod and sandbox environments.\n\nIn the internal repo, when a GitHub Release is created. the SecRel workflow is triggered, which will sign the image in GHCR if it passes SecRel checks. Then these signed images can be deployed to the LHDI environments that require signed images. See more details at Deploy to Prod.\n\nWhy not use the signed images for all environments so that there are fewer images to manage? Because if SecRel fails (for various reasons), the images are not signed. To allow testing to continue despite SecRel failures, the unsigned images are useful to have for deployment to certain environments for testing.\n\nSpecial branches\nThe main branch is intended to reflect the code deployed to the PROD environment of LHDI.\nOnly branch main goes to PROD, but only after main is tested in PROD-TEST.\nAny branch can be deployed to PROD-TEST.\nAll code changes should be pushed initially to the develop branch. When ready to deploy to PROD, the main will be updated (git reset) to the desired commit on the develop branch.\nThe only exception are hotfixes (including fixes for security vulnerabilities), which can be merged directly into main for PROD deployment. The hotfixes should also be applied to the develop branch so that when main is updated, the hotfixes are retained. See Partner Team Deploy Process for details.\ndomain-* branches facilitates deployment of domain-specific code, independent of the develop branch. See Partner Team Deploy Process for details.\nProcess diagram\n\nLegend:\n\nrounded boxes: activity performed by a person\nrectangular boxes: automation or object\nsolid line: causal connection or trigger\ndotted line: ordering association"
  },
  {
    "title": "Deploying VRO ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deploying-VRO",
    "html": "NOTE: This should only be done by VRO Engineers\n\nSee More of a overview of the deployment process here\n\nDeploy to Prod using main\nCreate a release tag on develop\nRun these commands on the public repo to sync main with develop\ngit checkout \"develop\"\ngit pull\n# Checkout branch           \ngit checkout \"main\"     \ngit pull\n# Hard Reset main to point to the desired develop\ngit reset --hard <CANDIDATE_COMMIT_HASH>\ngit push --force-with-lease\n\n\nThis should trigger the Mirror GitHub Action to copy the branches from the public repo to the internal repo's branches.\n\nVerify and Fix SecRel\nDeploy images with the release tag to prod-test for testing.\n\nPassing SecRel is required to deploy to prod-test\n\nOnce final testing is successful: NOTE: How do they have pre-release tags?\nConvert the GitHub release in the public repo from a Pre-release to a Release to prevent it from being automatically deleted. Nothing needs to be done in the internal repo.\nOptionally, create a GitHub release with a new git tag using the format release-..* so that it will never be deleted. (Do not use the create-release-tag GitHub Action workflow.) Deploy images with the release tag to prod LHDI env\nDeploy to non-prod using develop\nMerge a PR with changes into develop branch\nCreate a release tag on develop\nDeploy images with the release tag to a non-prod LHDI environment (dev, qa, sandbox, or prod-test)\n\nNote that automated deploys of the develop branch to the dev LHDI environment occur daily -- CI-CD Workflows.\n\nProd hotfixes\n\nIf it's acceptable to deploy the latest develop commit, then:\n\napply the hotfix to develop\nand do Prod deploy of Main\n\nOtherwise (there are develop commits that shouldn't be deployed):\n\napply the hotfix to main (test changes)\nCreate a release tag on main (using a tag with a -hotfix suffix, like v3.0.2-hotfix1 or release-3.0.2-hotfix1 representing the first hotfix for the previously released 3.0.2)\nand do Prod deploy of Main and also:\nIf relevant (for instance, if the hotfix should be included in future releases), then:\napply the hotfix to develop so that the hotfix is included for the next Prod deploy of Main (where main is reset to a develop commit).\nCreate a release tag on develop (using a tag like v3.0.<next consecutive integer>)\nCreate a Release Tag (Learn more about release tags)\nUse the create-release-tag GitHub Action workflow to create a new release Tag\nRun a new workflow and follow the Semantic Versioning guidelines to choose the next version number for the release tag to provide into the workflow.\nDeploy images with using the release tag\n\nFor the VRO Platform Team, click the \"Run workflow\" here for one of the \"Update Deployment\" GitHub Action workflows:\n\nPlease use the following \"Update Deployment\" workflows based on the env you are deploying:\n\nUpdate Deployment is used to deploy various Helm charts, including api-gateway, platform microservices, and domain-*.\nUpdate Deployment - App is only needed to update the vro-app Helm chart, which exposes Java-based REST API and updates the DB schema (via the db-init container).\nUpdate Deployment - Platform - Used to deploy changes to Postgres, RabbitMQ, Redis, CLI (rarely needs to be used)\n\nFor the \"Update Deployment\" GitHub Action workflow inputs, run bash scripts/image_vars.src imageVersions to show the latest image tag values for each container image. Details at Container Image Versions.\n\nVerify and Fix SecRel\nNavigate to the this Github workflow page and confirm/wait for the workflow to complete for your release version. Navigate to the analogous page in the internal repo and again confirm/wait for the workflow to complete for your release version.\nCheck for a post to the #benefits-vro-devops Slack channel that SecRel is running. Monitor the SecRel workflow by navigating to the internal GitHub repo and checking the GitHub actions for an action with the name of the release or by clicking the link in the Slack message.\nAny SecRel alerts that have either an expired acknowledgement (from prior releases) or is new to the changes in the release will require remediation. If SecRel alerts can be addressed without compromising the release date, they should be addressed and this process will have to be repeated. Otherwise, the two engineers will need to collaborate to weigh the severity of alerts and the harms of delaying the release date to determine next steps. If acknowledging alerts is chosen, engineers should be able to clearly articulate their argument for acknowledgement to the SecRel assessor if needed and file an issue summarizing this argument.\nOnce all SecRel alerts have been acknowledged or addressed, restart the SecRel GitHub workflow.\nRepeat steps 6-8 until SecRel GitHub workflow completes successfully."
  },
  {
    "title": "Deploy to Prod ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deploy-to-Prod",
    "html": "From this Slack post:\n\nAs of right now, deployments to production environments on LHDI are fulfilled by going through the Secrel pipeline. team-tornado is the team that manages that flow and can work with you on onboarding there. They have a support channel in Lighthouse slack at vaapi-secure-release-pipeline-support-channel.\n\nLHDI validates that you have gone through the SecRel pipeline through signed image enforcement, which you can read more about here: https://animated-carnival-57b3e7f5.pages.github.io/container-platform/signed-images/\n\nThe SecRel workflow GitHub Action implements the SecRel pipeline.\n\nReview Deploying VRO for the bigger picture. For exact instructions, see Deploying-VRO#prod-deploy-of-main.\n\nFor visual walkthrough, see Quick Deploy Instructions."
  },
  {
    "title": "Dependabot ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Dependabot",
    "html": "Dependabot has been enabled, and VRO dependencies (for Java, Python, Docker, and GH Actions) are currently up-to-date. On a daily basis or when we manually run it, it will automatically create PRs to upgrade library versions (like these). Having this will help keep VRO code patched against vulnerabilities and will reduce the work in resolving SecRel alerts, required for deploying to production.\n\nKnown problems\nChecks (GitHub Actions) fail on Dependabot-generated PR\n\nUPDATE: Based on https://docs.github.com/en/code-security/dependabot/working-with-dependabot/managing-encrypted-secrets-for-dependabot and https://docs.github.com/en/code-security/dependabot/working-with-dependabot/automating-dependabot-with-github-actions#accessing-secrets, PR #454 fixes this issue by:\n\nAdding a ACCESS_TOKEN secret to Dependabot's secrets\nUpdating Dependabot config to use the secret to retrieve jars from the VA's repo (starterBootPkgs https://maven.pkg.github.com/department-of-veterans-affairs/lighthouse-di-starter-boot) in order to build\n\nOriginal problem\n\nCertain checks fail because Dependabot doesn't have access to the GitHub Actions secrets -- see https://github.com/dependabot/dependabot-core/issues/3253#issuecomment-795101596 and in https://docs.github.com/en/code-security/dependabot/working-with-dependabot/automating-dependabot-with-github-actions#responding-to-events (\"GitHub Actions secrets are not available.\").\n\nA workaround is to merging the develop branch into the PR or manually committing to the PR branch. See example resolution in PR #386.\n\nError during Dependabot update\n\nError is shown in logs on the Dependabot tab. Example PR that resolves the error. Once the PR was merged, Dependabot created the PR, which is no longer relevant. Manually re-running Dependabot shows no errors.\n\nDependabot Processing Guide\nChecking for new Dependabot PRs\n\nDependabot-generated PRs can be found under the Pull Requests tab by searching on open PRs with the dependencies label\n\nAlso check for and address Code scanning and Secret scanning alerts.\n\nDeciding to accept the update\n\nIf the proposed update seems relatively minor or trivial, then\n\nNavigate to the mirrored dependabot PR in the internal repo, and check the PR actions for a SecRel run. Address any SecRel issues and update the PR in the public repo if necessary.\nIn the dependabot PR in the public repo, be sure that all testing actions along with container health checks both run and pass.\nOptionally, perform additional manual API testing as needed depending on the complexity of the change.\n\nSometimes, the generated PR does not have all the changes necessary to truly update the references successfully. Once functional testing passes the branch, you should push up your changes and run it through SecRel again to make sure you haven't introduced unforeseen security issues.\n\nWhen possible, update the constraints section in shared.java.vro-dep-constraints.gradle so that other projects dependencies can be updated.\n\nIf all looks well with the generated (or new) PR, then\n\nComment on the PR that all testing passes, and tag the Eng Lead (or some other relevant developer) for an additional approval.\nDependabot authors the PR, so only one additional reviewer other than you is required.\nOnce approved, merge the PR into the develop branch\nDeciding to skip the update\n\nIf the proposed update seems like a relatively major change that could take non-trivial testing and/or refactoring, you may decide to delay it for a future date (as with these PRs). If that's the case, then\n\nClose the PR with a detailed comment explaining why, tagging the Eng Lead (and/or some other relevant developers)\nOpen an issue so that it can be tracked and updated at a future date (see below)\nOpening an issue\nGoto the Issues tab, and click New Issue\nClick ‚ÄúOpen a blank issue‚Äù\nFill in ticket, perhaps using this as a model (Description + A.C. sections). Be sure to link to specific PRs when applicable, so that they will be linked to the issue in their comments\nAdd the Engineer and vro-issue labels under Labels section\nAdd ABD VRO Project under the Projects section"
  },
  {
    "title": "DataDog monitoring ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/DataDog-monitoring",
    "html": "DataDog\n\nLog in using LHDI's Okta sign-in page is https://ablevets-dots-va.okta.com/\n\nVRO's dashboard\nLHDI dashboards\n\nOur deprecated DataDog account:\n\nlasershark's dashboard - one of the Alpha customers"
  },
  {
    "title": "Data Visibility MVP Tech spec ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Data-Visibility-MVP-Tech-spec",
    "html": "Data Visibility MVP Tech spec\nBackground\n\nCurrently VA.gov activity data, including disability benefits claim submission data, is functionally inaccessible to Benefits Portfolio product teams, with the exception of a handful of engineers with command line access to query the production postgres database in vets-api. OCTO wants to develop a safer, more accessible, and more user-friendly way for teams to access this data.\n\nThus, VRO as a platform as an MVP will be resposible for safely and securly providing VRO partner teams within the Benefits portfolio the cliams data submitted via 526EZ forms through va.gov.\n\nIn-order to make this happen, the VRO team is responsible for coordinating this effort via collaboration across the Benefits Portfolio, in particular with the Disability Benefits Experience team(s) who are familiar with the va.gov Postgres database and the needs of engineers working on va.gov benefits products.\n\nPain points\nDisability benefits claim submission data is only avaiable via rails console in prod.\nCannot use any BI/dashboarding tools to view metrics.\nMVP goals and assumptions:\nFocused on the 526EZ form benefits claims submission data\nData dump from production vets.gov postgres db happens daily into a s3 bucket through a another process.\nData at rest is decrypted before being dumped into the bucket\nS3 bucket is already setup, encyrpted and secured via SSE-KMS or other AWS provided options\nBenefits claims data is available via sql initially from the VRO postgres db\nSolution\nUtilize Kubernetes cron job to run a python script daily\nUse Pandas or another dataframe python library to read the csv file, sanitize the data, filter any unwanted data, standardize datetime if nessessary.\nKeep track of processed csv file s3 bucket file names in a database.\nStore the processed claims data into the database using transactions.\nRe-try mechanism for errors when they happen during cron-job.\nSlack notification when a dump has been processed or failed.\nMonitor\nCreate Datadog dashboard to monitor the cron jobs\nLocal development\ns3 bucket and csv file\nGenerate fake data csv file without any PII to simulate daily dumps\nTo emulate s3 bucket functionality locally, use local stack\nRather than using docker-compose.yaml files for the container, use the kubernetes deployment files used for LHDI env locally and leverage minikube to run the container locally and it can be ingrained into the existing Gradle tasks. Added step will be that VRO will installing minikube.\nQuestions and comments\nHow do we handle storage of PII data because of possible ATO restrictions?\nWhat exactly does current data look like? This can help us design exception handling and job-retry mechanisms.\nHave a backup mechanism in place for the data in case of any failures or data loss"
  },
  {
    "title": "Culture and Norms ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Culture-and-Norms",
    "html": "Our teams core values\nVeteran's first - always consider if what you are working on provides value to Veterans.\nHelp others - how can you elevate others?\nBe a thoughtful and trusted partner to the VA and their contractors - treat others as you would like to be treated.\nLook to make small, incremental improvement - fail (learn) fast if needed.\nEnjoy your work - be proactive, flexible, resourceful and curious.\nMake the team be a safe place - demonstrate kindness, empathy, respect, trust, care and assume best intentions.\nGroup norms\nBe proactive - don't wait for someone to tell you what to do.\nPair on tasks often - offer and ask to pair.\nFollow Slack protocols - use threads, tag people as needed and change your status if you are away for more than a couple of hours.\nInclude summaries of links when adding links in tickets.\nKeep documentation in Wiki - the Wiki should be the source of truth.\nCreate user stories for tasks that are more than a half day of work.\nCreate a ticket for each sprint where small ad-hoc tasks can be added for tracking and visibility.\nStart with Slack threads and move to huddles if there is a lot of back and forth.\nHow we measure success\nWe produce quality deliverables and are proud of our work.\nWe have good balance.\nWe are always learning - we experiment and learn from each other.\nWe have delivered value to Veteran's - we have delivered capabilities to OCTO that other teams can benefit from.\nWe collaborate - we build good relationships within and outside of our team.\nWe work as a team towards a common goal.\nVRO is deployed and is providing value.\nHow we ensure quality\nThe deliverable provides value.\nThe Product Manager has signed off on the deliverable after all acceptance criteria have been completed and demonstrated.\nPRs have been reviewed by XXXX people and revisions based on feedback completed.\nBe proud of your work - self QA/test.\nDocumentation is updated and can easily be followed by others.\nProposed Norms\n\nStarting some proposals for consensus:\n\nDocument Coding Patterns that should be followed, especially if it follows best practices. The LHDI Starter Kit provides some initial code but we don't have to follow it if it causes undue friction.\nDocument DevOps How-To's for developers, such as Deploy to Dev and Deploy to Prod. Application developers don't need to know all the details; provide a \"Quickstart\" guide with references to relevant code, script, and/or configuration files. Document it once and reference it so you don't have to re-explain.\nDocument DevOps details that may be useful for others or demonstrate how to get the details, such as Kubernetes clusters.\nDocument available tools, such as DataDog monitoring, for diagnostics, monitoring, or reporting.\nDocument learnings or tips that takes a while to figure out, such as Docker containers (because it is involved in development and deployment) or your Development process, to help your teammates learn and include notes to remind your future self.\nRadiate intent because we want to get things done quickly and openly."
  },
  {
    "title": "Code structure ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Code-structure",
    "html": "This page does not seek to explain testing and how it is performed or the runtime/deployment infrastructure for the VRO project.\n\nThis page also does not seek to explain the details of any domain-specific code that is accessible within the VRO app. Each domain should include a link to a page here which provides details about its logic and components.\n\nInitial things to review\nApplication Entrypoint\nDomain-Specific Logic\nCamel Routes and Microservices\nShared libraries\nPutting It All Together: Project Folder Structure\nInitial things to review\nSoftware Conventions\ninitial files provided by LHDI's Java Starter Kit in PR #8\nsoftware changes in Current Software State\nGradle Build System\nConfiguration Settings and Environment Variables\nApplication Entrypoint\n\nVRO exposes a single API through a Java Spring Boot Application within the app subproject. The exact structure of the API endpoints along with the expected form of requests and responses can be subject to frequent changes. VRO uses OpenAPI to allow for dynamic creation of documentation for API endpoints. LHDI Docs for using OpenAPI for applications (requires membership in abd-vro Github team)\n\nTo see the most recent VRO API spec, run the software and browse to http://localhost:8080/swagger.\n\nThe Spring MVC framework is used to define the implementation of the Spring Boot Application, giving the following key components for defining the application logic:\n\napi defining VRO's API\ncontroller for the API\npersistence:model defining DB Entity classes\nservice:db for DB interactions\nservice:spi containing Java interface and model definitions\nservice:provider containing Apache Camel configurations\n\nThe following is a basic graphic depiction of the application\n\nDomain-Specific Logic\n\nAs VRO is a host of domain-specific logic, the Spring Boot Application defined within the app Gradle subproject takes a dependency on Gradle subprojects of the form domain-* which are responsible for defining their own domain-specific API routes and controllers which can then be exposed through the base VRO Spring Boot Application.\n\nFor instance, logic specific to some ABC functionality would have code following the structure:\n\ndomain-ABC: domain-specific code with subfolders:\nABC-api-controller: module that defines domain API endpoints and controller\nABC-workflows: defines domain workflows (i.e., Camel Routes)\nsvc-*: domain microservices supporting the workflows\nCamel Routes and Microservices\n\nThe Spring Boot web app processes a request by delegating to a workflow defined by Apache Camel routes. More information on Apache Camel and how it is used in this project can be found here.\n\nOnce a request is received by the Spring Boot application entrypoint, it can be routed by Camel through a number of different steps including to a microservice (discrete component which is responsible for completely handling a smaller piece of computation) via a message bus or to some data sink.\n\nCurrent list of microservices (each has an associated Gradle subproject):\n\n2 Python assessors\n1 Python pdf generator\n1 Java client for LH API svc-lighthouse-api\n1 Java client for MAS API svc-mas-api (in development)\n\nNote that clients to external APIs are also modeled as microservices for compatibility with Camel routes.\n\nSince the Python microservices have common code, it is extracted into a shared library:\n\nservice-python/main-consumer.py\n\nRabbitMQ is used as our message bus for transporting messages to microservices while Redis and Postgres are used as data sinks depending on the use case. Postgres is deployed as a containerized application through the help of a separate init container called db-init. Both definitions for Postgres and this init container are found in the top-level directory of the project.\n\nShared libraries\n\nVRO offers utilities and DB classes as shared libraries for use by domain-specific classes.\n\nshared/api folder: (Java) general API models\nshared/controller folder: (Java) Controller utilities, including InputSanitizerAdvice\nshared/lib-camel-connector folder: (Java) Camel utilities, including a Spring Configuration for Camel and CamelEntry\npersistence folder: DB utilities, including Java Entities\nPython MQ utilities to facilitate interaction with the MQ\ndomain (PR Move domain folder to new shared folder #551)\nPutting It All Together: Project Folder Structure\napp: VRO entrypoint; pulls in domain-specific api-controller modules\ndb-init: initializes and updates the database schema\npostgres: defines a customized database container\nshared: utility and library code shared across domains (Shared libraries)\nsvc-*: domain-independent microservices, typically for integrations (Shared microservices)\ndomain-ABC: domain-specific code with subfolders:\nABC-api-controller: module that defines domain API endpoints and controller\nABC-workflows: defines domain workflows (i.e., Camel Routes)\nsvc-*: domain microservices supporting the workflows"
  },
  {
    "title": "Container Image Versions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Container-Image-Versions",
    "html": "Container images are built and published to GHCR (GitHub Container Repository) -- see Deploying VRO. The images are tagged with an image tag, representing the version of the image. The image tag (or \"image version\" or \"version\") is:\n\nthe first 7 of commit hash when the image is built as part of Continuous Integration (CI CD Workflows),\nor a semantic version string formatted as v1.2.3 when a release tag is created, typically for testing in LHDI environments,\nor a semantic version string formatted as release-1.2.3 when a manual release tag is created, typically for deployment to prod LHDI.\n\nSee different image versions in the GitHub Container Repository. The latest image tag always refers to the last published image, which may or may not be signed by SecRel.\n\nPreviously: A single image tag\n\nPreviously, there was a single image tag for all container images and all containers were SecRel-scanned and deployed, regardless of whether a container had changed or not. With this constraint, as the number of images increases, more time is spent on SecRel scans, addressing SecRel alerts for unchanged images, and redeploying unchanged images. This coupling of container images to a single image cersion imposes an unnecessary constraint.\n\nAs described in Allow deployed containers to have different release versions #1725, we want a way to be able to deploy only the containers that have been updated, with minimal additional manual maintenance overhead. The remaining sections describe how this is achieved.\n\nDecoupling image versions\n\nThe image_vars.src and image_versions.src files determine which versions of each container is used during deployment. The image_vars.src file should not be modified manually. The versions can be overridden by manually-specified versions in GitHub Action workflows (e.g., Update deployment).\n\nTo see the latest release versions for each image, run bash scripts/image_vars.src imageVersions as described in Deploying-VRO#deploy-images-with-the-release-tag.\n\nTerminology and Benefits\npinning an image version = set the version for a particular container image so that:\nthe image is not republished to GHCR and SecRel does not rescan the (same) image\nthe pinned version is used during LHDI deployments\n(The version number has the format v*.*.* or release-*.*.*)\nunpinning an image version = unset the version for a particular container image so that:\na new image is built and SecRel scans the new image\nthe new image is deployed to LHDI\n(The version number is the first 7 of commit hash)\nPinning versions\n\nWhen a release is created (create-release-tag.yml), image versions that are unpinned will become pinned to that specified release version.\n\nUnpinning versions\n\nWhen the codebase is changed (e.g., a PR is merged or develop is changed) and new images will be published to GHCR, if the container image has changed for image versions that are pinned, then it will be unpinned.\n\nimage-version.sh automation script\n\nPinning and unpinning is performed by image-version.sh, which updates image_versions.src and is automatically called when certain GitHub events trigger GitHub Action workflows. The result of the script is that images that haven't changed will stay at their pinned versions, while change images will be unpinned. By looking at image_versions.src, it is apparent which images have not changed.\n\nManually pin image versions\n\nTo manually pin images so that they don't get automatically updated, use the export myimage_VER=\"v1.2.3\" syntax in the image_versions.src file so that they will be ignored by the image-version.sh script.\n\nDetecting changes to images\n\nImage versions are unpinned when the image has changed. There are many reasons for an image to change:\n\nwe updated our code used in the image (to add features or fix bugs)\napplication dependencies or OS libraries have been upgraded (to address SecRel alerts)\nthe base container image was updated (to get updated features or address SecRel alerts)\n\nThe image-version.sh script detects changes to images using the container-diff utility, which identifies every tiny difference. Comparison is done using the \"size\" and \"history\" analyzers. However, there are docker-build-time differences (e.g., random file identifiers, installing OS packages and libraries, and file creation timestamps) that cause two images that are practically the same to be considered different. The script's isImageSame() function tries to account for some trivial differences but it's not perfect -- e.g., any image size difference cannot be discounted and so the images must be considered different. Also in rare cases where images with identical sizes aren't necessarily the same, it also checks the \"history\" (of Dockerfile commands) to mitigate this risk. (Further improvement for better image-change detection is welcome.)\n\nWorkaround: In those rare cases where a change is not detected and to ensure that a new image is published, simply unpin the image version by removing the corresponding line in the image_versions.src file.\n\nKnown issues\n\nThe container-diff utility reports differences for the following images, even when there may be no apparent changes:\n\nvro-app - has it's own Dockerfile; uses Java Spring\nvro-cc-app - has it's own entrypoint.sh; Python image; container-diff reports differences in pip-related files\nvro-svc-bgs-api - has it's own Dockerfile; Ruby image\n\nPossible causes:\n\nOS packages are not set to specific version (search for hadolint ignore=DL3018 in Dockerfiles). As a result, new files are indeed being saved to the image.\nApplication libraries (e.g., pip packages) are being downloaded and cached in the image. The cache may have an updated catalog of available packages or the cache may be generating random filenames.\n\nFurther work is needed to improve the change detection mechanism.\n\nForce all images to be SecRel-scanned\n\nTo have SecRel scan all images, manually run SecRel with publish mode all, which will publish and then scan all images. This is useful to identify newly discovered vulnerabilities in unchanged/pinned images.\n\nTo help ensure new vulnerabilities are addressed and to express a desire for a new version of the image, simply unpin the image version by removing the corresponding line in the image_versions.src file."
  },
  {
    "title": "Configuration settings ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Configuration-settings",
    "html": "Configuration settings are needed for different use cases (unit tests) and environments (dev, qa, prod, ...).\n\nUse Cases\n\nVRO is run the following ways, where environment variables are set for the containers:\n\nlocally via Docker (and docker-compose), usually as a result of running ./gradlew app:dockerComposeUp\ndeployed to LHDI's EKS\nGuidelines for placing settings\nIf the setting is a secret, it must be put in the Secrets Vault. The secrets will be made available as environment variables.\nPrefer container-scoped settings or configuration files rather than exposing those settings outside the container in Helm configurations.\nPrefer to add them to application*.yml (for Java) or settings*.py (for Python). Those files allow different setting values per deployment environment. For example, the application-dev.yml is used for deployments to LHDI dev.\nIf the setting needs to be overridden, allow it to be overridden by an environment variable. This should rarely be needed.\nMinimize changes to Helm configurations (under the helm folder). This facilitates diagnosing issues in LHDI that originate from LHDI changes outside our control by allowing us to say: \"VRO hasn't change any Helm configurations but things stopped working today\". Settings that must be in Helm configurations:\nVRO_SECRETS_* environment variables -- see Secrets Vault\nEnvironment variables that refer to a Helm value or variable, e.g., pv.tracking.mountPath\nSettings that are shared across containers, i.e., when the setting value must be the same for those containers. For example, connection URLs to VRO's DB, MQ, and Redis.\n\nAlso check out Secrets-Vault#configuration-setting-vs-environment-variable and comments in scripts/setenv.sh.\n\nEnvironment Variables\n\nBefore using an environment variable, use a configuration setting that is scoped to only the relevant container -- read Secrets-Vault#configuration-setting-vs-environment-variable and see section \"Spring's application.yml\" below.\n\nUse environment variables in code so that they can be overridden for deployment. For example:\n\nbuild.gradle: in dockerRun settings block, set the default values so that it works in your local development environment -- example\ndocker-compose.yml: in environment settings block -- example\nDockerfile: using the ENV command -- example\n\nIn addition to the above, environment variables are initialized by:\n\nsetenv.sh (in the abd-vro-dev-secrets repo) - sets environment variables for local builds\nhelmchart/values.yaml (for LHDI's EKS deployments) -- Helm chart configurations\n\nEnvironment variables are used:\n\nin app/src/main/resources/application.yml (Java Spring Framework) - default values are set for environment variables that don't exist\napp/src/test/resources/application.yml (for unit tests)\napp/src/main/resources/application-compose.yml (used when running docker-compose)\napp/src/main/resources/application-docker.yml (used when running docker)\nin settings.py (for Python code)\nby VRO containers (when run by Docker or EKS) - the variables are referenced by application.yml (for Java containers) and settings.py (for Python containers)\n\nJava code in the VRO application should prefer to use configurations set in application.yml via Spring's @Value annotation -- environment variables should not be referenced directly. This encourages consistency in how VRO is configured.\n\nFor Python code, environment variables should be referenced in the settings.py file only (and not in the other files). The settings.py file functions like Java/Spring's application.yml.\n\nSpring's application.yml\n\nThe ENV environment variable is used to set spring.profiles.active.\n\nSimilarly, environment variable SPRING_PROFILES_ACTIVE (Spring Profile doc) determines which other application-<PROFILE>.yml is loaded, in addition to application.yml -- see this StackOverflow answer. Several application-<PROFILE>.yml files can be loaded by delimiting the profile names with a comma, where properties in the later profiles will override those in earlier profiles if there is overlapping settings. -- see Profiles.\n\nSpring property spring.profiles.include is used to load additional application-<PROFILE>.yml files regardless of the active profile. However, consider these guidelines when using this property.\n\nAssociated with profiles, Spring's @Profile(<PROFILE>) can be used to enable certain Spring Beans, Components, Services, etc. -- however, use it sparingly and responsibly.\n\n@ActiveProfiles can be used for a @SpringBootTest that requires special profile(s), e.g., with settings that are different from application-test.yml.\n\nVRO's use of Spring Profiles\n\nThe value of ENV can be any of LHDI's EKS environments (dev, qa, sandbox, prodtest, prod) or local (your local docker-compose environment). spring.profiles.active is set to the value of ENV, which will cause Spring to load application-$ENV.yml, in addition to the default application.yml.\n\nFollowing this suggestion, VRO has:\n\napplication.yml - shared properties across all environments; always loaded by Spring\napplication-prodtest.yml - non-shared properties for the prod-test environment\napplication-prod.yml - non-shared properties for the prod environment. Secrets should reference environment variables, which will be set in Kubernetes.\napplication-nonprod.yml - shared properties for non-prod environments (local, dev, qa, sandbox). Spring Profile Groups are set up (in application.yml under spring.profiles.group) so that application-nonprod.yml will be loaded, followed by one of the following (if they exist):\napplication-local.yml - for your local docker-compose environment\n(Optionally) application-dev.yml, application-qa.yml, application-sandbox.yml - for the corresponding LHDI EKS environment. These don't need to exist, esp. if their configuration is no different than application-nonprod.yml\n\nWhen running VRO, the loaded Spring profile names are shown in one the first few lines of the app's log output, e.g., gov.va.vro.VroApplication: The following 2 profiles are active: \"nonprod\", \"dev\".\n\nTo override the active profile (spring.profiles.active), set SPRING_PROFILES_ACTIVE. This may be useful to set properties for mock services for example.\n\nFeature flags\n\nFeature flags help developers gate which code should be enabled or not. The microservice for feature flags exists inside of service-python/featuretoggle and [java location TBD].\n\nFeature flag status is stored in Redis and checked by the main app, which attaches the data to routes as needed. The app queries the microservice which updates the timestamp of the last check in order to ensure flag data is up-to-date.\n\nUpdate the features.yml file in the featuretoggle service with the feature flag name and status."
  },
  {
    "title": "CI CD Workflows ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/CI-CD-Workflows",
    "html": "Quoting from this CI/CD page:\n\nContinuous integration (CI) is practice that involves developers making small changes and checks to their code. ... this process is automated to ensure that teams can build, test, and package their applications in a reliable and repeatable way.\nContinuous delivery (CD) is the automated delivery of completed code to environments like testing and development. CD provides an automated and consistent way for code to be delivered to these environments.\nContinuous deployment is the next step of continuous delivery. Every change that passes the automated tests is automatically placed in production, resulting in many production deployments.\n1. Continuous Integration (CI)\n\nWhen the develop branch is updated (e.g., due to a PR being merged), it triggers two major GH Action workflows:\n\nContinuous Integration: runs various tests, including linters, unit tests, end-to-end tests, and CodeQL vulnerability check.\nSecRel: builds and publishes Docker images and has SecRel sign them if the images pass SecRel checks. These images are published to GHCR (GitHub Container Registry) so they can be used in deployments to LHDI's Kubernetes environment.\n\nAt this stage, the published Dev images have the prefix dev_ and can be deployed to the dev and qa LHDI environments. Other environments have different requirements, so non-dev images (i.e., those without the dev_ prefix) are used to clearly distinguish them to prevent accidental deletion.\n\n2. Continuous Delivery (CD)\n\nEvery afternoon, an automated GH Action workflow called Continuous Delivery updates the deployment in the dev LHDI environment with the latest Dev images published (by the SecRel workflow). There is a small chance that the latest published Dev images may not reflect the latest develop commit since it can take 15 minutes for the images to be built and published.\n\nFAQ 1\n\nQ1: Why not deploy with every update to the develop branch? A: If someone is testing in the dev environment, an unexpected deployment would interfere and possibly produce unexpected behavior. Predictable deployments mitigate the likelihood of this scenario.\n\nQ2: Does that mean we have to wait until VRO is automatically deployed to do testing? A: Nope -- see the \"Manual deployments\" section below.\n\nQ3: Can I deploy and test a specific version of the develop branch? A: Yup! See the \"Manual deployments\" section below.\n\nQ4: Won't GHCR be filled with Dev images? A: We have a GitHub Action workflow for that! Delete old published DEV images runs twice a month to delete Dev images older than 30 days but keeping at least 10 of the latest versions (regardless of age) for each image. This can also be run manually with different parameters.\n\nManual deployment to Dev\n\nTo manually deploy to the dev environment, use the Update Deployment workflow to update containers that have changed (the DB, MQ, and Redis containers do not typically need redeployment). Click the \"Run workflow\" button, then:\n\n(Leave \"Use workflow from\" as develop)\nEnsure the dev is the target LHDI environment\nChoose a specific version to deploy or leave it as latest\nChoose which sets of containers to update in the deployment -- the default selections are usually fine. Helm (our deployment tool) will redeploy only images that have changed (usually only the VRO App and domain services) -- this makes deployments efficient. Unselecting an option will undeploy the associated containers -- this is only needed if there is some deployment problem. The following Helm Charts can be enabled:\nEnable & update VRO Application API\nEnable & update RRD domain services\nEnable MQ\nEnable DB\nEnable Redis\nEnable VRO Console\nFor \"If deploy fails, rollback to previous? (Always true for prod)\", leave it unchecked so that if it fails, the failure state will be available for diagnostic investigation. Let VRO DevOps know if a deploy unexpected fails. Since this is a non-production environment, it's okay that VRO is not operational so a rollback is not needed.\n\"Shut down deployment completely, then redeploy\" can be useful to completely reset and restart everything; however this is not typical, so keep it unselected (false). Use this option only if deployments continue to fail and diagnostics is unfruitful.\n\nReminders when using this workflow:\n\nMake sure no one is actively testing VRO in the target environment (i.e., dev).\nAutomated deployments will still run every afternoon (as part of Continuous Delivery). So if a manual deploy is done immediately before the automated deploy, the deployment may be overridden with the latest version.\n3. Continuous Deployment\n\nWe currently do not automatically deploy changes to the production environment (prod). Rigorous manual QA testing is first conducted in the qa and prod-test environments, with fake and realistic data respectively.\n\nBefore describing deployments to the sandbox, prod-test, and prod environments, let's talk about GHCR packages and signed images.\n\nGHCR Packages\n\nThe published images are called \"packages\" in GHCR, as seen here:\n\nmirror-* images are copied from external sources. These images are unmodified mirrors of the original and provide services used by VRO. They are added when the Publish 3rd-party image workflow is manually run.\nvro-* images are created from the VRO codebase.\nThese images are typically used for deployments to the sandbox, prod-test, and prod environments.\nNew tagged versions are added as part of the SecRel workflow when run on the main branch or when a release-* is created in the abd-vro-internal repo.\nNew versions of images are tagged using the first 7 characters of the commit hash or the release version (if a release was created).\nThese non-dev images should rarely be deleted.\ndev_vro-* images are the Dev counterparts to the vro-* images.\nThese images are typically used for deployments to the dev and qa LHDI environments\nNew tagged versions are added as part of the SecRel workflow when run on the develop and qa branches.\nThis allows you to address vulnerabilities quickly; from changes to the image or newly discovered vulnerabilities\nThis does not delay you from testing images while secrel is failing, as unsigned images are pushed to dev and qa\nNew versions of images are tagged using the first 7 characters of the commit hash.\nA dev_ prefix is used to distinguish from images used in prod, so that these Dev images can be readily deleted.\n\nThe tags for an image in GHCR can be seen by clicking on the image, then \"View and manage all versions\". To determine if an image is signed, compare the image \"Digest\" with the sha256-*.sig tag -- see these instructions.\n\nSigned images\n\nLHDI environments (clusters) pull images from GHCR. LHDI enforces the following policies for deployment to each environment:\n\ndev and qa do NOT require signed images\nsandbox, prod-test, and prod require signed images\n\nImages are signed by the SecRel (Secure Release) pipeline, which runs in the the abd-vro-internal repo. VRO's SecRel GitHub Action workflow publishes the images to GHCR and runs the SecRel pipeline on the images, which signs them if the images pass SecRel checks). If SecRel checks fail (for various reasons), the unsigned images are still available for deployment to dev and qa for testing.\n\nDeploying to sandbox, prod-test, and prod\n\nDeployments to the sandbox, prod-test, and prod environments are manual and use signed non-Dev images, which are created in 2 ways:\n\nCreate a GitHub release in the abd-vro-internal repo (Note that a release needs to be created in the abd-vro repo first to prevent the abd-vro-internal's release from being removed due to repo mirroring). The images will be tagged with the release name (e.g., release-1.2.3).\nUpdate the main branch. The images will be tagged with first several characters of the git commit hash.\n\nThese actions will trigger SecRel to publish and hopefully sign the images. If SecRel fails and cannot be remedied, identify the associated \"SecRel scan failed\" Slack message and notify @Tom to investigate.\n\nIf the SecRel workflow succeeds, run the Update Deployment workflow with the desired image tag to deploy to the desired environment -- see the \"Manual deployment to Dev\" above."
  },
  {
    "title": "Change Management Plan ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Change-Management-Plan",
    "html": "Branching Strategy\n\nThis strategy is taken and modified from Microsoft.\n\nUse feature branches for all new features and fixes\nMerge feature branches into the develop branch using pull requests\nWhen ready to deploy to production, merge develop into the main branch, conduct QA testing, then deploy when ready.\nDeploying VRO describes how branches are used for deploying specific versions of the code.\nSee Repository Branch Protection Settings for more configuration settings\nUse feature branches for your work\n\nDevelop your features and fix bugs in feature branches based off your develop branch. These branches are also known as topic branches. Feature branches isolate work in progress from the completed work in the develop branch. Even small fixes and changes should have their own feature branch.\n\nCreating feature branches for all your changes makes reviewing history simple. Look at the commits made in the branch and look at the pull request that merged the branch.\n\nReview and merge code with pull requests\n\nThe review that takes place in a pull request is critical for improving code quality. Only merge branches through pull requests that pass your review process. Avoid merging branches to the develop branch without a pull request.\n\nPull Requests guidelines\nDevelopment process\n\nPull Request Policy:\n\nAll pull request reviews will require at least 2 approvals\nDraft pull requests should be created immediately when work on a branch begins. This is facilitate conversations about in-progress tasks and ensure no work gets forgotten in an orphaned branch.\nAll pull requests will use Squash and Merge\n\nCertain Github Actions are triggered for pull requests to scan the changes to meet security requirements and coding style conventions.\n\nPR Templates\n\nA PR template is a way to automatically populate the body of a PR, to ensure developers fill in useful information for the team to share context and have a historical record of changes.\n\nThe PR template is https://github.com/department-of-veterans-affairs/abd-vro/blob/develop/.github/pull_request_template.md.\n\nRelease Strategy\n\nTriggering a new release is currently a deliberate decision and is detailed in Deploying VRO. All of the pipeline releases will be created from tags off main. The team decides when to release a new version following best practices. We should encourage following the semver definition of:\n\npatch versions including minor updates or changes\nminor versions including updates and added functionality that is backwards compatible\nmajor versions including breaking changes and added functionality\n\nEach release will contain Release Notes specifying bug fixes, breaking changes, and new features.\n\nDeploy to Prod describes details about deploying to releases to production.\n\nRepository Branch Protection Settings\n\nThe following branch protection rules should be applied against main and develop:\n\nRequire a pull request before merging\nRequire Approvals: 2 approvers\nRequire status checks to pass before merging\nRequire branches to be up to date before merging"
  },
  {
    "title": "BIP Claims API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/BIP-Claims-API",
    "html": "VRO uses BIP Claims API to get information about and do updates to VBA Benefit Claims.\n\ncurrently it exposes two endpoints via RabbitMQ. To connect, use exchange name bipApiExchange. The end points are following.\n\nqueue name: getClaimDetailsQueue. Expects a long collectionId as input. Returns a BipClaim object.\nqueue name: setClaimToRfdStatusQueue. Expects a long collectionId as input. Returns a BipUpdateClaimResp object.\nqueue name: updateClaimStatusQueue. Expects a RequestForUpdateClaimStatus object as input. Returns a BipUpdateClaimResp object.\nqueue name: getClaimContentionsQueue. Expects a long collectionId as input. Returns a BipUpdateClaimResp object.\nqueue name: updateClaimContentionQueue. Expects an UpdateContentionModel object as input. Returns a BipUpdateClaimResp object.\n\nAccess to BIP Claims API is available only within VA firewall.\n\nBIP Claims API uses mTLS. VRO's mTLS implementation is detailed in BIP-APIs.\n\nIntegration Requirements\n\nThe following benefit claim data is retrieved\n\nTemporary station of jurisdiction\nExistence of contentions' special issues \"Rapid Decision Rating Version 1\" (RDR1) and \"Rapid Ready Decision\" (RRD)\n\nThe following are the updates to claim data\n\nRemoval of contentions' special issue RDR1\nUpdate of lifecycle status to \"Ready for Decision\" (RFD)\nOpen API Specification\n\nBIP Claim API's Open API Specification is available from the Swagger page.\n\nAll the end-points that are used within the VRO application is also available from the Mock Bip Claim Api Swagger page in the local development environment.\n\nCode Walkthrough\nSecurity Requirements\n\nBIP requires a Bearer JWT for access. Following claims are used\n\nSubject (sub)\nUser Id (userID): Custom - VRO system user\nIssuer (iss)\nStation Id (stationID): Custom - VRO system user facility (?)\nApplication Id (applicationID): Custom - VRO application name\nExpiration (exp)\nIssued At (iat)\n\nThe JWT is created before each API call in BipApiService createJwt method.\n\nSubject claim is hard-coded in createJwt. Expiration and Issued At claims are dynamicaly created in createJwt. The other claims are made available to the application with environment variables through application.properties.\n\nUser Id: BIP_CLAIM_USERID through bip.claimClientId\nIssuer: BIP_CLAIM_ISS through bip.claimIssuer\nStation Id: BIP_STATION_ID through bip.stationId\nApplication Id: BIP_APPLICATION_ID through bip.applicationId\n\nJWT is signed by a secret provided by the BIP API team. The secret is made available to the application with the environment variable BIP_CLAIM_SECRET through application.properties bip.claimSecret setting.\n\nIn the VRO Kubernetes environment the related Kubernetes secrets for the BIP environment variables are\n\nBIP_CLAIM_USERID: bip.bipClaimUserId\nBIP_CLAIM_ISS: bip.bipApplicationIss\nBIP_STATION_ID: bip.bipStationId\nBIP_APPLICATION_ID: bip.bipApplicationId\nBIP_CLAIM_SECRET: bip.bipClaimSecret\n\nNote that due to an issue in the application (BIP_CLAIM_ISS is not specied in Helm charts) bip.bipApplicationIss is not functional. The hard coded value in application.properties bip.claimIssuer setting is used. This is in the list of issues for future fixes.\n\nA set of BIP environment variables are available for local development by sourcing the setenv.sh script. There were attempts to move these to application-local.properties but failed. Please see the note in setenv.sh script.\n\nAPI Hostnames\nEnvironment\tHostname\ndev\thttps://claims-dev.dev.bip.va.gov\ntest\thttps://claims-test.dev.bip.va.gov\nint\thttps://claims-int.dev.bip.va.gov\nivv\thttps://claims-ivv.stage.bip.va.gov\nuat\thttps://claims-uat.stage.bip.va.gov\npat\thttps://claims-pat.stage.bip.va.gov\npdt\thttps://claims-pdt.stage.bip.va.gov\ndemo\thttps://claims-demo.stage.bip.va.gov\nperf\thttps://claims-perf.prod.bip.va.gov\npreprod\thttps://claims-preprod.prod.bip.va.gov\nprodtest\thttps://claims-prodtest.prod.bip.va.gov\ncola\thttps://claims-cola.prod.bip.va.gov\nprod\thttps://claims-prod.prod.bip.va.gov\nAPI Calls\n\nThe following end points are used\n\nGET /claims/{claimId}\nGET /claims/{claimId}/contentions\nPUT /claims/{claimId}/contentions\nPUT /claims/{claimId}/lifecycle_status\n\nThe base URL is made available to the application with the environment variable BIP_CLAIM_URL through application.properties bip.claimBaseURL setting. The corresponding Kubernetes secret is bip.bipClaimUrl.\n\nFor local development and testing a Mock Server is available in docker compose with host name mock-bip-claims-api.\n\nBIP API Service\n\nAll API calls are implemented in Bip API Service. Bip API Service uses the custom RestTemplate bean (qualifier: bipCERestTemplate) described in BIP-APIs.\n\nBip Api Service is available to rest of the application as a Spring service. The only current customer is Bip Claim Service which uses it through Bip Api Service Interface. This is mostly for historical progression of the implementation but also makes it possible to unit test Bip Claim Service more easily as Bip Api Service Interface is overridden in a test configuration.\n\nBIP Claim Service\n\nBIP Claims API related functionality is provided to the rest of the application through BIP Claim Service. The public methods hasAnchors, removeSpecialIssue, and markAsRFD should be recognizable from the VRO-v2-Roadmap. The public method completeProcessing is used to double check temporary station of jurisdiction as specified in requirements.\n\nVBMS\n\nVBMS (provided by VA's BIP (Benefits Integration Platform) team) has the following APIs:\n\nClaim Service - specs for Release 23.1 (version 35.0, 08/23/2021) are in a 0003AE_5.2.2.d_Claims v4.0 SCD 23.1.pdf\nupdate claims, contentions\ndependent on BEP (Business Enterprise Platform), BGS (Benefits Gateway Service, where claims and contentions are persisted), and Oracle 11G DB\nClaims API Swagger spec has more accurate info; code base\nClaim Evidence API (see prior section)\nBAM Shared Services\n\nVRO will need to mark a claim as eligible for being assessed for fast-tracking (via a VBMS/BGS special issue?) so that users don't work on the claim.\n\nWe should investigate and ask if Lighthouse's Benefits Claim API can be updated to include the capability.\n\nVRO will need to mark a claim or contention as ready-for-decision (RFD) so it can be adjudicated and fast-tracked.\n\nMPI\n\nWhen VRO checks for fast-track-ability, VRO queries an API (probably MPI) to map a given veteran identifier into an ICN, which is needed to query Lighthouse for health data.\n\nLighthouse has a connection to MPI, so VRO should leverage it\nBenefits Claims API v2 provides a way get a veteran's ICN using MPI, but the veteran's SSN, name, and DOB is needed. Working with Kayla and Derek Brown (Lighthouse) to enable lookup by file number (a.k.a., BIRLS ID)\n\nNote from a Slack post:\n\nMPI queries straight from vets-api are mocked on localhost. This is necessary because MPI can only be accessed on the VA network, so our local machines don‚Äôt have direct access, even to the MPI test stacks."
  },
  {
    "title": "BIP Claim Evidence API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/BIP-Claim-Evidence-API",
    "html": "VRO uses BIP Claim Evidence API to upload evidence PDFs to E-Folder.\n\nAccess to BIP Claims API is available only within VA firewall.\n\nBIP Claim Evidence API uses mTLS. VRO's mTLS implementation is detailed in BIP-APIs.\n\nIntegration Requirements\nVRO uploads PDF documents to E-Folder\nVRO specifies meta data about the document\nOpen API Specification\n\nBIP Claim Evidence API's Open API Specification is available from the Swagger page.\n\nThe only end point used from BIP Claim Evidence API is the /files end-point.\n\nFuture Work\n\nIt should be possible to activate the Swagger page for the Mock BIP Claim Evidence API similar to Mock BIP Claims API easily. But that has not been yet done.\n\nCode Walkthrough\nSecurity Requirements\n\nBIP requires a Bearer JWT for access. Following claims are used\n\nSubject (sub)\nUser Id (userID): Custom - VRO system user\nIssuer (iss)\nStation Id (stationID): Custom - VRO system user facility (?)\nApplication Id (applicationID): Custom - must be equivalent to Issuer per documentation\nExpiration (exp)\nIssued At (iat)\n\nThe JWT is created before each API call in BipCEApiService createJwt method.\n\nSubject claim is hard-coded in createJwt. Expiration and Issued At claims are dynamicaly created in createJwt. The other claims are made available to the application with environment variables through application.properties.\n\nUser Id: BIP_EVIDENCE_USERID through bip.evidenceClientId\nIssuer: BIP_EVIDENCE_ISS through bip.evidenceIssuer\nStation Id: BIP_STATION_ID through bip.stationId\nApplication Id: BIP_APPLICATION_ID through bip.applicationId\n\nJWT is signed by a secret provided by the BIP API team. The secret is made available to the application with the environment variable BIP_EVIDENCE_SECRET through application.properties bip.evidenceSecret setting.\n\nIn the VRO Kubernetes environment the related Kubernetes secrets for the BIP environment variables are\n\nBIP_EVIDENCE_USERID: bip.bipEvidenceUserId\nBIP_STATION_ID: bip.bipStationId\nBIP_APPLICATION_ID: bip.bipApplicationId\nBIP_EVIDENCE_SECRET: bip.bipEvidenceSecret\n\nA set of BIP environment variables are available for local development by sourcing the setenv.sh script. There were attempts to move these to application-local.properties but failed. Please see the note in setenv.sh script.\n\nAPI hostnames\nEnvironment\tHostname\nivv\thttps://vefs-claimevidence-ivv.stage.bip.va.gov\nstage\thttps://vefs-claimevidence-pat.stage.bip.va.gov\npdt\thttps://vefs-claimevidence-pdt.stage.bip.va.gov\nuat\thttps://vefs-claimevidence-uat.stage.bip.va.gov\nprodtest\thttps://vefs-claimevidence-prodtest.prod.bip.va.gov\nprod\thttps://vefs-claimevidence.prod.bip.va.gov\nAPI Calls\n\nThe only end-point that is used is\n\nPOST /files\n\nThe base URL is made available to the application with the environment variable BIP_EVIDENCE_URL through application.properties bip.evidenceBaseURL setting. The corresponding Kubernetes secret is bip.bipEvidenceUrl.\n\nFor local development and testing a Mock Server is available in docker compose with host name mock-bip-ce-api.\n\nBIP Claim Evidence API Service\n\nThe API call is implemented in Bip Claim Evidence API Service. Bip Claim Evidence API Service uses the custom RestTemplate bean (qualifier: bipCERestTemplate) described in BIP-APIs.\n\nBip Claim Evidence Api Service is available to rest of the application as a Spring service. The only current customer is Bip Claim Service which uses it through Bip Claim Evidence Api Service Interface. This is mostly for historical progression of the implementation but also makes it possible to unit test Bip Claim Service more easily as Bip Api Service Interface is overridden in a test configuration.\n\nBIP Claim Service\n\nBIP Claim Evidence API related functionality is provided to the rest of the application through BIP Claim Service. The public method uploadPdf is self explanatory."
  },
  {
    "title": "BIE Kafka Client ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/BIE-Kafka-Client",
    "html": "Intro\n\nStatus: Currently, this page is a Work-In-Progress Tech Spec. Once implementation is completed, update and transition this page to become documentation for the BIE Kafka Client microservice.\n\nEpic and tickets: Consume BIP contention event streams #1642\n\nGoal: Implement a BIE Kafka Client microservice that other VRO components can use to subscribe to and handle BIE's Event Stream\n\nDiagram\nDotted arrows represent many events flowing, as a result of the topic subscription.\nThere may be one kafkaEvent-handler to handle multiple Kafka event exchanges; or one kafkaEvent-handlers per exchange.\n\nWhen VRO is deployed to LHDI, VRO uses the respective BIE env (TBD in Establish connectivity between VRO and BIE kafka service environments #1674). When VRO is run locally or as part of integration tests, the mock-kafka-server will be used instead.\n\nVerify connection to Kafka clusters using kcat (a.k.a kafkacat)\nvisit Lightkeeper-tool and setup lighthouse and kubectl cli tool and get the config file, set an alias for dev env alias kcdev=va-abd-rrd-dev\nrun kcdev apply -f sleep-pod.yaml to deploy a k8s pod to dev namespace using a manifest file like this one (run kcdev get pods you should see the pod from the list)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sleep-pod\n  namespace: va-abd-rrd-dev\nspec:\n  containers:\n    - name: sleep\n      image: debian:latest\n      command: [\"/bin/sleep\", \"3650d\"]\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: \"1Gi\"\n\nrun kcdev exec -it sleep-pod -- /bin/sh to access the pod\nrun apt-get update && apt-get install kafkacat to install kcat, then exit the pod\ncopy CA, private key, certificate from here (in VA network) and kafka_config.txt local files into the pod tmp folder e.g. kcdev cp /test.vro.bip.va.gov.pem sleep-pod:/tmp\n\nkafka_config.txt content\n\nbootstrap.servers=kafka.dev.bip.va.gov:443\nsecurity.protocol=SSL\nssl.key.location=test.vro.bip.va.gov.pem\nssl.certificate.location=test.vro.bip.va.gov.crt.pem\nssl.ca.location=VACACerts.pem\n\naccess the pod again and cd to tmp folder and run kcat -F kafkacat_config.txt -L\ncan access topic message by running kcat -F kafkacat_config.txt -L TST_CONTENTION_BIE_CONTENTION_ASSOCIATED_TO_CLAIM_V02 -C\nCreate keystore and truststore files\n\nRead the instruction and run the script\n\nIf done correctly, load the truststore file in Keystore Explorer, the setting should look like this \n\nKafka Topic Prefixes\n\nKafka Cluster Environments\n\nNOTE: PreProd environment urls have been updated to kafka.preprod.bip.va.gov:443 and https://schemaregistry.preprod.bip.va.gov:443\n\nBIE Kafka Schema Docs\nsvc-bie-kafka\nWhen this microservice starts up, it will immediately subscribe to Kafka topics based on a configuration settings.\nAuthenticate to mock BIE's Kafka service; only this microservice holds the credentials and is responsible for authenticating (microservice clients don't need to provide BIE credentials).\nWhen Kafka-topic events come in, send a RabbitMQ message into a RabbitMQ exchange with a payload as follows:\n# exchange: 'bie-events-contention-associated'\n  (async, one-way) payload = {\n    \"topic\": \"TST_CONTENTION_BIE_CONTENTION_ASSOCIATED_TO_CLAIM_V02\",\n    \"notifiedAt\": \"2023-06-12T19:29 UTC+00:00\", // when `svc-bie-kafka` received the event\n    \"event\": { ...entire Kafka event object }\n  }\n\n# exchange: 'bie-events-contention-deleted'\n  (async, one-way) payload = {\n    \"topic\": \"TST_CONTENTION_BIE_CONTENTION_DELETED_V02\",\n    \"notifiedAt\": \"2023-06-12T19:30 UTC+00:00\",\n    \"event\": { ...entire Kafka event object }\n  }\n\nThe RabbitMQ exchange name will be apriori constants, so the microservice and its clients know which exchange to use for each Kafka topic.\nFor the RabbitMQ exchange, create a topic exchange (or fanout exchange if desired), which will allow multiple svc-bie-kafka clients to subscribe to the topic.\nUsing routing keys was considered but we decided to implement a more straightforward 1-to-1 mapping between Kafka topic to RabbitMQ exchange.\nxample-workflow\nImplement an example VRO component that subscribes to RabbitMQ exchange(s) upon startup.\nIf using Java, it's encouraged to add a new Camel route to domain-xample/xample-workflows/ that does event-handling when a msg is available in the RabbitMQ exchange.\nWhen a Kafka event comes through the RabbitMQ topic exchange, log the event and save it to the DB.\nInitially, structure the DB entity generically. Iteratively add more specificity when we know what event fields should be stored as DB columns.\n-- table: 'bie_event' columns: \n- uuid: \"PK: Primary key used to reference this DB record\"\n- created_at: \"Creation time of this DB record\"\n- notified_at: \"When the event was received by VRO\"\n- event_type (indexed): \"Enum: contention-associated, contention-deleted, ...\"\n- event_details: \"JSON text with PII removed\"\n- event_at: \"Event timestamp extracted from event_details\"\n- benefit_claim_id (indexed): \"Benefit Claim ID extracted from event_details\"\n- contention_id (indexed): \"Contention ID extracted from event_details\"\n- classification (indexed): \"Classification of contention extracted from event_details\"\n\n-- table: 'diagnostic_code'\n- code: \"PK: Diagnostic Codes extracted from event_details\"\n\n-- many-to-many association table: 'bie_event_to_diagnostic_code'\n- bie_event_uuid: \"FK: Primary key used to reference this DB record\"\n- diagnostic_code: \"FK: Diagnostic Codes extracted from event_details\"\n# Follow guidance at https://stackoverflow.com/a/9790225\n\nTimestamps without a time zone should be in GMT\nCreate an integration test that registers a Kafka topic and publishes events to the topic, then checks if expected DB entry exists.\nFuture Improvements\n\nThe following possible improvements are not currently part of the Consume BIP contention event streams #1642 epic unless there is sufficient reason to include one or more of them.\n\nResilience: Store Kafka subscription state in Redis for observability.\nCorrectness when scaling: When multiple svc-bie-kafka instances, ensure there are no duplicate notification events in the RabbitMQ exchange."
  },
  {
    "title": "BIP APIs ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/BIP-APIs",
    "html": "VRO uses two BIP APIs.\n\nBIP Claims API\nBIP Claim Evidence API\n\nInformation specific to one of the individual APIs can be found in the above pages. This page, in addition to providing these links, describes the details of VRO Mutual TLS implementation.\n\nThe secrets that are pertinent to these integrations can be found in the HashiCorp vault under `vro-secrets/deploy/{ENV_NAME}/VRO_SECRETS_BIP and include:\n\nBIP_CLAIM_SECRET\nBIP_CLAIM_USERID\nBIP_EVIDENCE_SECRET\nBIP_EVIDENCE_USERID\nBIP_KEYSTORE_BASE64\nBIP_PASSWORD\nBIP_TRUSTSTORE_BASE64\nMutual TLS (mTLS)\n\nBIP APIs require mTLS. It is necesssary for VRO to use a client certificate signed by a VA recognized Certificate Authority (CA) during https handshake. In addition VRO needs to use a VA recognized CA to validate BIP certificates during https handshake.\n\nCertificates\nGenerated Certificates\n\nVenabi is the VA's new Internal Management System which can be used to manage the PKI certificates. Within VRO Kubernetes cluster the process is further simplified and it is possible to generate the certificates within one of VRO Kubernetes namespaces. The client certificates that are being used currently by VRO has been generated using these instructions.\n\nThe certificates tls.key, tls.crt, and va.crt has been generated in va-abd-rrd-dev namespace and can be found in va-abd-rrd-dev-va-gov-tls secrets.\n\nThe private key tls.key is not encrypted so there are no passwords needed for steps described later in this document.\n\nDetails for tls.crt can be displayed using\n\nopenssl x509 -in tls.crt -text -noout\n\nThe certificate tls.crt expires in January 6th, 2024.\n\nThe certificate tls.crt contains both the client public key and an intermediate CA. Only the client public key is used and will be referred as tls_bip.crt.\n\nThe certificate va.crt is not currently used.\n\nBy default, certificates generated are valid for one year, and are set to auto-renew ‚Öîrds of the way through their lifecycle.\n\nProduction Use\n\nA second set of certificates are generated in production. All the discussion in this document is identical for production certificates. The production certificate expires in March 17th, 2024.\n\nFuture Work\n\nAs discussed later in this document we do not yet know a way to use these certificates directly in the Java code and there are manual steps to make them usable. More discussions are needed with LHDI around the process on how to update them.\n\nThe certificates will be auto-renewed. What is the trigger to perform manual steps when the switch is made?\nShould LHDI make these certificates available to containers without additional steps?\nCan we forego container level https and move that complexity to API Gateway so that it is managed by LHDI?\n\nAt this time Java cert renewal is manual and steps in this document should be followed. We have time until January 6th, 2024 to make improvement.\n\nDownloaded Certificates\n\nDuring VRO implementation of mTLS we were not successful using the VA CA public keys in va.crt and tls.crt.\n\nInstead we downloaded the public keys that were used by BIP APIs from Chrome on a GFE:\n\nOpen Chrome on your GFE\nOpen one of the end points in BIP Claims API or BIP Claim Evidence API. If you use a GET end point you should typically get a jwt missing error\nOnce you get the error click on the lock (View site information) in the url bar to bring up a pop-up window\nSelect \"Connection is secure\" menu item to bring up a second pop-up window. From this pop-up you can select \"Certificate is valid\" to bring up the \"Certificate viewer\".\nGoto \"Details\" tab, select the certificate for root CA and \"Export\". Repeat for the intermediate CA\n\nTwo certificates downloaded are\n\nVA-Internal-S2-RCA1-v1.crt (root)\nVA-Internal-S2-ICA4.crt (intermediate)\n\nThese public keys are concatenated in a file named va_all.crt for later use.\n\nFuture Work\n\nWe have not retried to use the VA CA public keys in va.crt and tls.crt after we successfully used va_all.crt. This should be revisited in the future since it is possible that the steps we eventually ended up with could be succesful for public keys in va.crt and tls.crt as well.\n\nSelf-Signed Certificates\n\nVRO generates a set of self-signed certificates using a script called build-certificates.sh. These certificates are used for local development and end-to-end tests to mock the actual certificates and mTLS based https handshake.\n\nMore details and what is being generated are documented in the script itself.\n\nVRO mTLS Implementation\nPKCS #12 Files\n\nAs of now we do not know a simple way to consume the certificates tls.key, tls_bip.crt, and va_all.crt directly in the Java code to utilize them in https calls. Java uses either JKS files or PKCS #12 files. Since PKCS #12 files are not Java specific, we prefer them over JKS files in VRO.\n\nTo generate the PKCS #12 file keystore.p12 for client certificates we use openssl\n\nopenssl pkcs12 -export -in tls_bip.crt -out keystore.p12 -name keystore -nodes -inkey tls.key\n\nThis commands asks for an \"export password\". This password is to be recorded as it needs to be later used as a secret.\n\nTo generate the PKCS #12 file truststore.p12 for VA CA certificates we use keytool\n\nkeytool -import -file va_all.crt -alias all_cas -keystore truststore.p12\n\nThis command asks for a \"keystore password\". For simplicity VRO uses the same the value of \"export password\".\n\nFuture Work\n\nIt should be possible either to use keystore or openssl exclusively here. But these PKCS #12 files worked and we did not try either route any further.\n\nKubernetes Secrets\n\nVRO uses three environment variables to store the content of the [PKCS #12] files and the \"export password\".\n\nBIP_KEYSTORE\nBIP_TRUSTSTORE\nBIP_PASSWORD\n\nSince PKCS #12 files are binary we convert them to Base 64 using openssl\n\nopenssl base64 -in keystore.p12 -out keystore.b64\nopenssl base64 -in truststore.p12 -out truststore.b64\n\nWhat is stored in the environment variables BIP_KEYSTORE and BIP_TRUSTSTORE are the content of the Base 64 files.\n\nIn our environments these environment variables are made available to the app container using the following secrets\n\nbip.bipKeystore\nbip.bipTruststore\nbip.bipPassword\nManual Steps\n\nIn summary the manual steps to make the certificates available to the Java code are\n\nConvert certificates to PKCS #12 files\nConvert PKCS #12 files to Base 64 files\nStore content of the Base 64 files in Kubernetes secrets\nRestart the pod to make secret changes effective\n\nIf https handshake implementation remains in the Java code (as opposed to say moved to API Gateway) these manual steps need to be automated if certificate renewal is automated.\n\nJava implementation and Code Walkthrough\n\nJava uses Keystore objects to store certificate information in PKCS #12 files. VRO\n\nReads in the content of the PKCS #12 files and the password from the environment variables BIP_KEYSTORE, BIP_TRUSTSTORE, and BIP_PASSWORD through application.yml\nConverts Base 64 content to binary content\nCreates the keystore and trusstore objects as Keystore instances from the binary content and the password\nCreates a custom RestTemplate bean that can be used to make the https requests\n\nThe properties in application.yml that corresponds to the environment variables are\n\nkeystore for BIP_KEYSTORE\ntruststore for BIP_TRUSTSTORE\ntruststore_password for BIP_PASSWORD\n\nThe RestTemplate bean that is used to make the https requests is implemented in BipApiConfig. This file also includes the generation of the Keystore objects from the application.yml properties. This RestTemplate bean is autowired in the rest of the code using the Qualifier bipCERestTemplate.\n\nCurl Validation of Certificates\n\nValidation of the certificates using the Java code has been problematic since the BIP APIs are not available outside of VA Firewall\n\nOur GFE's typically do not have Java installed\nExtra permissions are necessary to install and maintain Java on the GFE's\nAdditional security features in GFE's makes it difficult to make https calls from Java\n\ncurl has been an invaluable tool to test the validity of the certificates. In principle it should be possible to run curl in your GFE but running it from the app container in one of our environment was easier. We copied the certificates tls.key, tls_bip.crt, and va_all.crt to the container using cat <<EOF and copy&paste in the /tmp directory. You should be able to use kubectl cp with enough permissions. Once the files are in the container you can run\n\ncurl -X POST <https://......> --cacert va_all.crt --cert tls_bip.crt --key tls.key --verbose\n\nTo see the https handshake to verify the validity of the certificates. The request can be made to any end point of interest."
  },
  {
    "title": "BIE contention event queues User Guide ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/BIE-contention-event-queues-User-Guide",
    "html": "Receive Contention Event data\nSubscribe to the following RabbitMQ Queues\nbie-events-contention-associated-to-claim (Kafka topic CONTENTION_BIE_CONTENTION_ASSOCIATED_TO_CLAIM_V02)\nbie-events-contention-updated (Kafka topic CONTENTION_BIE_CONTENTION_UPDATED_V02)\nbie-events-contention-classified (Kafka topic CONTENTION_BIE_CONTENTION_CLASSIFIED_V02)\nbie-events-contention-completed (Kafka topic CONTENTION_BIE_CONTENTION_COMPLETED_V02)\nbie-events-contention-deleted (Kafka topic CONTENTION_BIE_CONTENTION_DELETED_V02)\n\nNote: VRO Kafka Service Design Diagram\n\nRabbitMQ Sample Message Payload\n{\n  \"status\": 200,\n  \"eventType\": \"CONTENTION_UPDATED\",\n  \"claimId\": 12345678,\n  \"contentionId\": 12345678,\n  \"contentionTypeCode\": \"NEW\",\n  \"contentionClassificationName\": \"Musculoskeletal - Wrist\",\n  \"notifiedAt\": 1694451084874,\n  \"occurredAt\": 1694451062000\n}\n\neventType - There are total of 5 different event types:\nCONTENTION_ASSOCIATED_TO_CLAIM\nCONTENTION_UPDATED\nCONTENTION_CLASSIFIED\nCONTENTION_COMPLETED\nCONTENTION_DELETED\nLocal Development Setup\n\nMake sure you have the latest version of abd-vro and abd-vro-dev-secrets cloned in sibling directories. Run these commands\n\nexport COMPOSE_PROFILES='kafka'\nsource scripts/setenv.sh\n\n./gradlew docker\n\n./gradlew :dockerComposeUp\n\n./gradlew -p mocks docker\n./gradlew -p mocks :dockerComposeUp\n\n./gradlew :domain-xample:dockerComposeUp\n./gradlew :app:dockerComposeUp\n\npushd svc-bie-kafka && { source ./docker-entryprep.sh; popd; }\n\n./gradlew :svc-bie-kafka:integrationTest\n\n\nNOTE: The BIE team does not provide any mechanism to test or send Kafka payloads in non-local environments. So, testing in these environments is very limited since we need to wait for Kafka payloads in order to see the data in RabbitMQ queues.\n\nSample code for connecting to RabbitMQ Queues\n\nPython using hoppy\n\nJavascript Sample Code"
  },
  {
    "title": "BGS API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/BGS-API",
    "html": "BGS Web Services documentation\n\nAPI spec: Benefits Gateway Services (BGS) Web Services v120.doc\n\nClyde cautions about going around VBMS to do thing directly with BGS:\n\nThe reason is that anything that just goes to BGS has to be sync‚Äôd with VBMS for functionality in things like NWQ and other places to work. If you can do it through VBMS it would be a clearer architecture and reduce the need to sync more things.\n\nThe manageContention service from BGS could be used for updating special issues."
  },
  {
    "title": "API Gateway ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/API-Gateway",
    "html": "To support examination and testing of APIs implemented for particular domains and in various programming languages, VRO offers an API Gateway that provides a Swagger UI for manual exploration of APIs offered by VRO.\n\nRefer to PR #1591 and PR #1578 for details.\n\nTo expose APIs for each domain, an API Gateway offers a Swagger UI to inspect API offerings.\n\nIn a local (docker-compose) environment, it is available at http://localhost:8060/.\nIn the Dev (LHDI) environment, it is available at https://dev.lighthouse.va.gov/abd-vro/. It's also available in other LHDI Development Environments.\n\nWhen the user selects a \"Destination\" in the Swagger UI, the API Gateway queries domain-specific containers to retrieve the OpenAPI spec and presents them in a Swagger UI.\n\nAdding a new domain API for LHDI deployments and Swagger UI\n\nPre-requisite: a domain container serves up an OpenAPI spec on some exposed port in LHDI\n\nFirst, update Helm configurations to route domain URLs to the desired domain container:\n\nIn the VirtualService, add a URL rewrite, route destination, and header X-Forwarded-Prefix configuration\nThe URL rewrite makes the URL compatible with the route destination in the domain container.\nThe X-Forwarded-Prefix header is needed to prepend the prefix to URLs generated by the domain container.\nDeploy to LHDI and test to make sure the URL rewrite and X-Forwarded-Prefix works.\nIn particular, ensure that the URL to the domain's OpenAPI spec works.\n\nThen, update Swagger UI of the API Gateway:\n\nAdd an entry with the URL to the domain's OpenAPI spec to api-gateways application.yml\nTest the API Gateway locally (see section below) and update application-local.yml with URL rewrite configuration for the new domain container.\nDeploy to LHDI and test VRO's Swagger UI. For the dev LHDI env, go to https://dev.lighthouse.va.gov/abd-vro.\nTesting the API Gateway locally\nRun the API Gateway: ENV=local ./gradlew  :api-gateway:bootRun\nENV=local causes api-gateway/src/main/resources/application-local.yml to be loaded. In this file, the spring.cloud.gateway.routes configurations cause URLs to be rewritten, similar to the VirtualService settings in the Helm configuration.\nAlternatively, to use the api-gateway defined in docker-compose.yml, run docker compose up -d api-gateway.\nRun the domain APIs:\n# Start VRO's App API\n./gradlew :app:dockerComposeUp\n# This API's Swagger UI is available at http://localhost:8080/swagger\n\n# Optionally start a domain API, such as Team CC's API -- see domain-cc/README.md\n\nBrowse to http://localhost:8060/, read instructions, and click Swagger UI\nNote the definition \"0. Gateway API\" is selected and its basic API is shown. Expand the GET /hello endpoint and click \"Try it out\", then \"Execute\". The response should be a code 200 with body Hi!.\nSelect the definition \"App API\". To \"Try it out\", make sure to select the \"/vro-app\" server on the left dropdown. Selecting this server will cause requests to go to http://localhost:8060/vro-app/<endpoint>, which will get re-written as specified in application-local.yml (i.e., requests will be rerouted to the vro-app container at http://localhost:8110/<endpoint> by default).\nImplementation Details\nIt queries URLs specified by springdoc.swagger-ui.urls. The same URLs are routed differently depending on the environment:\nIn the local environment, the URLs are rerouted using spring.cloud.gateway.routes\nIn the LHDI environments, the URLs are rerouted using Istio's VirtualService in Helm\nTo have the Swagger UI offer the correct URL prefix to access the API endpoints, the server URL environment variable is set in Helm configurations.\nThe source API can be accessed directly (without going through the API Gateway). For example, to access the VRO App API:\nIn the local environment, http://localhost:8110/swagger-ui/index.html and http://localhost:8110/v3/api-docs\nIn the Dev (LHDI) environments, https://dev.lighthouse.va.gov/vro-app/swagger-ui/index.html and https://dev.lighthouse.va.gov/vro-app/v3/api-docs (based on vro-app's serviceUriPrefix)\nFor this to work, the Spring's OpenAPI implementation requires the X-Forwarded-Prefix header to be set correctly.\nThe API Gateway serves up it's own API in api-gateway/src/main/java/gov/va/vro/ApiGatewayRestController.java. It can be extended to include domain-independent deployment info, health of containers, and stats.\nDecision History\n\nAn API Gateway provides a single location to access all APIs provided by VRO, regardless of implementation language -- see ticket #1512. As a quick solution, it was implemented using Spring Cloud Gateway. It could be replaced with another API Gateway solution (e.g., Kong, which is used for other VA products) if the current implementation is insufficient or as new use cases arise."
  },
  {
    "title": "Apache Camel ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki/Apache-Camel",
    "html": "Apache Camel 3.11\n\nVRO uses Apache Camel 3.11.0, which uses Spring Boot 2.5.3. Apache Camel 3.13 through 3.15 are not compatible with gov.va.starter's current Spring Boot version (2.5.2).\n\nCamel-related code is limited to a very small portion of the codebase but is important for routing requests through desired workflows and to microservices.\n\nWhy Apache Camel?\n\nApache Camel provides a well-tested and stable implementation of Enterprise Integration Patterns (EIP) so that we can focus on VRO functionality and less on \"glue code\".\n\nWe use it to define and quickly update business workflows that connect VRO functionalities (a.k.a. microservices).\nCamel provides many core components and non-core components to connect to message queues, databases, email services, AWS services, etc.\nUsing Camel encourages VRO functionalities to be loosely-coupled and single-responsibility with the intent of improving software agility.\nCamelApp\n\nTo demonstrate how a Spring-based app uses Apache Camel to implement a microservice architecture, a sample CamelApp project is available for you to fork. Use it to:\n\nfamiliarize yourself with Java, Gradle, Spring, and Camel tools\nexperiment with the code\n\nFor a description of specific uses in VRO, see Routing API requests.\n\nCamel Concepts\n\nDesign concepts as they are applied to VRO: Camel Components, Camel Routes, and microservices\n\nMost VRO features are implemented into Camel Components (think Lego bricks). These components are reusable and replicable, and should be idempotent and robust to failures. Camel provides many components (mostly to integrate with other tools), and we can implement our own to process claims, generate PDFs, assess health data, etc.\n\nA processing workflow is implemented by connecting these components with Camel Routes. These routes can include actions that perform basic logic, data transformations, and filtering to map and send a payload (i.e., a JSON data object) for consumption by the next component.\n\nWhen there is a client to interface with some external system (e.g., Lighthouse API), it should typically be implemented as a microservice that listens on and responds to a message queue (e.g., RabbitMQ) so that anything (e.g., Camel component or route) can use the client. These microservices should also be idempotent and robust to failures. In a Camel route, a microservice can be treated like a Camel component."
  },
  {
    "title": "Home ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub",
    "url": "https://github.com/department-of-veterans-affairs/abd-vro/wiki",
    "html": "Virtual Regional Office (VRO) Overview\nVRO Architecture Diagram\nVRO Team working docs\nCulture and Norms\nTeam Processes\nVRO Engineer Onboarding\nActive efforts\nBIE Kafka Client\nBIE Contention Events User Guide\nVA.gov Data Visibility Initiative\nProblem Overview\nWelcome VRO Developers!\nSoftware Conventions\nVRO RabbitMQ Strategy\nLHDI's Boilerplate Instructions\nLocal Setup\nJetbrains SpringBoot Run Configuration Setup\nCode structure\nRouting API requests\nApache Camel defines processing workflows\nConfiguration settings\nDomain Applications in VRO\nDocker Compose\nDocker containers\nDevelopment process\nGradle\nPull Requests guidelines\nChange Management Plan\nCI CD Workflows\nDeploying VRO\nContainer Image Versions\nAPI Gateway\nExternal APIs to interact with other systems\nBIP APIs\nLighthouse APIs\nBGS API\nBIE Kafka Event Stream\nVRO Database\nTesting\nTesting using Swagger UI\nDevelopment environments\nEnd to End Tests\nMock Services\nPartner Teams\nNew Domain Setup\nPartner Team Deploy Process\nWelcome Contention Classification Developers!\n\nUpdating Contention Classification DC Lookup Table üìã\n\nWelcome Employee Experience Developers!\nWelcome DevOps!\nDeploying VRO\nLightkeeper tool\nKubernetes clusters\nHelm Charts\nDeploy to Prod\nQuick Deploy Instructions\nGithub Actions\nMachine User Account\nTokens and Secrets\nVRO Secrets\nSecrets Vault\nMaintenance\nDataDog monitoring\nDependabot\nWelcome Q/A!\nVRO Test Cases\nDeeper topics\nLighthouse DI Documentation repo - including diagrams\nDive into RabbitMQ/Microservice reliability\nSupport Model\nSupport Model ( Draft )"
  }
]
