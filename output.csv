title,url,html
Home ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/,"Virtual Regional Office (VRO) Overview
VRO Architecture Diagram
VRO Team working docs
Culture and Norms
Team Processes
VRO Engineer Onboarding
Active efforts
BIE Kafka Client
BIE Contention Events User Guide
VA.gov Data Visibility Initiative
Problem Overview
Welcome VRO Developers!
Software Conventions
VRO RabbitMQ Strategy
LHDI's Boilerplate Instructions
Local Setup
Jetbrains SpringBoot Run Configuration Setup
Code structure
Routing API requests
Apache Camel defines processing workflows
Configuration settings
Domain Applications in VRO
Docker Compose
Docker containers
Development process
Gradle
Pull Requests guidelines
Change Management Plan
CI CD Workflows
Deploying VRO
Container Image Versions
API Gateway
External APIs to interact with other systems
BIP APIs
Lighthouse APIs
BGS API
BIE Kafka Event Stream
VRO Database
Testing
Testing using Swagger UI
Development environments
End to End Tests
Mock Services
Partner Teams
New Domain Setup
Partner Team Deploy Process
Welcome Contention Classification Developers!

Updating Contention Classification DC Lookup Table üìã

Welcome Employee Experience Developers!
Welcome DevOps!
Deploying VRO
Lightkeeper tool
Kubernetes clusters
Helm Charts
Deploy to Prod
Quick Deploy Instructions
Github Actions
Machine User Account
Tokens and Secrets
VRO Secrets
Secrets Vault
Maintenance
DataDog monitoring
Dependabot
Welcome Q/A!
VRO Test Cases
Deeper topics
Lighthouse DI Documentation repo - including diagrams
Dive into RabbitMQ/Microservice reliability
Support Model
Support Model ( Draft )"
VRO v1 Roadmap ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-v1-Roadmap,"Task Dependency diagram for Version 1.0 of VRO 

Legend:

gray box: completed task ‚úÖ
red-outlined box: in-progress task üèÉ
light-green box: low priority task that could be removed from version 1.0 scope if needed

Tasks for Version 1.0 of VRO (only for asthma and hypertension; targeting Aug 31st to have VRO in operation, if not sooner)

‚úÖ setup: Set up RabbitMQ container, Camel routes, and deploy to LHDI
‚úÖ version 0.1
‚úÖ ruby_pdf_generator Service: Ruby PDF generator (given all pdf contents as request parameters)
‚úÖ generate_summary_pdf_Endpoint: Add generate_summary_pdf endpoint to API
‚úÖ ruby_assess_data Service: Ruby assess fast-tracking eligibility given all health data
‚úÖ assess_data_Endpoint: Add assess-data-for-fast-track endpoint to API (given all health data)
üèÉ get_ATO: Get ATO for deployment into prod
‚úÖ add_authentication: Add authentication to API endpoints; using simple API-key for now
db
‚úÖ save_claims_to_db: Save claims processing to DB. See LHDI's persistent volumes.
claim_stats_Endpoint (low priority): Add claim_stats endpoint to API for reporting and monitoring. Other metrics (from Zach):
how many claims are we evaluating programmatically for sufficient evidence?
how many of those claims are deemed to have sufficient evidence?
how many claims deemed to have insufficient evidence lead to an auto-exam?
how many claims that are deemed to have sufficient evidence lead to: a) immediate rating, and b) not an immediate rating (e.g., deferral, manual exam)?
‚úÖ devops
url_for_vro: Add DNS entry for a url to VRO for all relevant envs; LHDI DNS and routing
deploy_to_non_prod_envs: Deploy VRO to staging/preprod VA env
test_in_non_prod_envs: Test VRO to staging/preprod VA env
deploy_to_prod: Deploy to LHDI's Prod environment (via GH Action or ArgoCD)
OBE use_ver0.1_endpoints: Have RRD (in vets-api) call 2 endpoints of VRO version 0.1
‚úÖ assess_claim
‚úÖ lh_health_api_client: LH Patient (Veteran) Health API (FHIR) client. See existing Ruby code VeteransHealth::Client.
‚úÖ assess_claim_service: Assess fast-tracking eligibility given claim (VRO must query for health data). See existing Ruby code (HypertensionProcessor, AsthmaProcessor)
‚úÖ assess_claim_Endpoint: Add assess-claim endpoint to API that doesn't require health data
‚úÖ use_assess_claim_Endpoint: Have RRD (in vets-api) call VRO's assess-claim endpoint
‚úÖ evidence_summary_doc (low priority - we can use the Ruby version in the meantime)
‚úÖ generate_summary_doc Service (low priority): Improved PDF generator (given all pdf contents as request parameters)
‚úÖ generate_summary_doc_Endpoint (low priority): Add generate_summary_doc endpoint to API
‚úÖ use_generate_summary_Endpoint: Have RRD (in vets-api) call VRO's evidence_summary_doc endpoint

Implied acceptance criteria for tasks above:

‚úÖ API endpoints should have OpenAPI spec documentation with examples.
‚úÖ Authentication is required for all service endpoints (assess_claim_Endpoint and generate_summary_doc_Endpoint) after version 0.1
‚úÖ Integration testing between RRD (in vets-api) and VRO is needed in a non-prod env before deployment to prod
‚úÖ VRO passes Secure Release process
Code improvements tasks

Would be ideal to get these completed as part of version 1.0:

Document how to update version numbers of VRO, container images, etc.
Mechanism to ensure microservices are idempotent and following good patterns
Improve robustness to failures by using retry pattern -- retry logic in Camel
üèÉ Clean up codebase; comment out unused dependencies
Add more automated end-to-end and integration testing
Additional DevOps tasks

Would be ideal to get these completed as part of version 1.0:

Persist RabbitMQ message queue contents for reloading in case of failures
Connect VRO non-prod env with other VA non-prod envs
Set up test code coverage and other PR requirement checks. Search jacoco_enforce_violations in the code.
Enable linter and pre-commit hook
Set up LHDI monitoring and diagnostics tools, i.e. DataDog monitoring, Prometheus, Grafana, Jaeger
Set up DB querying (BI) tools for reporting, e.g., Metabase/Superset
Set up DB monitoring tools to detect slow queries, e.g., NewRelic
Document how to debug and modify DB data in prod
Document how to extract microservices into Docker containers and deploy them
Add Python bug and vulnerability scanners (SecRel does not currently support Python code scanning)"
VRO v2 Roadmap ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-v2-Roadmap,"iMVP (Integrated MVP)

Workflow Diagram

Legend:

light-green box: implemented in v1
trapezoid: microservice for External APIs; listens for requests added to an associated RabbitMQ queue
rounded box: decision point (consider implementing using Camel Dynamic Routing)
""DB"" all refer to the same database and ""bipClaimsApi` represents the same microservice; they're drawn separately to reduce clutter
Also to reduce clutter, the VRO_API_error box represents returning an error code back to the VRO API client.

For details, refer to:

iMVP-claim-and-contention-update-scenarios

Another workflow digram from Mural

Tasks

(only for single-issue hypertension CFI and presumptives; targeting Dec 31st deployment)

Ordered by priority:

#433 VRO retrieves OCR results from MAS/VBAAP
#434 Exam-status endpoint for MAS/VBAAP to notify VRO
#428 IncomingClaims: VRO receives hypertension CFI claims from MAS/VBAAP
#435 Exam ordering: VRO calls MAS/VBAAP to order an exam
#432 VRO handles PDF upload to eFolder
#430 Mark claim ""RFD"" to enable appropriate adjudicator assignment
#431 Remove special issues to enable claim downstream routing
#436 Parse OCR annotation data
(OCTO) Add feature flag (a.k.a. feature toggle) service
(OCTO) Encrypt RabbitMQ messages
fix Python microservices shut down when RabbitMQ shuts down

Handling presumptives (NEW claims):

#446 Check fast-tracking eligibility for presumptive claims
#447 Use OCR data when assessing sufficient health evidence
#448 Incorporate health evidence from OCR data into the evidence summary document
#905 iMVP RFD and PDF logic changes
#811 Relevant documents unavailable for automated review
#1201 Support parity between MAS- and VRO-generated ARSDs

Other tasks:

DB metrics
claim_stats_Endpoint: Add claim_stats endpoint to API for reporting and monitoring. Other metrics (from Zach):
how many claims are we evaluating programmatically for sufficient evidence?
how many of those claims are deemed to have sufficient evidence?
how many claims deemed to have insufficient evidence lead to an auto-exam?
how many claims that are deemed to have sufficient evidence lead to: a) immediate rating, and b) not an immediate rating (e.g., deferral, manual exam)?
Ensure RabbitMQ messages are processed despite microservices failures
üèÉ (Ryan, Cameron) Connect VRO non-prod env with other VA non-prod envs
üèÉ Clean up codebase; comment out unused dependencies
üèÉ (Afsin) Add more automated end-to-end and integration testing
üèÉ (OCTO) Document how to update version numbers of VRO, container images, etc.
DONE: Persist API request contents for reloading in case of failures
Document how to extract microservices into Docker containers and deploy them
Document how to debug and modify DB data in prod
De-duplicate BP readings between LH and HDR (#1373)
Code improvements tasks
Mechanism to ensure microservices are idempotent and following good patterns
Document conventions on how requesters handle errors, such as resubmitting the request -- see comment (OBE task: Improve robustness to failures by using retry pattern -- retry logic in Camel)
Additional DevOps tasks
DONE: Set up test code coverage and other PR requirement checks. Search jacoco_enforce_violations in the code.
DONE: Enable linter and pre-commit hook
Set up LHDI monitoring and diagnostics tools, i.e. DataDog monitoring, Prometheus, Grafana, Jaeger
Set up DB querying (BI) tools for reporting, e.g., Metabase/Superset
Set up DB monitoring tools to detect slow queries, e.g., NewRelic"
LHDI's Boilerplate Instructions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/LHDI%27s-Boilerplate-Instructions,"VRO software is deployed on the Lighthouse Delivery Infrastructure (LHDI) platform, which offers tools, services, and support team.

LHDI's Java Starter Kit was used to populate this codebase (see PR #8) using Java (AdoptOpenJDK) 17 and Gradle 7.4.

LHDI's Boilerplate Setup Instructions
About
CI/CD Pipeline
Secrets
Default Pipeline Behavior
Dependencies
Deploying the Application
What's Next
About

This is a Java Spring Boot application that has been created using the Lighthouse DI Java 17 Starterkit. It is intended to be used as a starting point for building Java APIs and should be customized to deliver whatever functionality is required. If no other changes have been made, this application will have these features included by default.

CI/CD Pipeline

This project comes with a skeleton Github Actions CI/CD pipeline out of the box. You can always choose to rewrite the pipeline using a different CI/CD tool; this pipeline serves as an example that you can use and run with minimal setup.

Secrets

In order to run the pipeline, you will need to create a personal access token and add it to your repository's secrets in Github. The access token should have write:packages scope.

The secrets you need to configure are

ACCESS_TOKEN: the personal access token
USERNAME: the Github username of the user who owns the access token
Default Pipeline Behavior

The default pipeline has 3 jobs, which do the following things:

Runs CIS benchmark tests against the application Docker image using docker-bench-security
Builds and tests application
Publishes Docker image to VA GHCR repository
Dependencies

The pipeline runs on Github's ubuntu-latest runner, which is currently Ubuntu 20.04. The Github Actions Ubuntu 20.04 documentation lists the software installed by default. To learn more about choosing a Github runner and Github-hosted runner types, see the job.<job-id>.runs-on documentation.

Software required for the pipeline but not installed by default, such as Java 17, hadolint, and spectral, is installed in the pipeline. The installation for app build dependencies is implemented as an action in <./.github/actions/setup-pipeline/action.yml>.

Deploying the Application

The pipeline does not currently deploy the application to the DI Kubernetes clusters out of the box, although this setup will be coming in the future. To learn how to deploy your applications, see the DI ArgoCD docs.

Common Errors

Error: Cannot find plugin

Error Message:

* What went wrong:
Plugin [id: 'gov.va.starter.plugin.cookiecutter', version: '0.1.20', apply: false] was not found in any of the following sources:

- Gradle Core Plugins (plugin is not in 'org.gradle' namespace)
- Plugin Repositories (could not resolve plugin artifact 'gov.va.starter.plugin.cookiecutter:gov.va.starter.plugin.cookiecutter.gradle.plugin:0.1.20')
Searched in the following repositories:
    MavenLocal(file:/Users/aasare/.m2/repository/)
    Gradle Central Plugin Repository
    MavenRepo
    BintrayJCenter
    maven(https://palantir.bintray.com/releases)
    maven2(https://dl.bintray.com/adesso/junit-insights)
    starterBootPkgs(https://maven.pkg.github.com/department-of-veterans-affairs/lighthouse-di-starter-boot)
    nexus(https://tools.health.dev-developer.va.gov/nexus)


Fix: Set your Github token as per the instructions in the Required Dependencies section above.

Error: Failed to get resource

Error Message:

Failed to get resource: GET. [HTTP HTTP/1.1 401 Unauthorized: https://maven.pkg.github.com/department-of-veterans-affairs/lighthouse-di-starter-boot/starter/java/build-utils-property-conventions/starter.java.build-utils-property-conventions.gradle.plugin/0.1.32/starter.java.build-utils-property-conventions.gradle.plugin-0.1.32.pom)]


Fix: Set your Github token as per the instructions in the Required Dependencies section above, ensure that read:packages is true.

What's Next

Once you have verified that you are able to run the application successfully, you can now start customizing the application to deliver the functionality you would like.

By default, this application assumes the use of a build, test, release cycle as defined in this development guide. Take a look at that guide to see how you can make changes, test them and get them deployed to a target environment.

The application itself is organized into the following three tiers of functionality:

API
Service (business logic)
Persistence

To see how each of these tiers is used by default, take a look at the Project Structure documentation."
Virtual Regional Office (VRO) Overview ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Virtual-Regional-Office-%28VRO%29-Overview,"Document Purpose: To provide a catalog of VRO offerings along with links to more detailed documentation. This page will be updated regularly to ensure it contains the most current information.

Welcome to the VRO software platform! The VRO team is here to support you as a new partner product team getting up and running in VRO.

üëã Who is the VRO team?

We are the software platform team for the OCTO Benefits Portfolio, and we support partner teams building products that improve claim processing for digitally submitted (ie. va.gov) claims. Together, these products form the 'Virtual Regional Office' product suite.

We work to ensure that teams building in VRO have access to shared knowledge, processes, and tools, allowing you to quickly build and validate ideas that improve the VA's employee-facing claims process.

Chat with us and follow along in DSVA Slack: #benefits-vro-support.

Meet the team:

OCTO (VA) Enablement Team
Zach Goldfine, Product Owner
Julie Strothman, Design Lead
Cory Sohrakoff, Engineering Lead
Product & Research
Diana Griffin, Product Manager
Bianca Rivera Alvelo, Designer/Researcher
Software Engineers
Cheng Yin
Erik Nelsestuen
Josiah Jones
Teja Naraparaju
Tom Greene
DevOps Engineers
Mason Watson
üõ† VRO Features, Tools, and Support

VRO provides a software development environment for Claims Fast-Tracking Crew teams to quickly integrate with existing VA services and deploy to the VA's Lighthouse Delivery Infrastructure (LHDI) platform. In service of this goal, we offer key features relevant to claims processing products, tools to expedite your development process, and the support, guidance, and subject matter expertise of our team members.

üå± VRO is evolving! VRO is a new software platform, and will continue to grow and mature alongside our partner products. We collaborate with product teams to understand your application's needs and evolve VRO services and tools to meet them while incorporating them with the holistic needs of the product suite.

Features
Event-based, scalable microservice architecture

VRO implements an event-driven architecture with Queue-Processor components that act like an internal microservice, modularizing functionalities so that each can be updated and maintained more easily. These components are connected together using well-tested and stable Enterprise Integration Patterns (EIP) tools (such as Apache Camel) so that we can focus on VRO functionality and less on ‚Äúglue code‚Äù. This approach promotes low software coupling and, as a result, simplifies debugging and maintenance.

Learn more
Reusable software patterns

To help expedite time-to-deployment and maintain consistency between products, VRO provides reusable software patterns that implement VRO Software Conventions. VRO offers libraries and encourages reuse of software patterns to minimize onboarding, diagnosing, and development time.

VA system integrations

Claims Fast-Tracking products need to access and update claim data in order to deliver desired outcomes, which requires integrating with other VA systems. VRO offers a growing number of integrations with VA systems (such as Lighthouse APIs, BIP APIs, and BGS) and other services (such as Slack). Leveraging VRO's existing integration services could save many weeks (if not months) of work per integration.

Learn more
Database and Redis cache

VRO provides a dedicated Postgres database for persisting (non-PII and non-PHI) data, as well as a Redis cache to temporarily store and track data.

API gateway

VRO's API Gateway provides a single location to access all APIs provided within VRO, regardless of implementation language. To expose APIs for each domain, the API Gateway offers a Swagger UI to inspect API offerings, retrieving the OpenAPI spec for the selected API from domain-specific containers and presenting it in a Swagger UI.

Learn more
Tools
Established development process

VRO partner teams can get up and running quickly using our established development process with Gradle utilities and Github Actions workflows to automate code testing, Docker image creation, and housekeeping tasks.

Learn more about:

VRO development process
Gradle utilities
Github Actions
Flexible deployment configurations

For teams building in VRO, flexible deployment configurations and processes ease deployment under the VA's cATO (continuous authority to operate). The build and deployment pipeline incorporates the required Secure Release process and scanning, which minimizes software vulnerabilities and keeps the software up-to-date.

Learn more about:

Helm Charts for deployment configuration
Deployment process
CI CD Workflows
Secure Release (SecRel) process
Dependabot
Support
Software conventions

The VRO team develops, maintains, and expands our Software Conventions to help partner teams jump start their development, reduce time-to-deployment, and maintain consistency across products in the VRO product suite.

Inherited Continuous Authority to Operate (cATO)

Traditional ATO processes can add weeks or months of coordination and overhead before an initial product launch, but VRO benefits from Lighthouse Delivery Infrastructure's continuous ATO (cATO) process to stay in compliance while being able to deploy frequently to production. Products within the VRO product suite benefit further by inheriting VRO's ATO -- we incorporate your product scope into our cATO process so that you don't have to start from scratch.

Hands-on support and collaboration

The VRO team is here to support our partner product teams, from initial idea validation through development and launch, and with ongoing support and maintenance of a stable production environment where your product can grow and scale. We work iteratively to support new requirements and improve the maintainability and sustainability of the ABD-VRO codebase, with built-in DevOps to ensure VRO is operational and scaling as needed and team members available to help onboard partner teams, review Pull Requests, and offer technical support, product guidance, and design standards."
VRO Test Cases ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Test-Cases,"Latest Test Cases for VRO v1.0

amida vro test cases -17 aug 2022.xlsx"
VRO RabbitMQ Strategy ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-RabbitMQ-Strategy,"Scope

The goal of this page is to identify the strategy that VRO implements for Partner Teams' integrations with VRO microservices via RabbitMQ. This document includes:

VRO & Partner Team Responsibilities
Exchange / Queue Declarations and Bindings
Requests / Response Models
Partner Team Recommendations

For more information on RabbitMQ specifics or AMQP protocol concepts, consult the documentation on the RabbitMQ website.

VRO Microservice Responsibilities
declaring exchanges and their type (direct exchange, fanout exchange, topic exchange, or headers exchange)
declaring request queues
binding request queues to the exchanges
The following settings are expected for exchange declarations:
must be declared durable¬†(exchange will survive a broker restart)
must be auto-deleted (exchange is deleted when the last queue is unbound from it)
The following settings are expected for request queue declarations:
must be declared durable (queue will survive a broker restart)
must be auto-deleted (queue that has had at least one consumer will be deleted automatically when the last consumer has unsubscribed (disconnected))
must not be exclusive (queue is able to have more than a single connection besides declaring service)
Partner Team Responsibilities

In order for partner teams to communicate via RabbitMQ to VRO's microservices, queues and exchanges must be declared identically to the VRO microservice that will be processing requests and supplying responses. Failure to do so will result in the applications' inability to consume or publish to an exchange/queue, and depending on the RabbitMQ library client, a runtime exception.

The following settings are expected for exchange declarations:
must match VRO declarations for exchanges
must not publish messages to the default exchange
The following settings are expected for request queue declarations:
must match VRO declarations for queues
The following settings are expected for response queue declarations:
must declare response queues as necessary
must bind response queues to the appropriate exchanges
Publishing request requirements:
must provide reply_to property in request message
must correlate requests made to responses received by using correlation_id property in request message
must also keep track of those correlation_ids
must implement time-out or retries policies for requests where expected response was never received
must provide any other message level properties such as delivery_mode, see here for more info on message level properties in request message
must implement any sort of publisher acknowledgements if desired
Message consumption
must provide message consumer acknowledgements, rejections, or negative acknowledgements to the server upon receiving a response message
must validate the correlation_id if consuming a message as a response to a request
Request / Response Model

In order to allow multiple partner teams to utilize downstream VRO microservices, the request and response model for each of the VRO's microservices through RabbitMQ shall be consistent. Keep in mind, publishing with the correct message properties and payload is the partner team application's responsibility. It is also the responsibility of each VRO microservice to validate requests before processing, or, in the case of a pass-through service like svc-bip-api, pass the request as-is to a downstream service and report the response as reported by the downstream service.

Requests

The following structure shall be used for publishing requests to VRO microservices via RabbitMQ:

Required Message Properties:
content_type=""application/json""
app_id - name of calling application
reply_to - name of response queue if a response is desired
correlation_id - the ID assigned to a request that will be returned with a response to correlate the response to the request made if a response is desired, it will need to be correlated to a request
Required Payload:

The required payload is determined by the VRO microservice to which the requests are routed.

Responses
Required Message Properties:
content_type=""application/json""
correlation_id - id used to correlate the request for which this response was made, if the request contained a correlation_id
app_id - id of application returning the response
other message properties set by RabbitMQ (see here)
Required Payload (aka. response body)

VRO microservices are responsible for determining the majority of the body of the response. The only required fields are the integer value statusCode and string value statusMessage for any type of request. The statusCode should represent a typical rest response code or the VRO microservice's defined values. The statusMessage should be a string representation of the statusCode for quick readability of the response. If additional information regarding the status is required, the VRO microservices is responsible for defining that field to be added with statusCode and statusMessage.

Example payloads:
Response for a request that does not have any additional fields in response body:
{
   ""statusCode"":201,
   ""statusMessage"":""CREATED""
}

Response for a request that resulted in a 500
{
   ""statusCode"":500,
   ""statusMessage"":""INTERNAL_SERVER_ERROR""
}

Response for a request to a service that also returns an array of results:
{
   ""statusCode"":200,
   ""statusMessage"":""OK""
   ""results"":[
     {
        ""fieldName"":""thing1"",
        ""intVal"":1
     },
     {
        ""fieldName"":""thing1"",
        ""intVal"":1
     }
   ]
}

Partner Team Recommendations

Partner teams are welcome to implement their own solutions, provided that those solutions adhere to the information presented above. If there are any questions or concerns, contact the VRO team.

For parter teams implementing their application in python, they can use a solution that follows this strategy by utilizing the hoppy library for asynchronous request/response patterns with a client that has configurable RabbitMQ connection parameters, retries and timeout policies, queue and exchange declarations, and more!"
VRO Engineer Onboarding ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Engineer-Onboarding,"Editors: Any new additions to onboarding that require the reverse process for offboarding should be added to VRO Engineer Offboarding.
Scope:

This document is specifically designed for Engineers onboarding to the VRO (Virtual Regional Office) software project with the intent of streamlining and reducing the startup time for ramping up on VRO.

For offboarding process, see VRO Engineer Offboarding.

Technical Intro:

VRO Software uses the following:

Mostly Java - heavy use of the Spring Framework
Many microservices are written in Python (under the service-python folder)
Apache Camel - important to learn
RabbitMQ - used to communicate with microservices
Gradle - heavily used; also see the buildSrc folder
Docker - heavily used locally and for deployments
Note that for most use cases, this contract does not supply a Docker Desktop license, meaning that you will need a different container runtime to use Docker locally. Colima is one option that is emerging as a good alternative.
Postgres - learn as needed
Redis - learn as needed
Kubernetes (for DevOps) - LHDI provides the infrastructure
Current state (as of May 2nd, 2023): VRO is being updated to support partner teams, such as the Contention Classification team.
Engineer Checklist:

Below are actions you should take or links with resources to review. As you are going through onboarding, feel free to add other pages to this list if you think they'd be useful.

Get added to recurring team meetings such as¬†Weekly VBA Stakeholder Sync, Daily VRO Team Sprint Standups, Biweekly VRO Sprint Planning / Review / Retrospective, etc.,
Join relevant slack channels -- you don't have to join ALL these channels; start with the #benefits-vro-* and #benefits-cft-* and you can expand from there as you get more involved in the work. Feel free to ask the team in #benefits-vro if you're wondering about whether to join a particular channel.
Most relevant channels for you: #benefits-vro (our team channel for coordination across the VRO team), #benefits-vro-engineering (engineering-specific collaboration for the VRO team), #benefits-cft (cross-team collaboration channel for the 3 teams in the Claims Fast-Tracking Crew)
Watch ""The Way We Work"" presentation
Go through Benefits Portfolio onboarding materials and onboarding buddy meetings (see #benefits-onboarding channel in Slack; you'll be tagged there with your onboarding buddy assignments and materials to review)
Set up team 1:1s (See their slack profiles for a calendar URL where you can find available times):
VA Product Owner, Premal Shah
OCTO engineering leads, Steve Albers and Cory Sohrakoff
VA / VRO onboarding buddy meetings (likely initiated by them)
Get added to the VA GitHub organization (ask someone who is already in the org)
Get added to the OCTO-VRO GitHub team to have access to VRO's GitHub repos
Get added to the OCTO Benefits Portfolio roster
If you need access to VRO's LHDI deployment environments, get added to the VA-ABD-RRD GitHub team
If you need access to potentially ""dangerous"" actions (such as deploying to prod or deleting packages), get added to VRO-RESTRICTED.
When you have your VA.gov email, connect it to your GitHub account by adding as an email in your settings. You must also make sure the name on your profile matches your name. This will allow you to perform SSO to various apps through GitHub on the VA's network, e.g. Aqua.
When you have your PIV card, set up an Okta account following these instructions
Add your name and skills to POCs
Read VRO's wiki, in particular the following (some content may need updating):
Problems being solved and Roadmap provides context
Routing-API-requests, Code structure, and Development process (from Slack thread)
(Optional) If interested in the initial incarnation of VRO (before Amida touched it), watch old recording '2022-06 VRO code walk-through.mp4' in Shared Meeting Notes
Gradle projects
Current Software State is a good intro to the codebase history
LHDI's Boilerplate Instructions and if needed Setup on Windows. Amida wrote up some setup instructions for Windows in Software Design Document (linked from Roadmap#references)
External APIs to interact with other systems
Review the materials on this wiki page for walkthroughs of the code
Watch this demo from the Lighthouse Delivery Infrastructure (LHDI) team on the SecRel pipeline and SD Elements
Remember to periodically add what you're working on (or responsible for) to the Engineering POCs spreadsheet (pinned at the top of the #benefits-rrd-engineering Slack channel)
For background, context, and motivation, read VA Automated Benefits Delivery Team - Team Onboarding as a lower priority (whenever you have downtime between meetings or waiting on blockers to be resolved). Important topics from that document include:
Teams and vision: various responsibilities of various teams (requires sharepoint)
Slack usage

(Unlikely) If you'll be working in va.gov's codebase (a.k.a. RRD), you should review:

Technical onboarding
RRD technical overview
VA DevOps Release Process (may be out of date)
Currently, Yang and Yoom are familiar with va.gov's RRD codebase, which should be sufficient."
VRO Engineer Offboarding ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Engineer-Offboarding,"Editors: Any new additions to offboarding that require the reverse process for onboarding should be added to VRO Engineer Onboarding.
Scope:

This document is specifically designed for Engineers offboarding from the VRO (Virtual Regional Office) software project.

For onboarding process, see VRO Engineer Onboarding.

Offboarding checklist
Remove from recurring team meetings
Remove from VA-ABD-RRD team and VA-ABD-RRD GitHub team
Update status as deactivated on OCTO Benefits Portfolio roster
If onboarded to VFS Platform:
Open a Platform ticket to offboard user. This includes stuff like Slack, SOCKS, AWS, team roster spreadsheet ‚Äì pick and choose as needed.
If leaving vets-api codebase:
Open a PR to remove email from flipper.admin_user_emails in config/settings.yml
If staying at VA, ask them to leave Sidekiq team (otherwise, removal from GH org takes care of this)
If not onboarded to VFS Platform, the Platform offboard ticket template can still be used. But we can also open a separate ticket to remove user from GitHub org."
VRO Database for RRD ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Database-for-RRD,"OBSOLETE
Introduction

VRO Database is primarily used for audit purposes. It does not store any Personal Identifiable Information (PII) or Personal Health Information (PHI). The following information is available

Claim ID (VBMS ID), disability action type, presumptive and RFD (Ready for Decision) flags
Claim Submission data associated with the attempt to process each Claim Submission Request through the workflow
any external vendor ID (Reference ID ‚Äî used by external vendors to track the claim in their systems)
along with the associated type (representing the specific vendor associated with the Reference ID)
Non PII Ids for the Veteran who owns the claim
Contentions in the claim
Exam Order state as issued by VRO and updated from MAS
Non PHI summary information for the assessment for each of the contention
Non PHI summary information for each evidence PDF generated for the contention
Various event audit logs as a claim goes through VRO system
Database version information to support future changes through database migrations

All tables and fields are available in the Entity Relationship Diagram (ERD). This version of ERD is manually generated using DBeaver on 3/12/2023.

Note that the database is designed for multi issue claims although iMVP will not support such claims.

Technologies and Code Walkthrough
Database

VRO uses PostgreSQL. Since RDS is currently not available in LHDI, a PostgresSQL Docker based container serves as the database. The subproject postgres is used to build the container.

The Dockerfile in the postgres subproject is a simple wrapper around the Docker Hub PostgresSQL image. The primary reason to use the Dockerfile instead of using the Docker Hub image directly is to run the initialization script init_db.sh. This script creates the application database and a database user to run database migrations that are different from the default database postgres and the super user for security purposes.

The version of the database can be found in the subproject Dockerfile.

VRO achieves persistance through persistent Docker Volumes. The setup the volumes can be found in the application docker-compose file for local development and in the Helm's chart templates for deployment.

Database Migrations

VRO uses Flyway to initialize and change the database. The subproject db-init containts all database migration related artifacts. In particular all versioned migrations are SQL based and in the directory here. These migrations create all the schemas and tables. In addition an additional user with limited privileges is created. This user is used to access the database within all non migration VRO functionality.

The subproject db-init also contains a Dockerfile. The container based on this Dockerfile is used to run the migrations both in the local development docker-compose file and in application deployment.

The strategy for creating migration files is simple: Each work branch and subsequent Pull Request should be its own contained version number. Furthermore, you should create one migration file per proposed table change. Smaller, incremental changes on a per-file basis allows for better maintainability, troubleshooting, and testability.

Spring Data JPA

VRO uses Spring Data JPA to access and manage data in the database from the Java code. The subproject persistance/model contains the Object-relational mapping (ORM). In particular entity files that map to the database tables are in here.

To access and manage entity data , VRO uses JPA Repositories. All the JPA repositories are in here. These JPA Repositories inherit basic CRUD methods and also contains explicit definition of more complex methods needed by the VRO applications. In either case implementation of the methods are provided by Spring Data JPA automatically.

Service Interfaces and Implementations

VRO defines 3 service interfaces (Java Interfaces) in the service/spi subproject to populate and access the database. These interfaces are

Save to Db service: This service is used within the Camel routes to store information about claims, assessments, and evidence pdf generations.
Claim Metrics service: This service serves as the service later for the REST interface that exposes information in the database.
Audit Event service: This is also used within the Camel routes and logs various information about events that occur as claims are processed and external systems are called.

These services are implemented (Java Implementations of Interfaces) in the service/db subproject as Spring Boot services and are autowired in the rest of the projects. Implementations use JPA Repositories in the subproject persistance/model.

Usage within Camel Routes

The Save to Db service and Audit Event service are primarily used in Camel routes which are defined in the service/provider.

Usage within REST Interface

The Claim Metrics service is used in the implementation of claim-metrics and claim-info REST end points. These end points are defined in Claim Metrics resource and implemented in Claim Metrics controller.

Database Tables and Fields
General

The Entity Relationship Diagram (ERD) shows all the tables and fields used. All the tables reside in the claims schema. These are the tables in the database

claim: This table stores audit information about the incoming Claims themselves (as unique entities). The corresponding entity class is ClaimEntity.
claim_submission: This table stores audit information concerning each attempt to submit and process a Claim Submission through VRO. The corresponding entity class is ClaimSubmissionEntity.
veteran: This table stores audit information about the veterans in the claims. The corresponding entity class is VeteranEntity.
contention: This table stores audit information about the contentions in the claims. The corresponding entity class is ContentionEntity.
exam_order: This table stores audit information about the contentions in the Exam Order status. The corresponding entity class is ExamOrderEntity.
assessment_result: This table stores audit information about the assessmen results for the claims. The corresponding entity class is AssessmentResultEntity.
evidence_summary_document: This table stores audit information about the evidence documents created for the claims. The corresponding entity class is EvidenceSummaryDocumentEntity.
audit_event: This table stores log information as claim processing progresses through the Camel routes. The corresponding entity class is AuditEventEntity.
schema_history: This table stores migration version information and is used by Flyway database migrations. Database migration are described in Database Migrations

The tables claim, claim_submission, veteran, contention, exam_order, assessment_result and evidence_summary_document all have created_at and updated_at columns. These columns are inherited by the corresponding Entities from a BaseEntity. BaseEntity uses Spring Data JPA @CreatedAt and @LastModifiedDate annotations to implement the functionality. With these annotations Spring Data JPA automatically populates the fields without additional code in VRO.

For the same tables id column implementation is also shared in BaseEntity.

Details for Claim Table

The following are the column descriptions for the claim table.

vbms_id: This represents the VBMS system ID for the Claim. This is not intended as the source of truth and should be paired with submission_date to determine when the last known valid associated timestamp was.
veteran_id: Foreign key to the veteran table and identifies the Veteran the claim belongs to.
disability_action_type: i.e. INCREASE
presumptive_flag: Represents the Claim's presumptive status for fast-tracking
rfd_flag: Represents a Ready For Decision (RFD) state
Details for Claim Submission Table

The following are the column descriptions for the claim_submission table.

claim_id: Foreign-key to the corresponding ID in the claim table.
reference_id: Represents an external vendor's internal system ID for tracking the Claim. Used in addition to id_type to identify the source of the claim.
id_type: Represents the external source, or vendor, of the Claim. Used in addition to reference_id to identify the source of the claim. This was the constant va.gov-Form526Submission for V1. For V2 this constant is 'mas-Form526Submission'
incoming_status: Status of the incoming claim. This was the constant submission for V1.
submission_source: Taken from the claimSubmission.claimDetails.claimSubmissionDateTime initially sent by MAS
submission_date: Taken from the claimSubmission.claimDetails.claimSubmissionSource initially sent by MAS
off_ramp_reason: Explanation for why the claim was off-ramped.
in_scope: Boolean flag representing the claim is in scope of being processed. Set by VRO. Defined in #428 but potential duplicate of off_ramp_reason=outOfScope ‚Äî we might not need this anymore
Details for Veteran Table

The following are the column descriptions for the veteran table.

icn: The Internal Control Number (ICN) for the Veteran. It uniquely identifies the Veteran in VHA systems.
participant_id: The Participant Id for the Veteran. It uniquely identifies the Veteran in VBA systems and is actually the database ID in the CorpDb.
icn_timestamp: Since it is possible for ICNs to change, you can tell when the ICN was last updated with this timestamp. In theory, you could also use updated_at, but that column could possibly apply to other pieces of data here, so icn_timestamp provides a targeted ""last known good time"".
Details for Contention Table

The following are the column descriptions for the contention table.

claim_id: Foreign key that links the contention to the claim it is submitted with.
diagnostic_code: The diagnostic code for the contention. It links the contention to the VASRD codes.
condition_name: Name of the condition to be assessed
classification_code: Taken from the claimSubmission.claimDetails.classificationCode initially sent by MAS

Note that this design assumes multiple contentions per claim for future developments although iMVP will support only one contention (Hypertension) in single issue claims.

Details for Exam Order

The following are the column descriptions for the exam_order table.

claim_submission_id: Foreign key that links the Exam Order to the Claim Submission that issued it.
collection_id: Collection ID associated with the Claim Submission.
status: The current status of the Exam Order
ORDER_SUBMITTED: initial status from VRO, which creates a record when it issues a new Exam Order
VRO_NOTIFIED: TODO: Seek clarification on if this will continue to remain the initial submitted status follow-up from IBM/MAS
DRAFT: This was the initially assumed initial submitted status follow-up from IBM/MAS indicating the Exam Order is in Draft State
FINAL: status from IBM/MAS indicating the Exam Order has been completed
ordered_at: TODO: Change to status_at to represent the various statuses

A Claim Submission can have multiple Exam Orders.

Details for Assessment Result Table

The following are the column descriptions for the assessment_result table.

contention_id: Foreign key that links assessment result to contention.
evidence_count_summary: Summary of evidence counts for the assessments. This is a JSON objects that summarizes assessment and is provided by the assessment microservice.
sufficient_evidence_flag: Originally from #447. Represents that the Assessment Result has determined there is sufficient evidence to mark the claim as RFD.
Details for Evidence Summary Document Table

The following are the column descriptions for the evidence_summary_document table.

contention_id: Foreign key that links evidence summary document to contention.
evidence_count: Summary of evidence counts for the document. This is a JSON objects that summarizes the information shown in the document.
document_name: Name of the document generated.
folder_id: Represents the UUID of the folderId returned by BIP on PDF upload, in order to facilitate easier tracking down of the file in eFolder.
Details for Audit Event Table

This is the table structure for audit events:

event_id: A unique id identifying the request
route_id: The id of the camel route from which the event is issued. Example: ""mas-order-exam""
payload_type: The type of payload being processed. Example: ""Automated Claim""
details: Other details pertinent to the event, but specific to the type of processing. For Example, collectionId, offRampReason, presumptiveFlag, et cetera.
message: A descriptive message explaining the action. Example: ""Collecting evidence""
throwable: The stacktrace of an exception, if the even indicates an error.
event_time: Date and time the event was issued.

Simplified ER (detailed ERD can be found here: Entity Relationship Diagram (ERD)): "
VRO Database ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Database,"VRO's database does not store any Personal Identifiable Information (PII) or Personal Health Information (PHI).

Use non-PII identifiers for the Veteran
Use non-PHI summary information for contentions

The VRO platform only supports Postgres as a DB engine at this time.

Local Development

The postgres subproject builds the Docker container to serve as the database, which is useful for local testing. The postgres/Dockerfile uses the PostgresSQL image as a base image and runs the initialization script init_db.sh. This script creates a database user to run Flyway database migrations -- this user must be separate from the super user for security purposes.

Data is retained between container restarts through persistent Docker Volumes -- see the volumes configuration in docker-compose.yml.

Connect to DB

To connect to the local Postgres DB:

Start the VRO Platform Base: ./gradlew :dockerComposeUp (see Docker Compose)
Or start only the postgres container: docker compose up -d postgres-service db-init
Get the connection URI: echo postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:5432/vro
Connect using psql (if needed, install psql)
psql postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@localhost:5432/vro -c ""\dt claims.*""
                   List of relations
 Schema |         Name         | Type  |     Owner
--------+----------------------+-------+----------------
 claims | bie_contention_event | table | vro_admin_user
 claims | schema_history       | table | vro_admin_user
(2 rows)

To connect using a GUI, try DBeaver -- it provides an Entity Relationship Diagram (ERD) of the tables and columns.
Remote Environments

VRO uses Postgres on AWS RDS in all LHDI remote environments. The URLs for accessing RDS DB Instances are made available to applications as K8s secrets which are passed along as environment variables to applications through Helm.

A useful set of these environment variables can be referenced here. For any application seeking to use the DB, developers should ensure they include these definitions via symlink in their Helm chart deployment definition definition under a subdirectory named named_templates. Then in their deployment definitions they should set environment variables for the db client like in the xample-workflows deployment definition

Developers may use their PostgresSQL client of choice to connect to the database by querying these environment variables at application startup or runtime.

VRO Engineers

See the internal wiki for documentation on AWS entities created for the RDS integration.

These AWS entities are most easily managed by interacting with the LHDI kubernetes clusters (e.g. with the kubectl command or through the Lens GUI tool). They are specified using AWS controllers for Kubernetes.

Database Migrations

VRO uses Flyway to initialize and change the database. The db-init subproject contains all database migrations and is used to initialize and update the database schema.

All versioned migrations are SQL based and in the migrations directory. These migrations create all the schemas and tables.
An additional user with limited privileges is created. This user is used to access the database within all non-migration VRO functionality. A separate user can be created for each new domain.

The db-init/Dockerfile will run the migrations both in the local development and in LHDI deployments.

The strategy for creating migration files is simple:

Each work branch and subsequent Pull Request should be its own contained version number.
Create one migration file per proposed table change.
Smaller, incremental changes on a per-file basis allows for better maintainability, troubleshooting, and testability.

NOTE: At this time, migrations are only applied in remote environments on deployment of the VRO Application.

Database Tables and Fields

DB tables have created_at and updated_at columns. These columns are inherited by the corresponding Entities from a BaseEntity. BaseEntity uses Spring Data JPA @CreatedAt and @LastModifiedDate annotations to implement the functionality. With these annotations Spring Data JPA automatically populates the fields without additional code in VRO.

Similarly, the id UUID column is also included in BaseEntity.

All the tables reside in the claims schema. These are the tables in the database:

schema_history: stores DB migration version information and is used by Flyway for database migrations.
bie_contention_event: stores BIE Kafka events related to contentions
Accessing the DB via Java

VRO uses Spring Data JPA to access and manage data in the database from the Java code. The subproject shared/persistence-model contains the Object-relational mapping (ORM).

To access and manage entity data, VRO uses JPA Repositories. All the JPA repositories are in shared/persistence-model. These JPA Repositories inherit basic CRUD methods and also contains explicit definition of more complex methods needed by the VRO applications."
VRO Console ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Console,"To investigate and recover from errors particularly in the production environment, a VRO Console container was implemented and took inspiration from Rails Console.

The VRO Console container (or simply Console) facilitates diagnostics, such as examining the processing state of the claim by looking at Camel routes and the DB contents, and realtime updates to VRO's state in Redis and VRO's DB.

Enable Console Locally
export COMPOSE_PROFILES=debug -- the console container only starts if the docker-compose debug profile is enabled.
Start VRO. Note the vro-console-1 container; check out the logs: docker logs vro-console-1.
Attach to the container: docker attach vro-console-1

If you need to restart the console container:

cd app/src/docker
docker-compose up -d console
docker attach vro-console-1

Connect to deployed Console container

In order to connect to Console container deployed in LHDI, set up kubectl using the Lightkeeper tool.

Next to connect to the DEV deployment:

# For convenience
‚ùØ alias kc='kubectl -n va-abd-rrd-dev'

‚ùØ kc get pods
NAME                             READY   STATUS     RESTARTS   AGE
vro-api-645dc44c64-w95mw             0/6     Init:1/3   0          7m52s
vro-api-postgres-559c5bddbb-7rm2r    1/1     Running    0          7m53s
vro-api-rabbit-mq-74bd4c5bfc-lxb7v   1/1     Running    0          7m53s
vro-api-redis-555446854-jwfqg        1/1     Running    0          7m53s

# The console container is in the pod with several containers
‚ùØ kc exec -i -t vro-api-645dc44c64-w95mw -c abd-vro-console -- sh -c ""java -jar vro-console.jar""

For other deployment environment, adjust the kubectl namespace in the alias.

More details in PR #695.

Console Usage
Inspect DB contents

Added in PR #531.

On the groovy:000> prompt, try the following:
?  # display help
// Note the printJson (alias pj) custom command

:show variables
// Note the `claimsT` variable, which can be used to query the claims DB table

claimsT.findAll().collect{ it.claimSubmissionId }
c = getAnyClaim()
// Different ways to do the same thing:
printJson c
printJson getAnyClaim()
pj c
// exit   # This will stop the container
// Instead, press Ctrl-p Ctrl-q to detach from the container without stopping it
See https://groovy-lang.org/groovysh.html for other built-in console commands
Inspect Redis

Added in PR #614.

groovy:000> :show variables
// Note the redis and redisT variables

groovy:000> redis.keys ""*""
===> [claim-1234]
groovy:000> redis.hlen(""claim-1234"")
===> 2
groovy:000> redis.hkeys(""claim-1234"")
===> [""type"", ""pdf""]
groovy:000> redis.hget(""claim-1234"", ""type"")
===> hypertension
groovy:000> redis.hget(""claim-1234"", ""pdf"")
===> JVBERi0xLjQKMSAwIG9iago8PAovVGl0bGUgKP7/KQovQ3JlYX ... (truncated base64 encoding of the generated pdf)

// Using RedisTemplate redisT
groovy:000> ops=redisT.opsForValue()
groovy:000> ops.hget(""claim-1234"", ""type"")
===> hypertension
Wiretap Camel routes

Listen to messages at certain predefined wireTap Camel endpoints. Added in PR #597

groovy:000> :show variables
// Note the camel variable

// Check out http://localhost:15672/#/queues for current queues and see how `console-*` queues are added as the following commands are run.

// Initially no routes in the CamelContext of the Console container
groovy:000> camel.routes
===> []

groovy:000> wireTap claim-submitted
// Now, submit a claim using Swagger
// Expect to see a log message

groovy:000> wireTap generate-pdf
// Now, generate a pdf using Swagger
// Expect to see a log message

groovy:000> camel.routes
===> [
  Route[rabbitmq://tap-generate-pdf?exchangeType=topic&queue=console-generate-pdf -> null], 
  Route[rabbitmq://tap-claim-submitted?exchangeType=topic&queue=console-claim-submitted -> null]
]
Inject message into workflow

To submit a message from the VRO Console into a Camel Route endpoint:

// Create the request as a JSON String
req=""""""{
  ""resourceId"": ""123444"",
  ""diagnosticCode"": ""A""
}""""""

// Create the Camel endpoint URI
exchangeName=""v3""
routingKey=""postResource""
uri=""rabbitmq:"" + exchangeName + ""?skipQueueBind=true&routingKey="" + routingKey

// Inject the message -- see CamelEntry for alternatives ways to inject
resp=pt.requestBody(uri, req, String)
===> {""resourceId"":""123444"",""diagnosticCode"":""A"",""status"":""PROCESSING"",""reason"":null}

This requires that the Camel Route endpoint be exposed outside of the JVM, e.g., the endpoint uses rabbitmq: and not direct: or seda:. The VRO Console has access to RabbitMQ queues, not the internal JVM queues or endpoints.

Customizations
Add custom console commands

See the PrintJson and Wiretap classes.

Add custom function

Add to the console/.../groovysh.rc file."
VRO Architecture Diagram ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VRO-Architecture-Diagram,"VRO is deployed to Kubernetes clusters provided by LHDI.

Legend

circle: external service or API not controlled by us
Lighthouse APIs
BIP APIs
BGS API
BIE Kafka Event Stream
(oblique) parallelogram: Docker container
API Gateway
Docker containers
cylinder: data storage, such as a database, persistent volume, or temporary cache
VRO Database
Helm Charts
How to ...
Export diagram as image

To export the diagram as an image:

Click on the diagram. This opens the Mermaid diagram editor (https://mermaid.live/edit#...).
Click the ""Actions"" link near the bottom of the page, then click the PNG or SVG button.
Update diagram
Click on the diagram. This opens the Mermaid diagram editor (https://mermaid.live/edit#...), where you will:
Edit the markup code as desired -- see Mermaid tutorial.
When done, click the ""Actions"" link near the bottom of the page, then the ""Copy Markdown"" button.
Open this wiki page in a new browser window and click the ""Edit"" link.
Delete the diagram markdown text (which starts with [![](https://mermaid.ink/img/pako: and ends with )).
Paste over the new diagram markdown text, which you copied earlier. Optionally remove the ?type=png from the markdown text to make it render in GitHub's dark theme better.
Click the ""Preview"" tab.
If it looks correct, click the ""Save page"" button."
Virtual Regional Office (VRO) Platform Outline ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Virtual-Regional-Office-(VRO)-Platform-Outline,"Overview

The Virtual Regional Office (VRO) is a suite of products that contribute to making disability benefits claims submitted on va.gov maximally actionable and minimally burdensome for claims adjudicators, resulting in faster and more accurate claim processing and shorter wait times for Veterans to get a decision on their claims.

Within the VRO ecosystem, the VRO platform supports the teams building these products, by providing them with the data, systems access, authority, and infrastructure needed to quickly and effectively develop new product hypotheses.

Outcome Summary

Our Mission: The VRO platform exists to provide an environment where it's easy to build software to improve the VA's internal claims process.

Our Vision: Teams on the platform have access to shared knowledge, processes, and tools, allowing them to quickly build and validate ideas that improve the VA's internal claims process. As a platform team, our success is measured by time-to-value for our partners, ie. the product teams building and iterating on applications in VRO.

Related/Associated product(s)

The VRO platform team (#benefits-vro) currently supports these application teams:

Contention Classification (#benefits-cft-classification)
Employee Experience (#benefits-cft-employee-exp)
Problem

No place to build and test ideas to improve VA's internal claims process

As the OCTO Benefits Portfolio increases its efforts to impact the end-to-end disability benefits lifecycle beyond the Veteran-facing services provided on va.gov, teams with ideas to improve aspects of the VA's internal (employee-facing) claims process have not had a go-to place to quickly build and test those ideas. Prototypes could be shoe-horned into existing codebases, but would not have a chance to scale there.

VRO provides a dedicated space for these product opportunities, tailored to the needs of claims processing tools

Claims data visibility

Getting claims-related data often requires formalized data requests that can take weeks and still return the wrong information. In addition, teams need deep knowledge of the domain to frame a request properly, which dissuades teams from utilizing data for crucial decisions.

VRO aims to provide visibility into real time claim lifecycle events and va.gov claim submission data. (See VA.gov Data Visibility Initiative)

Coordination cost of integrating with VA systems

Claims processing tools rely heavily on interactions with multiple VA systems to retrieve and update relevant claims data. Integrating with these systems often requires extensive coordination across different parts of the organization with different ways of working, as well as navigating the technical and operational complexity of the given system ‚Äì draining time and energy that could otherwise be put into accelerated product development.

Products within the VRO product suite benefit from system integrations implemented and maintained by the VRO platform team, and made available to the product suite as microservices, including fully documented helper libraries to reduce developer discovery burden

Support for rapid, iterative product experimentation

Most OIT teams aren‚Äôt currently set up to support rapid software delivery to production in weeks, due to prescheduled six-week cycles. Even when the software is developed, it takes months to get approval to go to production.

The VRO platform provides a software development environment for partner product teams to deploy to the VA's Lighthouse Delivery Infrastructure (LHDI) platform, including build utilities, DevOps support, and ATO support.
The platform team provides processes and tools for releasing software in VRO, documentation & code from past experiments, best practices, and libraries for development, design, and research. Teams will be able to learn from & leverage the work others have done in the benefits space.
Desired Outcomes
Teams building applications in the VRO product suite can deploy an MVP to production in weeks, not months
Teams testing product ideas in VRO have the data they need to validate or invalidate their hypotheses in days or weeks, not months
Undesired Outcomes
Teams on the VRO platform take longer to validate ideas than if they were to use another platform.
Teams building on VRO independently work on redundant or duplicative efforts
Measuring Success
Current Objectives

Objective: Continue to mature the platform to support partner teams

Key Result: By December 31, 2023, VRO partner teams will have the ability to perform daily automated deployments to production, independent of the platform team.

Objective: Build Claims Data Visibility

Key Results:
By Oct 2023, VRO will have visibility to real time and historical contention history for all claims submitted.
By December 31, 2023, VRO will have visibility to all disability benefit form data submitted on VA.gov

Objective: Support the priorities of all teams on the claims fast tracking crew.

Key Result: By December 2023 at least 40% of partners indicate that they would be very disappointed if VRO were no longer available to them."
Virtual Regional Office (VRO) Overview ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Virtual-Regional-Office-(VRO)-Overview,"Document Purpose: To provide a catalog of VRO offerings along with links to more detailed documentation. This page will be updated regularly to ensure it contains the most current information.

Welcome to the VRO software platform! The VRO team is here to support you as a new partner product team getting up and running in VRO.

üëã Who is the VRO team?

We are the software platform team for the OCTO Benefits Portfolio, and we support partner teams building products that improve claim processing for digitally submitted (ie. va.gov) claims. Together, these products form the 'Virtual Regional Office' product suite.

We work to ensure that teams building in VRO have access to shared knowledge, processes, and tools, allowing you to quickly build and validate ideas that improve the VA's employee-facing claims process.

Chat with us and follow along in DSVA Slack: #benefits-vro-support.

Meet the team:

OCTO (VA) Enablement Team
Zach Goldfine, Product Owner
Julie Strothman, Design Lead
Cory Sohrakoff, Engineering Lead
Product & Research
Diana Griffin, Product Manager
Bianca Rivera Alvelo, Designer/Researcher
Software Engineers
Cheng Yin
Erik Nelsestuen
Josiah Jones
Teja Naraparaju
Tom Greene
DevOps Engineers
Mason Watson
üõ† VRO Features, Tools, and Support

VRO provides a software development environment for Claims Fast-Tracking Crew teams to quickly integrate with existing VA services and deploy to the VA's Lighthouse Delivery Infrastructure (LHDI) platform. In service of this goal, we offer key features relevant to claims processing products, tools to expedite your development process, and the support, guidance, and subject matter expertise of our team members.

üå± VRO is evolving! VRO is a new software platform, and will continue to grow and mature alongside our partner products. We collaborate with product teams to understand your application's needs and evolve VRO services and tools to meet them while incorporating them with the holistic needs of the product suite.

Features
Event-based, scalable microservice architecture

VRO implements an event-driven architecture with Queue-Processor components that act like an internal microservice, modularizing functionalities so that each can be updated and maintained more easily. These components are connected together using well-tested and stable Enterprise Integration Patterns (EIP) tools (such as Apache Camel) so that we can focus on VRO functionality and less on ‚Äúglue code‚Äù. This approach promotes low software coupling and, as a result, simplifies debugging and maintenance.

Learn more
Reusable software patterns

To help expedite time-to-deployment and maintain consistency between products, VRO provides reusable software patterns that implement VRO Software Conventions. VRO offers libraries and encourages reuse of software patterns to minimize onboarding, diagnosing, and development time.

VA system integrations

Claims Fast-Tracking products need to access and update claim data in order to deliver desired outcomes, which requires integrating with other VA systems. VRO offers a growing number of integrations with VA systems (such as Lighthouse APIs, BIP APIs, and BGS) and other services (such as Slack). Leveraging VRO's existing integration services could save many weeks (if not months) of work per integration.

Learn more
Database and Redis cache

VRO provides a dedicated Postgres database for persisting (non-PII and non-PHI) data, as well as a Redis cache to temporarily store and track data.

API gateway

VRO's API Gateway provides a single location to access all APIs provided within VRO, regardless of implementation language. To expose APIs for each domain, the API Gateway offers a Swagger UI to inspect API offerings, retrieving the OpenAPI spec for the selected API from domain-specific containers and presenting it in a Swagger UI.

Learn more
Tools
Established development process

VRO partner teams can get up and running quickly using our established development process with Gradle utilities and Github Actions workflows to automate code testing, Docker image creation, and housekeeping tasks.

Learn more about:

VRO development process
Gradle utilities
Github Actions
Flexible deployment configurations

For teams building in VRO, flexible deployment configurations and processes ease deployment under the VA's cATO (continuous authority to operate). The build and deployment pipeline incorporates the required Secure Release process and scanning, which minimizes software vulnerabilities and keeps the software up-to-date.

Learn more about:

Helm Charts for deployment configuration
Deployment process
CI CD Workflows
Secure Release (SecRel) process
Dependabot
Support
Software conventions

The VRO team develops, maintains, and expands our Software Conventions to help partner teams jump start their development, reduce time-to-deployment, and maintain consistency across products in the VRO product suite.

Inherited Continuous Authority to Operate (cATO)

Traditional ATO processes can add weeks or months of coordination and overhead before an initial product launch, but VRO benefits from Lighthouse Delivery Infrastructure's continuous ATO (cATO) process to stay in compliance while being able to deploy frequently to production. Products within the VRO product suite benefit further by inheriting VRO's ATO -- we incorporate your product scope into our cATO process so that you don't have to start from scratch.

Hands-on support and collaboration

The VRO team is here to support our partner product teams, from initial idea validation through development and launch, and with ongoing support and maintenance of a stable production environment where your product can grow and scale. We work iteratively to support new requirements and improve the maintainability and sustainability of the ABD-VRO codebase, with built-in DevOps to ensure VRO is operational and scaling as needed and team members available to help onboard partner teams, review Pull Requests, and offer technical support, product guidance, and design standards."
VA.gov Data Visibility Initiative ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/VA.gov-Data-Visibility-Initiative,"Problem Overview
Executive Summary
Problem
Consequence
Opportunity
Goal
VRO's role
Enabling team Q&A
Executive Summary
VA.gov activity data, including disability benefits claim submission data, is functionally inaccessible to Benefits Portfolio product teams, with the exception of a handful of engineers with command line access to query the production database in vets-api.
OCTO wants to develop a safer, more accessible, and more user-friendly way for teams to access this data.
The VRO team is responsible for coordinating this effort via collaboration across the Benefits Portfolio, in particular with the Disability Benefits Experience team(s) who are familiar with the va.gov Postgres database and the needs of engineers working on va.gov benefits products.
Problem
VA.gov activity data is currently trapped in a Postgres database in vets-api

Teams working to improve the end-to-end experience of digitally submitted disability benefit claims need access to va.gov activity data (including claim submission data) in order to learn about problems, validate ideas, troubleshoot issues, measure experiments, and iterate on solutions. However, teams can't easily access this data from the database where it's currently stored.

How is va.gov claim submission data trapped in this Postgres DB?

VA.gov stores all activity in a Postgres database that can only be queried via prod Rails console, and the data can‚Äôt be used by BI tools/etc. to interact with the data. Due to the nature of some of the information (PII data) it is often not able to be logged.
Only a handful of Benefits portfolio team members have access to query the production database via the prod Rails console.

Why not just get the data from the VBA side?

While va.gov claim submissions end up in the Enterprise Data Warehouse (EDW, which sits one layer above the source-of-truth Corporate Data Warehouse, CDW) after claim establishment, all of EDW's claims that come from va.gov are slightly mislabeled when it comes to identifying va.gov as their source. While this mislabeling problem is currently being investigated in hopes of resolving it point forward (outside the scope of this effort), the problem will still apply to historical claims data (since Nov 2021) in EDW.
Even disregarding that EDW data is slightly mislabeled, no one in OCTO has access to EDW; queries must go through VBA's PA&I team

What about getting the data via Kafka streams? The current thinking is that in the long-term, the ideal would be for the VES Event Bus kafka service to act as the source for this data, however, OCTO would like to implement an intermediary solution without waiting for this option to solidify.

Consequences
Va.gov activity data in its current state is functionally inaccessible for the majority of Benefits Portfolio teams' needs.

What's wrong with getting this data from the Postgres DB?

Having engineers going into the prod Rails console and querying for data risks potentially impacting actual production and end-users, or altering real production data
It's hard to look at large amounts of this data, because of the risk of overloading the actual production system (due to size of the queries)
While only a small number of Benefits Portfolio engineers can access this data (Yang, Luke, Steve, and Kyle Soskin are the ones we're aware of), increasing that number would violate the principle of least privilege
The folks who have access to query the database are not in support roles tasked with fielding requests for data, so there's no official way to ask for a data pull

What's wrong with requesting this data from PA&I?

Given that PA&I fields requests from all across VBA and OCTO, it can take weeks or months for data requests to be fulfilled.
Furthermore, because all of VBA's reporting is slightly mislabeled when it comes to va.gov as a claim source, it's not possible for PA&I to report on claims from va.gov with full confidence of accuracy.
Opportunity
OCTO needs a way for Benefits Portfolio teams to retrieve va.gov activity data in a safer, more accessible, and more user-friendly way:
without using a prod command line to access the va.gov Postgres database,
without lengthy turnaround times on the scale of weeks or months,
and with confidence that the data we're seeing covers all va.gov 526 claims.

Ideally, non-engineers who need visibility into the data will be able to retrieve it for themselves without having to go through an engineer. Building upon that ideal scenario, we can imagine enabling the configuration of data dashboards to meet teams' specific and ever-present needs for data analysis and insights. And in a perfect world, this va.gov activity data would be matched up to ""downstream"" claim lifecycle data from EDW (available via kafka event topics) so that teams could follow claims from submission on va.gov through to claim completion in VBMS.

Note that the VA has a larger effort underway related to reducing/eliminating va.gov engineers' dependencies on interacting with the prod Rails console (as shared by Bill Chapman in the July Benefits Portfolio engineering all-hands) ‚Äì our focus on data visibility represents just one aspect of this overall effort.

Goal
By December 31, 2023, Benefits Portfolio teams will have visibility to all disability benefit form data submitted on VA.gov.

More context:

The whole benefits portfolio should be part of discovery of needs even if we choose to implement early solutions that focus on a particular team or crew
At minimum, ""visibility"" = Benefits portfolio engineers can pull data via a secure solution (e.g. read-only credentials, scoped only to claim data) that doesn't require them having prod console access to the va.gov Postgres DB.
Currently, ""all disability benefit form data"" is a hypothesis about what will be valuable to the portfolio teams. For now, we can assume that we're talking about 526EZ form submission data (including historical submission data), but we will refine expectations of what data to include based on further discovery that defines and prioritizes data visibility needs across Benefits Portfolio teams. Other types of data that may be prioritized include va.gov activity and error data.
VRO's Role
Given VRO's mission to make it easy to build software to improve the VA's internal claims process, with particular emphasis on our vision of allowing teams to quickly build and validate product ideas, the VRO team is well positioned to lead the effort of identifying a pathway to deliver value in this problem space.

Our VA partners are asking the VRO team to:

Be responsible for coordinating this work. If VRO's research or roadmap requires work or input from other teams (such as DBEx), that's totally fine.

Expectations:
VRO should make it as easy as possible for other teams to stay informed and complete relevant tasks.
This work should (as always!) follow OCTO's principle of working in the open. All chatter about this project should be in open channels for folks across the portfolio (i.e. #benefits-vro-support #benefits-portfolio #benefits-cft or in a place that‚Äôs new / opt-in / not 1:1 DMs).

Collaborate with teams across the Benefits Portfolio to define and prioritize the needs related to visibility of va.gov data. There are a variety of needs related to claim data across the portfolio, both at submission and beyond. Some are related to monitoring in the moment, and some are more driven by product/design, researching historical data. The Disability Benefits Experience (DBEx) teams, given their knowledge of the va.gov database, should be primary collaborators on this effort.

Required output:
Recommend a prioritized set of needs as initial and subsequent areas of focus for the teams' efforts between now and the end of the year
Expectations:
Set up a touchpoint/meeting between VRO and DBEx team (or teams, but likely starting with DBEx Team 2) by first week of August
Build out a comprehensive list of needs (and their relative priority/frequency)** through end of August and use that list to define our roadmap.

Collaborate with DBEx on shaping and scoping solutions to the portfolio's prioritized needs.

Required output:
Recommend a roadmap to deliver on the priority needs, including determining which team will implement which portions of which solutions (assuming the solutions include elements that span va.gov and VRO).
Expectations:
VRO should aim to implement the solution as far as possible, reducing dependency on DBex or any other teams if possible.

Implement against the agreed upon roadmap.

Expectations:
An MVP solution should be started by ~ end of August
A note about the long-term

Our product owners recognize that a ""productized"" version of available data via dashboards and other tooling is a product in itself, somewhat separate from VRO as a platform. We are empowered to recommend the best structure for long-term maintenance and expansion of this work stream, however, the expectation is that VRO will lead the initial shaping and roadmapping of solutions to this set of problems, identify a path to quick value, and deliver on it.

Enabling Team Q&A

Q: Who are the ultimate decision-makers about what can/can't be done with the va.gov data and what can/can't be built on the va.gov side? (We know it's all OCTO; is there anyone we don't know yet who's a key decision-maker?)

A: Need to loop in/keep aligned with the ATO team for Lighthouse and va.gov (Jesse House has been the person Steve has talked to; #platform-security-review is the team's slack channel). Do the same with VRO's cATO contacts.

Q: Is it accurate to think of this effort as addressing one slice of the overall ""Steve and Bill idea"" (ie. Steve Albers & Bill Chapman's exploration of internal APIs and/or other solutions to reduce/eliminate dependencies on accessing the va.gov database via prod console)?

A: Short answer, no. They're related problems but we don't want to create dependencies between these efforts.

Q: Technical question: is there an existing backup of the va.gov database?

A: No. It's hard. System written in almost NoSQL fashion... it's not trivial to extract data from the DB; have to decrypt. Would be potentially a performance impact because it needs to run in the same space as production.

Q: For Steve & Cory: How baked are your ideas on where to start (e.g. replicate data to an S3 bucket)? We don't want to go back to square one if you're already feeling confident that there's an obvious first step we should take.

A: Kyle Soskin is writing up a best practices guide on using Sidekiq for backend queries and we should wait for that before making decisions. There's a backlog ticket for CE teams (DBEx 2) to do this monthly data extraction -- but maybe this is a short-term thing, we might only want it once! It makes sense for them to own it, but in terms of capacity it might make sense for VRO to build it, if we can get to it sooner.

Q: When we talk about access as a problem, is [technical] skill part of the problem? For example, could we assume that everyone who needs access to this data can use SQL?

A: Maybe for an MVP but since part of the problem is making the data accessible beyond engineers, probably can't assume e.g. all PMs are SQL fluent.

Q: What would overthinking this look like?

A: Don't over-engineer in the beginning. For example, if we felt like a JSON file or something would be insufficient and assumed we need complex data viz to solve the need. We CAN start small!
We don't need all data in real-time. Figuring out which data is part of the question for us! Picturing a list that lays out, ""We need this, this often, and this is the person who needs it"" -- then prioritize this list.
There's also underthinking it! Doing a data pull, dropping it on Sharepoint and calling it done is not taking holistic enough view of the problem. There are many needs!
A good deliverable would be laying out what we can/should do now with sidekiq and which things should wait until data is available from EventBus.
We're excited about the potential linkage between claim submission data and downstream claim lifecycle data (but don't start there!)"
Updating Contention Classification DC Lookup Table ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Updating-Contention-Classification-DC-Lookup-Table,"Getting the lookup table updated
Submitting Updates
Please make updates to the lookup table in a separate PR without any additional changes to application logic!
Ensure the title of the PR indicates that the lookup table is being updated and include the version number
this will make sure that users can see the lookup table updates from the releases section on github!
Spreadsheet Versioning
Currently we store the mappings in a spreadsheet HERE.
There are different sheets available for different versions.
Locking

Make sure the latest version intended for release has been ""locked"" so no further edits can be made to the sheet. 

Export the desired sheet as .csv

Github

Make sure the .csv filename follows the file format

""Contention Classification Diagnostic Codes Lookup table master sheet - DC Lookup {{ TABLE_VERSION }}.csv""

Add the new .csv file to domain-cc/cc-app/src/python_src/util/data

Update table_version.py to include the updated version number

If the CI passes, you should be good-to-go! Have a great day üí´"
Update vets api parameters for VRO ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Update-vets-api-parameters-for-VRO,"For VRO's MVP (where the va.gov backend calls the VRO API), vets-api proxies requests through to VRO. There are entries in AWS Parameter Store in the dsvagovcloud AWS account that influence how that integration works.

The relevant params are:

/dsva-vagov/vets-api/dev/vro/api_key
/dsva-vagov/vets-api/sandbox/vro/api_key
/dsva-vagov/vets-api/sandbox/vro/url
/dsva-vagov/vets-api/staging/vro/api_key
/dsva-vagov/vets-api/staging/vro/url
/dsva-vagov/vets-api/prod/vro/api_key
/dsva-vagov/vets-api/prod/vro/url
Access

On our team, currently @cweagans has access to update those params. To get access to make these updates yourself:

Attend a platform onboarding session.
Fill out this form. For ""Desired AWS Access"", it should be something along the lines of ""I need to create/update/delete the following parameter store values"" (and paste in the params from above)."
Ubuntu VM Setup ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Ubuntu-VM-Setup,"This guide is a supplement to Local Setup for getting a development environment running on a Ubuntu Linux VM.

Rather than getting a development environment working on Windows or WSL2, you may find it easier to work within a virtual machine. The following steps will guide you through requesting and setting up VirtualBox on you VA laptop.

Note: While connected to the VA VPN, some HTTPS requests are intercepted, and the network presents its own certificate. This causes connection failures within the virtual machine. A simple solution is to disconnect from VPN when network issues arise.

Requesting VirtualBox
Navigate to yourIT and select the ""Need Something New? Submit a Request"" option.
Choose ""IT Software or Hardware Request"" and fill the request form.
For ""Non-Medical Software Details"" enter the following as the request for VirtualBox 7.xx (or the latest version): Please install VirtualBox 7.xx software. https://trm.oit.va.gov/ToolPage.aspx?tid=6679, https://www.virtualbox.org/wiki/Downloads
For ""Justification"" enter the following: VirtualBox is required to conduct official VA business, specifically it facilitates software development for VA.gov, VRO, and other OCTO efforts.
After filling the request form, click the ""Order Now"" button.
IT will contact you in order to remote into your laptop and install the software. It may take several days for them to service your request.
Installing Ubuntu 22.04
Download the Ubuntu 22.04 ISO from here
From VirtualBox, select the ""New"" button.
Name your VM and select the ISO Image before moving to the next screen.
VirtualBox has an unattended installation feature that will install and configure the guest OS for you. On the screen, you may setup a username, password, and hostname for your VM. You may also want to install Guest Additions if given the option.
Select a reasonable amount of memory for the VM. (For example, I selected 8192 MB for a host laptop with 16 GB of memory.)
Select a reasonable amount of processors for the VM. (For example, I selected 4 for a host laptop with 8 CPUs.)
On the disk screen, allocate a virtual hard disk with enough space for development. (I created an 80 GB disk.)
On the final screen select the Finish button. VirtualBox will install and configure the guest OS for you.
After the installation is finished, you will want to ensure that Enable Nested VT-x/AMD-V is selected in your VM. Go to Settings > System > Processor and select the approprate checkbox.
Start the VM and log in.
Your user cannot use sudo, but the root account is conifigured with the same password as your username. To add your user to the sudo group, from a terminal type su and enter your password. Then, usermod -aG sudo <your username here>. Type exit to leave the root shell. Afterward, you may need to log out and log in again for sudo to work.
Development Prerequisites (adapted from LHDI Developing on Non-Mac OS)
Git

Install git:

sudo apt install git


Configure your name and email for git:

git config --global user.name ""FIRST_NAME LAST_NAME""
git config --global user.email ""MY_NAME@example.com""

VS Code (optional)

Don't install VS Code from the Ubuntu Software app. There are some issues with it that cause problems later with Gradle. Download the Linux x64 .deb file from here instead. Install it with:

sudo apt install <VS Code .deb file>

Java
sudo apt install openjdk-17-jdk

Python Setuptools
pip3 install --upgrade pip setuptools

Docker

Follow the official instructions from Docker:

Install Docker Engine on Ubuntu | Install using the apt repository
Linux post-installation steps for Docker Engine

Docker plugins (such as docker-compose) are not put into the PATH by default, you can link docker-compose to /usr/local/bin to make it easier. You can do similar for other plugins as needed.

sudo ln -s /usr/libexec/docker/cli-plugins/docker-compose /usr/local/bin/

Hadolint
Download the latest hadolint-Linux-x86_64 release.
Rename the file hadolint.
chmod +x hadolint
sudo mv hadolint /usr/local/bin
Spectral

Install the binary following the official instructions with this command:

curl -L https://raw.github.com/stoplightio/spectral/master/scripts/install.sh | sudo sh

Shellcheck
sudo apt install shellcheck

Pre-commit
sudo apt install pre-commit
Run pre-commit install from your abd-vro project to install the git hook.
Go

Go is required by some pre-commit hooks. Install it following the Linux instructions here. However, you will need to prefix both commands in step 1 with sudo for it to work properly.

Adr-tools
sudo apt install adr-tools

Kubectl

Follow the offical Install using native package management instructions.

Helm

Follow the From Script offical installation instructions.

Istioctl

Follow the download instructions here. Though in addition to step 2, you can permanently add istioctl to your path with the following command:

echo 'export PATH=$HOME/.istioctl/bin:$PATH' >> ~/.profile

Minikube

Follow the offical installtion steps for Linux here

Codeclimate

Follow the Anywhere installation steps. (This requires Docker to have already been installed.

Mkdocs
sudo apt install mkdocs

Scala

Follow the Linux x86-64 instructions here.

LHDI Lightkeeper

From the offical installation instructions:

Install jq, which is a dependency of the installer script: sudo apt install jq
curl https://$GITHUB_ACCESS_TOKEN@raw.githubusercontent.com/department-of-veterans-affairs/lighthouse-di-lightkeeper/main/install.sh | bash -s $GITHUB_ACCESS_TOKEN

Note: For this step to work, GITHUB_ACCESS_TOKEN must be configured as described in Local Setup."
Troubleshooting Local Environment ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Troubleshooting-Local-Environment,"This troubleshooting page is created to address some common issues for local environment build
Failing at build without any issues present: In order to build successfully make sure you have the latest code as well as abd-vro-dev-secrets and run scripts/setenv. Since some of the environment variables need to be overridden, it's advised to open a new terminal and run the build in the new terminal (setnev.sh does not override)."
Software Conventions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Software-Conventions,"VRO Software Conventions

VRO is opinionated about the technical architecture (i.e., event-driven and Apache Camel-based).

This page describes software conventions that should guide new development in VRO to help maintain consistency and expedite time-to-deployment, not to mention to evoke a great developer experience.

Background

VRO has developed an MVP (in Dec 2022) and iMVP (in Feb 2023). The keyword is ""MVP"". VRO needs some TLC:

The code works but should not necessarily be used as the basis for new code.
New teams/engineers will inevitably copy-and-paste to develop new functionality.
VRO code should follow best practice; follow conventions and be consistent (which improves maintainability); be modular with minimal coupling (to help ensure components don't inadvertently affect each other as new features are added); and be scalable to handle high volumes.

So I propose:

(Mostly done) We document instructions, set coding conventions, and/or incorporate working example code into the VRO codebase.
(In progress) We refactor code for reuse, maintenance, and modularity while keeping existing VRO capabilities functional. We can leverage Camel routes to fork a secondary path that runs refactored code in parallel with existing code, and then compare the results. Once results are the same for many claims, then we can be confident that our refactored code can replace the existing code.

Other context:

VRO is one of the first to use the LHDI platform, LH SecRel pipeline, and LH cATO process, which are gradually being developed, expanded, and matured.
Contributions to and management of the VRO codebase will change hands over time.
Consistent code implies easier debugging and maintenance across domains. Coding conventions help to ensure consistent code.
Terminology
""domain"" = area of functionality to be added to VRO, such as va.gov integration (VRO v1), MAS integration (VRO v2), or Contention Classification (CC)
""MQ"" = message queue, such as RabbitMQ, provides a message bus, enabling communication across all containers
""container"" = a Docker container is used to encapsulate functionality into modular, scalable, stateless components
VRO's container categories:
App container: there's a default App container that exposes an API using Java Spring Web and combines Java-based domain-specific API endpoints into a single API
A domain can expose its own API but some manual updates will be needed to expose the API via the API Gateway
Workflows container: defines domain-specific workflows using Camel Routes (in Java) or Prefect in Python; a domain should have its own Workflows container; typically a single Workflows container is needed per domain regardless of the number of workflows.
The workflow library should be able to send requests to Service containers using the message queue.
Service container: holds one or more microservices that implement (in any language) step(s) in a workflow, providing some basic, stateless functionality. A multi-step workflow is expected to use multiple service containers. These services are typically domain-specific but they can be generalized to be domain-independent and reusable by multiple domains.
Platform container: offers a domain-independent resource or service, such as containers that run the RabbitMQ, Postgres DB, and Redis Cache services.
A container may be composed of several VRO Gradle modules (or subprojects).
A Gradle module is a folder with a build.gradle file. A module is used as a library or for a container (which may import libraries). A container module uses a container.*-conventions Gradle plugin (search for id .*container.*-conventions in build.gradle files to identify all the container modules).

Example of VRO Container Categories:

Folder Structure

Top-level folders are for domain-independent code, except for domain-... folders where domain-specific code resides. VRO software resides in these folders:

app: VRO entrypoint; pulls in domain-specific api-controller modules
console: VRO shell console for diagnostics (inspired by Rails Console)
db-init: initializes and updates the database schema
postgres: defines a customized database container
shared: utility and library code shared across domains (Shared libraries)
svc-*: domain-independent microservices, typically for integrations (Shared microservices)
mocks: mock services for development and testing, typically mocking external APIs
domain-ABC: domain-specific code with subfolders:
ABC-api-controller: module that defines domain API endpoints and controller
ABC-workflows: defines domain workflows (i.e., Camel Routes)
svc-*: domain microservices supporting the workflows

Other top-level folders contain configurations for building and deploying VRO.

Shared libraries

VRO offers utilities and DB classes as shared libraries for use by domain-specific classes.

shared/api folder: (Java) general API models
shared/controller folder: (Java) Controller utilities, including InputSanitizerAdvice
shared/lib-camel-connector folder: (Java) Camel utilities, including a Spring Configuration for Camel and CamelEntry
shared/persistence-model folder: DB utilities, including Java Entities
Python MQ utilities to facilitate interaction with the MQ

These libraries should have minimal dependencies to help avoid dependency conflicts (aka ""dependency hell"").

Be cautious when updating these files since they can inadvertently affect multiple domains that use the libraries. (Versioning the libraries can help avoid this scenario but adds significant complexity and maintenance overhead.)

Shared microservices

VRO offers integrations with external services (such as Lighthouse API and BIP API) via shared microservices, which are deployed in separate containers (for independent updating, down-time handling, and scalability). These domain-independent shared microservices are in the top-level svc-... folders. Other (non-integration) cross-domain microservices can be added as long as they are domain-independent.

Note that external services can go down, so the domain's workflow should incorporate error-handling and retry mechanisms for workflow robustness. Do not rely on RabbitMQ-specific retry mechanisms (RabbitMQ Microservice reliability or any MQ-specific features) in case VRO moves to using some other MQ; instead, use Camel EIPs to do retries as part of the workflow.

Adding a New Domain

To enable capabilities for a new domain in VRO, the partner team will typically implement a Workflows container and several Service containers, as well as adding API endpoints by creating an API-Controller module.

Code for one domain should not reference code in another domain. Keep domains decoupled, including keeping containers for different domains independent of each other. This allows containers for one domain to be restarted without affecting unrelated containers.
The API-Controller module should rarely need to be updated once created, whereas the Workflows and Service containers are restarted more frequently.
Add a domain-... folder at the top level of VRO's codebase. All code for the domain should reside in subfolders under this domain folder. For an example, refer to the domain-xample folder.
Domains should include automated tests that validate the workflow; end2end tests. These tests can be called manually in the local developer environment, and automatically during CI by GitHub actions.
API Endpoints and Controller library module

Under the domain folder, add a ...-api-controller subfolders and populate them with Java code. Because the folder name is used as the jar artifact file name, the folder name should be unique, regardless of where it is in the codebase.

The app container module (in the top-level app folder) pulls in each domain's api-controller module to present a single API spec.
Note that API endpoints should rarely be deprecated. Instead, use version numbers in the URL. https://www.mnot.net/blog/2012/12/04/api-evolution
The controllers should be very minimal in terms of logic. It should check the request payload and immediately inject it into a workflow. This enables error-recovery and testing, where the payload can be injected directly into the workflow, bypassing the need for an API and Controller.

APIs implemented in non-Java languages are served by VRO without having to implement a ...-api-controller subfolder -- see API Gateway.

Shared library module

Domain-specific shared libraries can be implemented in a single subfolder or multiple subfolders. You can use any folder name, such as constants, util, or dto. These subfolders define library modules that can be imported by other modules in the domain.

In the domain-xample example, the shared module illustrates defining shared domain-specific constants, DTO (data transfer object) classes, and utility classes.
Note that DAO (data access objects) belong in the top-level persistence folder because all domains share a DB ‚Äì see the Add DB Entities section.
Once the code is stable, domain-independent classes can be moved to one of the shared libraries for use by other domains. Ensure these classes are very well tested with unit tests and clearly documented.
Workflows container module

Workflows are implemented within a single ...-workflows subfolder. Because the folder name is used as the Docker image name, the folder name for the container should be unique, regardless of where it is in the codebase.

For Java, workflows are defined using Camel Routes and Camel Components, which are building blocks for implementing EIPs.
The Workflows container should implement only Camel Routes and basic processing (workflow logic or data object conversions), not the microservices themselves.
However for rapid prototyping, early implementations can include microservices in the single container, with the understanding that the microservices will later be extracted into their own containers (as described in the next section).
A workflow step can call a method on a Java Bean, a custom Camel Processor, etc. to perform basic logic to determine the next processing step. Nothing (except maybe a rejection from a manual code review) prevents that component from being big and complex. To avoid this situation, split the workflow into simple steps, which will facilitate extracting a step into an appropriate service container later. In the long-term, this will make the workflow easier to understand (workflow logic and transmitted data is encoded at the workflow level), comprehensive (because hidden substeps become an explicit step), and easier to modify (by modifying routes instead of service internals).
Workflows should split the processing steps into logical stages, where each stage is one or more Camel Routes. The benefit of segmenting the workflow is to facilitate testing and error recovery, where for example a payload (a claim) can be injected into any stage of the workflow.
Prefer to design workflows for asynchronous (event-based) processing, leveraging MQ queues to trigger the next step in the workflow. In addition to decoupling workflow steps, this facilitates being able to (manually or as part of another workflow) inject a payload in strategic intermediate steps within the workflow.
For Python, Prefect library can be used instead of Camel. To send requests to Service containers, Prefect tasks should send the request over the MQ.
Service (microservice) container module

A microservice is implemented within a svc-... subfolder. There should be a separate folder for each microservice. Because the folder name is used as the Docker image name, the folder name for the service should be unique, regardless of where it is in the codebase.

For scalability, a microservice is encapsulated in a container so they can be replicated as needed.
Microservices can be easily scaled by having replicated microservices listen to the same MQ queue; more effort is required to scale using a REST API
queue-based asynchronous model vs REST API
A microservice should be stateless, idempotent, and basic (e.g., implementing one step in a workflow).
A microservice listens for JSON requests on the MQ and always sends a JSON response back to the client via the MQ. Benefits:
No REST API to set up and manage; less DevOps maintenance (e.g., exposing ports for REST API). Fewer libraries to include implies fewer libraries to maintain for SecRel.
Makes services easier to test since the input and output are always JSON strings.
JSON String are easily parsed in any programming language. There is no JSON data structure enforcement unless a JSON Schema is specified and applied.
A microservice response is always expected by the client to ensure the request was received and processed (or errored). The client can handle the response synchronously (blocks the workflow while waiting for a response) or asynchronously (workflow continues and reacts whenever a response is received).
https://developer.ibm.com/articles/how-messaging-simplifies-strengthens-microservice-applications/: decoupled communication, pull instead of push workload management, simplified error handling, security is configured in central MQ instead of potentially inconsistently in each microservice
The expected JSON structure for a microservice response is very flexible. The only requirement is a header field:
{
  ""someRequestCorrelationId"": 987654,
  ""anyField"": { ""someSubField"": ""anyValue"" },
  ""anotherField"": 1234.5,
  ""header"": {
    ""statusCode"": 200, // use HTTP status codes
    ""statusMessage"": ""error msg or optional msg useful for debugging/observability"",
    ""jsonSchemaName"": ""optional domain-specific JSON Schema name for msg body"",
  }
}
To convey an error or exception to the microservice client, the response JSON String should contain the fields statusCode and statusMessage.
The statusCode integer value should correspond to an HTTP status code. If this key is not present, then the client can assume status code = 200.
The statusMessage String value should convey some diagnostics and preferably actionable information. This should be present for a non-success statusCode to provide details for diagnostics.
Implement business logic in the Workflow container.
Do not rely on RabbitMQ-specific retry mechanisms (or any MQ-specific features) in case VRO moves to using some other MQ. Handle retries as part of the Workflow, especially since the retry strategy (e.g., constant-time retry 3 times, exponential backoff, send Slack notification for each retry) will likely depend on the domain. Check the shared libraries or domain-specific code for implemented retry strategies that can be reused.
A microservice should not call another microservice ‚Äì implement that as steps in the workflow when possible.
Except for communication via the MQ container, a microservice should avoid interacting directly with Platform containers. If DB values are needed, have the Workflow load it from the DB and feed that as input to the microservice. If the service needs to write to the DB, have the service output the data back to the Workflow container, which would write to the DB. This facilitates unit testing (so that a mock DB is not needed) and flexibility (e.g., output data can be sent to logs in addition to the DB).
Integrations with external APIs (VA services) should be implemented as a microservice in order to better isolate and handle external API problems. Additionally it should be implemented in a general manner in order to promote it to a shared microservice.
The microservice should indefinitely retry connecting to external services rather than exiting and causing the container to fail. Otherwise, when the microservice is deployed to LHDI (where mock services don't exist), the microservice will fail and Kubernetes will keep retrying to start the container. If the microservice loops indefinitely retrying to connect to the external service, this would avoid Kubernetes from unnecessarily restarting the container since the problem is the connection, not the microservice container itself.
If there is a temporary network or external service issue, the microservice container should not be restarted and interrupt/cancel other activities/processing occurring in the microservice.
Another reason to keep the container running is to enable diagnostics within the running container in case there is some credential or login issue when deployed to LHDI. If the container keeps restarting, it's challenging to log into the container to diagnose the problem.
If using Java Spring to connect to the external service, you get this for free because Spring will automatically retry connecting.
Ideally there is one microservice per container, but if several microservice are very basic, then those microservice can be in a single container to conserve resources.

When implementing the microservice in Java, Groovy, and other Java-based languages:

To listen to the queue(s), use a RabbitMQ client with the help of Spring ‚Äì see module svc-xample-j.
Or use a Camel Route to listen to the queue(s), like the Workflow container module. Use classes in the :shared:lib-camel-connector module to facilitate this.

When implementing the microservice in other languages, such as Python or Ruby :

Use a RabbitMQ client to listen to the queue(s). There's likely common code to use the client, so set up some mechanism (like a common folder) to share that code across services for easier maintenance and updating. (TODO: create a common library for Python)
All together now

Once the above pieces are implemented, a call to the endpoint flows as follows:

VRO client sends a request to API endpoint.
Controller parses the request and sends it to the MQ via the CamelEntrance utility class using the queue named derived from the endpoint URL string. (This convention avoids having to share queue name constants across containers.)
The controller can choose to send a quick response to the client or wait for some result from the triggered workflow, depending on the semantics of the endpoint.
The Camel Route (that is listening on the queue) initiates the associated workflow given the request. At the termination of the workflow, the last response message is sent back to the controller.
One step at a time, the workflow sends JSON-string messages to specific queues (or topics). (Again, use consistent queue naming conventions to avoid synchronizing these constants across containers.)
A service (that is listening on the specific queue or topic) performs its function and returns an output, which is typically used as input to the next step in the workflow.

Figure: Interfacing with MQ from various components 

Running the xample domain microservices

To run the xample microservices, add xample to the COMPOSE_PROFILES environment variable so that the xample microservices are started in order for the xample-resource POST endpoint to succeed.

export COMPOSE_PROFILES=""$COMPOSE_PROFILES,xample""

Add DB Entities

Since the DB is shared across domains, the associated DB code resides at the top-level persistence folder, rather than the domain-specific folder.

Updates to the DB schema require adding a Flyway migration (in the db-init folder)
DB models should be consistent across domains and clearly documented with column comments. Great care should be taken to prevent the logic in one domain incorrectly modify data values in another domain.
DB ORM models should reside in the persistence folder in the appropriate subfolder. These classes do not have to be in sync but they should be consistent with each other across languages. Note that these classes can be used by any domain.

Note: A microservice should avoid interacting with the DB directly ‚Äì a workflow should act as the intermediary (see the DB-related bullet in the Service (microservice) container section). With that said, if direct service-to-DB interaction is desired, use the following guidance.

For interacting with the database via Java :

Add associated Java Entity classes in the model subfolder for use by workflows or some Repository class

For interacting with the database via Python :

Add associated SQLAlchemy ORM classes in the sqlalchemy subfolder

For interacting with the database via Ruby on Rails :

Add associated Rails ActiveRecord classes in the rails subfolder
Other Details
Message Queue (RabbitMQ)
The payload or message body is a JSON string.
Requests/calls/messages can be synchronous or asynchronous.
MQ queue names are constants that must be shared or synchronized across containers. Using naming conventions reduces the number of constants to be shared (via a library or environment variables).
In order for Camel to automatically marshal/unmarshal DTO classes for transmission in the MQ, either define the class in the gov.va.vro.model package (or nested package) or add to the vro.camel.dto-classes (in conf-camel.yml).
Configuration Settings and Environment Variables
Configuration settings for software settings within a container
scripts/setenv.sh for environment variables
Prefer to add them to application*.yml (for Java) or settings*.py (for Python). Those files allow different setting values per deployment env. Adding environment variables incurs the cost of keeping docker-compose.yml (for local development) and helmcharts (for each LHDI deployment environment) updated.
Valid reasons why a setting should be in the setenv.sh file:
A setting to support local development (and automated end2end testing)
Secret credentials like username, password, private token. Only fake secret values belong in this file. Sensitive non-production values are in the abd-vro-dev-secrets repo. Critical production values are set within the Kubernetes deployment environment -- see Secrets Vault.
A setting that must be the same or shared across containers
Feature flags: TBD
SpringBoot Profiles
See https://www.baeldung.com/spring-profiles
And PR Use @ActiveProfiles consistently #831
Merging Workflows Containers

To conserve infrastructure resources, lightweight workflows from a domain can be combined with workflows from other domains to reside in a single container. A lightweight workflow is defined as one that uses very little memory and cpu. The workflow code should be stable and expect no changes between major VRO deployments (e.g., when the VRO API is updated). Combining workflows across domains is easily done without any code change by treating the workflow module as a library (instead of a container) and importing it into a multi-domain workflows container, which gets deployed instead of the domain-specific container ‚Äì similar to how the app container imports api-controller modules from several domains.

Versioning

Semantic versioning is used for VRO releases.

All code (all Gradle modules) have the same version when a code release is created.
A deployed container uses a Docker image tagged with a specific release version. Since containers may be independently updated, deployed containers may refer to different versions.
Port numbers

VRO platform services use the typical port numbers for the particular service (usually less than port 10,000)

5432: Postgres DB service
5672: Rabbit MQ service
15672: RabbitMQ Management UI
6379: Redis cache service

The API Gateway uses the following:

8060: API Gateway
8061: health check port for the API Gateway
Ports for APIs within VRO

VRO domains offering APIs use this port numbering convention:

81Nx = VRO microservice ports, where N is an index

The VRO (Java) App uses the following:

8110: VRO App API
8110: health check port for the VRO App

The (Python) Contention Classification uses the following:

8120: Contention Classification API
8121: health check port for Contention Classification
Ports for VRO microservices

VRO microservices use this port numbering convention:

10NNx = VRO microservice ports, where NN is an index

example: 1010x = ports used for svc-lighthouse-api microservice
example: 1020x = ports used for svc-bgs-api microservice

10NN1 = health check port for microservice NN

example: 10101 = health check port for svc-lighthouse-api microservice
example: 10201 = health check port for svc-bgs-api microservice
example: 10301 = health check port used for svc-bie-kafka microservice

VRO microservices only need to connect to RabbitMQ and do not typically need to expose any service ports, except for health checks.

Mock services use this port numbering convention:

20NNx = mock service ports, where NN is an index
20NN1 = health check port for mock service NN

So the following mock services would use these ports:

20100: mock Slack
20101: for health check
20200: mock Lighthouse API
20201: for health check
20300: mock BIP Claims
20301: for health check
20310: mock BIP Claim Evidence
20311: for health check
20500: mock BGS API
20501: for health check

(Note that the 2 BIP mocks use 2030x and 2031x to denote high similarity.)

To see if a port is already being used search the code base for usages of that port.

Running Xample Domain containers

A xample-integration-test GH Action workflow demonstrates an end-to-end test, from VRO API request to microservice.

To manually test svc-workflow and svc-xample-j locally, use Docker Compose:

source scripts/setenv.sh
# Build all containers
./gradlew docker

# Start the relevant containers
./gradlew :dockerComposeUp
./gradlew :domain-xample:dockerComposeUp
COMPOSE_PROFILES="""" ./gradlew :app:dockerComposeUp

Open a browser to http://localhost:8110/swagger and go to the POST ‚Äã/v3‚Äã/xample-resource section and open it. Click ""Try it out"". In the Request body, replace null with:

{
  ""resourceId"":""1234"",
  ""diagnosticCode"":""J""
}

Note: diagnosticCode must be J in order for the request to be routed to the svc-xample-j microservice, as coded in Xample's Camel route.

Click Execute. The response code should be 201 with response body:

{
  ""resourceId"": ""1234"",
  ""diagnosticCode"": ""J"",
  ""status"": ""PROCESSING"",
  ""statusCode"": 0
}

The above can also be done with curl:

curl -X POST ""http://localhost:8110/v3/xample-resource"" -H ""accept: application/json"" -H ""Content-Type: application/json"" \
  -d '{""resourceId"":""1234"",""diagnosticCode"":""J""}'

# The API response:
{""resourceId"":""1234"",""diagnosticCode"":""J"",""status"":""PROCESSING"",""statusCode"":0}%

Check the logs:

docker logs vro-xample-workflows-1
docker logs vro-svc-xample-j-1
Xample code highlights
An HTTP API endpoint is implemented by XampleController
When an API request is received, the controller sends a one-way MQ message to the Xample Workflow for processing
In the meantime, the controller sends an API response (with status=""PROCESSING"")
The Xample Workflow uses Camel to route a MQ message to the svc-xample-j microservice
After an artificial delay to simulate processing, the microservice sends a MQ response back to the Xample Workflow
When the Xample Workflow receives the MQ response, it logs the microservice's response (with status=""DONE"")
Run microservice outside of Docker

To run any VRO component outside of Docker, you'll need to configure your environment so that the component can communicate with containers inside of Docker -- some steps for running vro-app in IntelliJ.

For the svc-xample-j microservice, no additional setup is necessary since the defaults to connect to the MQ should work.

source scripts/setenv.sh
./gradlew :dockerComposeUp
./gradlew :domain-xample:dockerComposeUp
COMPOSE_PROFILES="""" ./gradlew :app:dockerComposeUp

# Stop the Docker instance of svc-xample-j so you can run svc-xample-j's bootRun outside of Docker
docker stop vro-svc-xample-j-1

./gradlew :domain-xample:svc-xample-j:bootRun

To test, run in a new console:

curl -X POST ""http://localhost:8110/v3/xample-resource"" -H ""accept: application/json"" -H ""Content-Type: application/json"" \
  -d '{""resourceId"":""1234"",""diagnosticCode"":""J""}'
To build and restart specific containers
To build the svc-xample-j Docker image: ./gradlew :domain-xample:svc-xample-j:docker
To build all images under the domain-xample folder: ./gradlew -p domain-xample build
To run this the updated image: ./gradlew :domain-xample:dockerComposeUp
Alternatively, you can use Docker Compose directly: docker compose -f domain-xample/docker-compose.yml up svc-xample-j

For more see Docker Compose."
Testing using Swagger UI ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Testing-using-Swagger-UI,"VRO's Swagger UI is available for testing VRO's API endpoints

Local testing
Run VRO: ./gradlew :dockerComposeUp :app:dockerComposeUp -- see Docker Compose and Development process for details and alternatives
Open a browser to http://localhost:8110/swagger or http://localhost:8111/swagger

Testing MVP endpoints:

Verify VRO is running by testing an endpoint (without authenticating) by expanding an endpoint (e.g., /v1/health-data-assessment) and clicking the ""Try it out"" button, then ""Execute"". The response should be code 403 with ""error"": ""Forbidden"".
Authenticate by clicking the ""Authorize"" button (near the top of the page), fill in the X-API-Key (at the bottom of the popup window) with one of these values (original PR), then click the ""Authorize"" button (NOTE: use just test-key-01 or test-key-02 in the Authorize dialog). A ""Logout"" button should appear. Click ""Close"".
Retry the endpoint (now with the X-API-Key header).

Testing iMVP endpoints:

Verify VRO is running by testing an endpoint (without authenticating) by expanding an endpoint (e.g., /v2/automatedClaim) and clicking the ""Try it out"" button, then ""Execute"". The response should be code 401.
Authenticate by clicking the ""Authorize"" button (near the top of the page), fill in the ""Bearer Authentication (http, Bearer)"" (at the bottom of the popup window) with a valid JWT token, then click the ""Authorize"" button. A ""Logout"" button should appear. Click ""Close"". Use an example JWT, or use python to create a mock JWT:
Install PyJWT
python3 -m pip install PyJWT

Checkout generate-mock-jwt.py
Run the script
python3 jwt-maker.py

Retry the endpoint (now with the ""Bearer Authentication (http, Bearer)"" header)

To connect to Lighthouse, a service-data-access/src/main/resources/private.pem file is needed. If it doesn't exist, create it using this.

Testing on Non-prod LHDI deployment

Swagger UI can be accessed on the VA network at https://dev.lighthouse.va.gov/abd-vro/swagger."
Team Processes ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Team-Processes,"Dependabot tickets
We will rotate who is taking the lead on Depandabots each sprint.
A ticket will be created at the start of each sprint during planning and assigned to one team member.
Any Dependabots for the sprint should be linked to in the sprints Dependabot ticket so that we can track our work.
The Dependabot Lead for the sprint can post in the Benefits-VRO-DevOps channel for any Dependabot tickets they may need help with.
Dependabot PRs need 2 reviewers: the Dependabot Lead + another teammate. The assignee is the Dependabot Lead but they also act as a reviewer since Dependabot is the author of the PR.
For more information see Dependabot.
Secrel issues
All of the teams engineers should monitor the Benefits-VRO-DevOps Slack channel for Secrel issues.
If a Secrel alert is for something that may have been caused by an engineers work, that engineer should indicate in the Slack alert that they are looking into the issue.
The person assigned to the Secrel monitoring ticket for a given sprint will indicate in the alert thread in Slack if they are looking into the alert or they will tag the engineer whose work may have caused the alert in the Slack thread.
Creating tickets
When someone other than the Product Manager creates a ticket for the backlog they should post a slack message in the team channel with a link to the ticket tagging the Product Manager. The ticket creator should indicate their opinion on the priority level of the ticket in the slack message. If the ticket creator feels the ticket should be worked on in the current sprint, they should state this in the slack message and seek agreement from the Product Manager before starting. Tickets should be created for most work that a team member performs that takes more than a half day of time so the team can get credit for the work completed.
Pull Requests (PRs)
PRs must be approved by a minimum of two reviewers.
Dependabot PRs are authored by Dependabot. This means that the two reviewers are the Dependabot Lead for the sprint, and another teammate.
For more information on PRs see Pull Requests"
Support Model ( Draft ) ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Support-Model-(-Draft-),"Support Model
Introduction

Objective of this page is to define software process to support abd-vro application in production environment

Point of Contacts

#	Service Name	Mode of Contact	Point of Contact	: Escalation
1.	Data Access Service			
2.	Health Assessment Service			
3.	Compile Evidence PDF Service			
4.	Retrieve Evidence PDF Service			
5.	Claim Notification and Order			
6.	Persistence Layer			
Infrastructure Engagement Process
How to Engage MAS team for MAS failures
How to Engage LHDI team
Support Tools ( ServiceNow, PagerDuty )
Environment Access Guide

Link steps to access given environment

Standard SOP/Troubleshooting Guide
LHDI Point of Contact
MAS Point of Contact
Escalation Policy
Oncall Schedule and Rotation Policy"
Secrets Vault ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Secrets-Vault,"Secrets are stored in a LHDI's HashiCorp Vault, which resides in the VA network. Secrets include credentials, tokens, and certificates for all deployment environments. Scripts and Helm configurations have been created to formalize and reproduce deployment of secrets to all LHDI environments.

HashiCorp Vault

Secrets for all LHDI deployment environments are stored in a single vault at https://ldx-mapi.lighthouse.va.gov/vault, which requires VA network access. Following the security principle of least privilege, only members of the VRO Admins GitHub Team can log in using their GitHub credentials. Log in to the web UI using these instructions using vro-admins (which corresponds to the new VRO Admins GitHub Team) as the ""Role"".

(Context: A separate VRO Admins GitHub Team was created to limit access to secrets. By default, LHDI allows all members of VA-ABD-RRD GitHub Team to have access to a vault store, which is contrary to the principle of least privilege. There's a vault store for va-abd-rrd but it is unused.)

In the Vault, secrets are organized under the deploy/ folder. Subfolders for each environment are used as follows:

default: provides default secrets for all environments; used for the LHDI dev environment
qa, sandbox, prod-test, prod: used for the respective LHDI environment and overrides any default secrets
Only differences from default secrets need to be present. As a result, there are few secrets in the qa environment and there is no dev subfolder.

Within each environment subfolder are other subfolders, which will be referred to as ""groups"". Each group contains key-value pairs. Typically the key is an environment variable name that is mapped verbatim for use by containers. Occasionally, Helm configurations map the secret to a different environment variable name as expected by different container -- for an example, search for DB_CLIENTUSER_NAME. In summary, the full Vault path to a group is $TEAM_NAME/deploy/$ENV/$GROUP. There are no subfolders deeper than the group level.

The groups are as follows:

db: secrets for VRO's database; maps to Kubernetes secret named vro-db
mq: secrets for the message queue; maps to Kubernetes secret named vro-mq
redis: secrets for the Redis cache; maps to Kubernetes secret named vro-redis
VRO_SECRETS_API, VRO_SECRETS_LH, VRO_SECRETS_MAS, ...: secrets used by VRO components; these VRO_SECRETS_* groups map to a Kubernetes secrets named vro-secrets-....
These VRO_SECRETS_* groups are treated differently than the above groups to allow new secrets to be added without having to update Helm configurations, thereby minimizing maintenance. Most new secrets will be added in these groups.
Unlike the other groups, each VRO_SECRETS_* group is passed as a single aggregate environment variable to VRO containers that use them, as specified in Helm configurations. For example, the VRO_SECRETS_API group maps to the VRO_SECRETS_API environment variable for the app container. The aggregate environment variable contains multiple export commands like export APIAUTH_KEY01=.... Upon startup, the container runs set-env-secrets.src to execute the export commands in the aggregate environment variable, resulting in exported environment variables (such as APIAUTH_KEY01) being available for the application.
To handle multiline strings and special characters, secret values can be base64-encoded. These secrets use a key name that ends with _BASE64 so that the set-k8s-secrets.sh script will decode the value properly and sets an environment variable without the _BASE64 suffix.
Unique key names to avoid collisions

While key-value pairs are organized in separate subfolders, the key names (which are typically used as environment variable names) should be unique within each LHDI environment to avoid any collisions when they are mapped to environment variables for Docker containers. For example, if there was a MY_SECRET key name in both the redis and VRO_SECRETS_API group subfolders AND a container uses both groups, then the container will only have one environment variable rather than the desired two. Note that this collision can also occur between MY_SECRET and MY_SECRET_BASE64 key names because the _BASE64 suffix is elided from the container's environment variable name.

Adding/Modifying a secret

Ask a VRO Admin to add, remove, or update the secret in Vault. Securely provide the secret for each LHDI environment -- minimally, one secret value for dev and another for prod.

If the secret is added to an existing VRO_SECRETS_* group, no Helm configuration changes are needed.
If the secret is added to another group, Helm configurations should be updated to use the new secret.

Run Deploy secrets from Vault for each LHDI environment to update the Kubernetes secrets. The Docker containers will not use the secrets until they are redeployed.

Updates to secrets not being propagated?

There are circumstances where Kubernetes logs ""Error: couldn't find key VRO_SECRETS_... in Secret va-abd-rrd-.../vro-secrets"" -- see Slack thread for screenshots. This occurred because a single aggregate vro-secrets secret was used for all VRO_SECRETS_* groups, as that introduces issues with propagation of secret updates because containers still references the old aggregate secret:

Symptom: Sometimes redeploying the pod works and sometimes it fails with this error.
Current hypothesis for this inconsistent error: If other running pods reference the vro-secrets secret, then old versions of it may be available and is being used by new pods. This article prompted the hypothesis.
Workaround: Restart all old pods that reference the vro-secrets secret. Then start everything back up. If a restart isn't sufficient, a complete shutdown of all pods may not be necessary to remove all references to the old secret.
Additionally, marking the secret immutable may be contributing to the use of old secrets because immutable secrets aren't expected to change, so any changes (included a destroy and re-create) are not propagated. As a result, the vro-secrets secret is marked mutable in set-k8-secrets.sh.

Now the vro-secrets-* secrets are individual secrets, where an individual secret is used by one or a very small number of containers. This reduces the number of containers that need to be shut down simultaneously to release all references to the old secret. This improvement should mitigate this probably of this problem.

Adding a non-secret environment variable

To set a non-secret environment variable for a container in an LHDI environment, add it to the relevant Helm chart(s) under helm/. If the variable value is different for each environment, also add it to helm/values-for-*.yaml files.

With that said, before adding an environment variable, please read the next section.

Configuration setting vs. Environment variable

It is preferred to use a configuration file scoped to only the application/microservice/container (e.g., Configuration Settings#springs-applicationyml).

An environment variable is needed when any of the following are true:

it is a secret (username, password, token, private certificate, ...) -- use Hashicorp Vault (as described on this page)
used by multiple containers -- set it in helm/values*.yaml files and reference it in Helm charts (under helm/)
needs to be manually changed in deployment environments -- let's discuss

We should minimize the number of unnecessary Helm configurations, which will reduce DevOps maintenance and overhead, and reduce the number of factors that can cause VRO deployments to fail.

Setting the Vault-token secret

A Vault token is needed to access vault. The automation (a self-hosted GitHub Runner) expects a the Vault token to be a Kubernetes secret named vro-vault in the LHDI dev environment. The token expires monthly. Run scripts/set-secret-vault-token.sh ""$VAULT_TOKEN"" to set the token, where $VAULT_TOKEN equals the string copied from the Vault web UI (click ""Copy token"" in the upper-right corner drop-down menu).

Setting Kubernetes access tokens

Kubernetes access tokens for each cluster (i.e., non-prod and prod) are needed to be able to deploy the secrets to the LHDI environments. The access tokens expire in 90 days. Run scripts/set-secret-kube-config.sh to set the devops-kubeconfig secret.

Setting GHCR secret

A GHCR secret in Kubernetes named devops-ghcr needs to be set for LHDI to pull images. Run scripts/set-secret-ghcr.sh ""$ENV"" ""$PAT"" for each LHDI environment, where $ENV is dev, qa, etc. The $PAT is a GitHub personal access token -- generate one using the abd-vro-machine account. This only needs to be run once (or every time the PAT expires).

FAQ
Why use Vault?

It is a centralized, secure location in the VA's network designed to hold secrets. From the Vault, secrets can be quickly and consistently redeployed to various LHDI environments in case they need to be reset or rotated.

Why use a self-hosted GitHub Runner?

Our GitHub Action workflow starts a self-host runner that runs within our LHDI dev environment to pull Vault secrets and set Kubernetes secrets, all within the VA's network. This is more secure than using HashiCorp's GitHub Action which would pull Vault secrets outside the VA network and into the GitHub Action workflow environment. The runner is a container in the vro-set-secrets-... Kubernetes pod and can deploy secrets to any LHDI environment when initiated by the Deploy secrets from Vault GitHub Action workflow. The pod is deployed to the dev LHDI environment (because that environment doesn't require SecRel-signed images) and can deploy secrets to other environments.

Why are some Kubernetes secrets immutable?

There has been unexplained occurrences where Kubernetes secrets have changed and caused problems. Making them immutable aims to reduce (but not entirely prevents) this problem."
Setup on Windows ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Setup-on-Windows,"There are two ways to set up the application on Windows:

Install directly on the Windows OS, using a unix-like command line to execute the scripts
Install on Ubuntu using WSL
Installing Directly on Windows

Git Bash shell: https://git-scm.com/downloads Once install, you need to run the following to make line endings on windows compatible: git config --global core.autocrlf true

Docker Desktop: https://docs.docker.com/get-docker/

To install hadolint, first install Scoop (https://github.com/ScoopInstaller/Scoop) and follow the instructions here: https://github.com/hadolint/hadolint#install

You will also need scoop to install shellcheck: https://github.com/koalaman/shellcheck#readme

To install Spectral, find the windows executable and put it on the PATH

For the remaining setup, just follow the instructions on the README https://github.com/department-of-veterans-affairs/abd-vro#readme, executing all commands from within the Git Bash shell (not DOS or Powershell).

I should note that, although this way is simpler, not all services started successfully on docker. Postgres and Rabbit come up successfully but the vro services do not start.

Installing via Ubuntu
Make Sure wsl2 is enabled
Download Ubuntu from https://digitaltransfusion.net/ubuntu2004.zip
extract it to c:\Tools\Ubuntu2004 and start Ubuntu.exe. This will install Ubuntu
Open a windows command shell (cmd) and type the following:
wsl --set-default Ubuntu-20.04
wsl --set-version Ubuntu-20.04 2
wsl --set-default-version 2

Enable Ubuntu on Docker: 

Restart wsl
wsl --shutdown


Restart docker.

Open a new command window (cmd)

Type wsl. This should now switch to an Ubuntu shell.

type ""sudo docker ps"" to make sure it works

update linux

sudo apt update && sudo apt upgrade

Install prerequisites
d ~/
sudo apt install openjdk-17-jdk shellcheck

curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | bash

sudo wget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v1.16.3/hadolint-Linux-x86_64 && sudo chmod +x /bin/hadolint


type ""exit""
type ""wsl""
nvm install 17.0.0

npm install -g @stoplight/spectral-cli

export GITHUB_ACCESS_TOKEN=<replace-with-token-from-github>

cd ~/

mkdir repos, cd repos

git clone git@github.com:department-of-veterans-affairs/abd-vro.git



For the remaining setup, just follow the instructions on Local Setup, executing all commands from within Ubuntu."
Routing API requests ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Routing-API-requests,"This page describes how API requests are routed to a microservice and an API response is sent back to the client, leveraging Apache Camel to provide most of the glue code and common Enterprise Integration Pattern (EIP) implementations.

Synchronous API request

Referring to the code version after PR #71 was merged. To run the code, see Development process and open http://localhost:8110/swagger in a browser to see the API spec, including the ""Demo API"" provided by the DemoController mentioned below.

In the app Docker container, when a Controller class receives an API request, it converts the requests into a model, feeds that model as a message into a Camel route, and converts the result of the last route endpoint into an API response object for sending back to the client -- see DemoController for an example.
The DemoController uses CamelEntrance to inject messages into a Camel endpoint for Camel routing.
Camel routes are defined by RouteBuilder subclasses, like PrimaryRoutes and ClaimProcessorRoute. For example,
DemoController.assess_health_data calls CamelEntrance.assess_health_data_demo
to inject an AssessHealthData message into the direct:assess_health_data_demo endpoint,
which is defined in PrimaryRoutes as a route that goes to rabbitmq:assess_health_data?routingKey=health_data_assessor (the health_data_assessor queue in RabbitMQ's assess_health_data direct exchange -- RabbitMQ/AMQP concepts).
In a separate service-ruby Docker container, a Ruby microservice is subscribed to RabbitMQ's health_data_assessor queue (using RabbitSubscriber service-ruby/src/lib/rabbit_subscriber.rb). As configured in microservices.rb, when a message is popped out of that queue, HealthDataAssessor will process the message and return a result, which is sent as a reply RabbitMQ message.
It is intended that communication with microservices use RabbitMQ, typically via a direct exchange and routing key to the queue. The message should be a JSON string (automatically encoded as byte[]) -- this will facilitate replacing RabbitMQ with other messaging systems (like AWS SQS) if needed.
The reply message is received back on the Camel route in the app Docker container. When the last endpoint on the route is completed, the last message is converted to the return object of CamelEntrance.assess_health_data_demo, which will be mapped to the API response object in DemoController.
""Asynchronous"" API request

When an API request takes more than 30 seconds to complete, consider sending an immediate API response that includes a URL for the client to check on the status and/or to poll for the result. For an example, check out the POST claim API in the CamelApp, where a claim is processed asynchronously in the background and the claim's status is updated to reflect processing completion.

To achieve this, we leverage the recipientList EIP to send the POSTed claim to multiple endpoints. Since all the endpoints are configured to be asynchronous, we do not wait for any of the results and move on to the next endpoint of the Camel route (using the same message that was sent to the recipientList).

Dynamic Routing

When the route of a claim (a.k.a., a Camel message) is determined dynamically (e.g., based on the claim characteristics or the processing logic), we use a dynamicRouter EIP. For example the DynamicClaimRouter class routes a claim to an endpoint depending on claim's contentionType attribute. A more complete example is provided in the CamelApp."
RabbitMQ Microservice reliability ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/RabbitMQ-Microservice-reliability,"This is a condensation of a short tech talk discussing findings and recommendations from issue #565.

RabbitMQ uses acknowledgments to ensure that queue consumers reliably receive messages. A consumer can listen for messages on a queue using one of two settings:

Auto-ack (no-ack) - Once RabbitMQ has successfully written a message to the TCP socket, its job is done and it forgets about the message. From RabbitMQ's perspective, it does not expect any acknowledgment back from the consumer. From the consumer's perspective, the receipt of the message comes with an implication of acknowledgment (courtesy of TCP's syn-ack handshake); no further action is needed to tell RabbitMQ to forget about the message.
Explicit ack - The consumer is responsible for explicitly sending an acknowledgment back to RabbitMQ. If it doesn't (due to buggy code, or a system failure that the consumer can't handle), RabbitMQ will wait a certain amount of time (30 minutes by default), then re-queue the message for delivery to another consumer. Note that an ack can mean: (1) a consumer received the message (redundancy for the TCP syn-ack) or (2) consumer successfully processed the message (may not be needed if the caller is expecting a response to the request) -- the semantics is for us to decide but we should be consistent.

The remainder of this page describes a few scenarios for how VRO microservices could interact with RabbitMQ to ensure reliability (or not), starting with some obviously unworkable ones.

No acknowledgment

The microservice consumes requests with auto-ack on, and performs the work without communicating out about any success or failure.

This does not provide any safeguards in case of failure, and should only be used if the microservice is intended to perform optional ""best-effort"" work.

Explicit acknowledgment

The most basic way to address the above issue is to have the microservice explicitly acknowledge that it performed its work successfully.

See the re-queuing section below for a description of RabbitMQ's behavior if it never receives the explicit ack.

Responding to app

Currently, every microservice in VRO is invoked by a caller (for example the API server, labeled here as ""App"") that depends on some sort of result from the microservice. As with the request sent to the microservice, the response is published to RabbitMQ, and subsequently consumed by the caller.

In this case, the response message also serves conceptually as an acknowledgment from the microservice that it performed its work successfully. The next two scenarios describe what happens when a failure occurs.

Responding to App with error

If the microservice encounters a failure and can recover (in Java and Python microservices that wrap the entire task in a try/catch, this likely accounts for nearly all failures), the microservice can send an error response back to the app.

This is nearly identical to the previous scenario, just with different content in the response message -- the error response acts like the ack.

Letting the App time out

Occasionally there are failures can't be handled by the microservice, e.g. if its container gets killed by OOM. In this case, the microservice isn't able to send back any response, so it's up to the App to time-out after a reasonable amount of time.

This works well when the App itself is bound by a request/response context that is expected to return within a reasonable amount of time. Other callers/requesters must ensure to set a response timeout and handle it by resubmitting the request (itself or via RabbitMQ's features) or raising an error for the originating caller to handle.

Re-queuing asynchronous work

In the event that the microservice is responsible for work that finishes at some indeterminate time in the future, RabbitMQ's re-queuing of unacknowledged messages can be used. When the microservice consumes messages with explicit acks, RabbitMQ will wait a certain amount of time (30 minutes by default) for the ack, and re-queue and re-deliver the message after a time-out.

This sequence diagram shows the flow when a second microservice is available to consume the message."
Release Tags ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Release-Tags,"Review current tags and follow Semantic Versioning guidelines to choose the next version number for the release tag. For the desired branch (usually develop), use the create-release-tag GitHub Action workflow in the public repo to update version numbers in the code and create a release tag.

When the release tag is created, the create-gh-release GitHub Action workflow will automatically create a GitHub Release (these auto-created GitHub releases with suffix -auto are candidates for deletion during cleanup).
Additionally, the mirror GitHub Action workflow will copy the release tag to the internal repo, where an identical GitHub Release is auto-created in the internal repo, which will trigger SecRel GitHub Action workflow to run and sign the images. The packages/images published to GHCR should be tagged with the release tag version number (v*.*.* or release-*.*.*).
The Secure Release process pushes packages to the GitHub Container Repository. Note that the packages are associated with the abd-vro-internal repo.
You can follow progress of SecRel (Secure Release) workflow run on the internal repo's Actions.
If the SecRel workflow is not successful, the SecRel alerts need to be addressed -- see SecRel Processing Guide.

All GitHub releases that are annotated as a Pre-release (and its associated git tag) will be automatically deleted by the Delete old Pre-Releases and tags GitHub Action workflow. To prevent deletion, remove the Pre-release annotation.

Details

Refer to diagram at Deployment-Overview#process-diagram.

create-release-tag workflow runs the following:

./gradlew release creates a git tag locally (until it is pushed to the remote in the last step)
scripts/image-version.sh pin to ""Pin unpinned image versions"" to the release version number created in the previous step -- see Container Image Versions
git push updated image versions to develop
triggers mirror workflow to push changes to internal repo
triggers SecRel workflow (on internal repo) but any processing is skipped due to a gate-check, which exists to avoid unnecessary processing. SecRel will be triggered in the next step.
git push release tag
triggers Auto-create GitHub release on public repo
triggers mirror workflow to push changes to internal repo
triggers Auto-create GitHub release on internal repo)
triggers SecRel workflow (on internal repo) on release tag
publishes non-dev images (i.e., those without the dev_ prefix) to GHCR -- see CI-CD-Workflows#1-continuous-integration-ci
calls SecRel pipeline to scan images

Periodically, the Delete old Pre-Releases and tags runs to declutter pre-releases and git release tags the public repo. These deletions will be automatically propagated to the internal repo when the mirror workflow runs, at which point:

Git release tags (associated with GitHub pre-releases) are deleted in the internal repo
These tag deletions will result in GitHub releases disappearing in the internal repo, but the number of releases (shown on the front page) is incorrect (i.e., there are orphaned GH releases) -- submitted bug report to GitHub.
8/24/2023: Bug report closed with:

Our engineering team has flagged this issue as it appears to be a bug in displaying the releases. There is no current ETA on when the fix will be available, but I recommend you keep an eye on the GitHub Blog and our social media feeds for the latest announcements about new features and fixes. Our policies prevent us from maintaining a ticket in an open state for extended amounts of time. With this I am going to resolve this ticket. I understand that this doesn't resolve the issue reported, however it will likely be some time before any changes are made."
Release Process ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Release-Process,The release process requires access to the VA system. Information on the release process can be found on this page of the internal wiki.
Quick Deploy Instructions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Quick-Deploy-Instructions,"Spun off from the Deploy to Prod page.

This is a straight-forward walk-through, with visual aides, for cutting releases and performing deploys for VRO.

These instructions are valid as of April 20th, 2023. If this changes, please remember to update this doc and this date.

June 1, 2023: Prepended ""[THIS IS NOW AUTOMATED]"" to obsolete steps.

There are actually two (2) GitHub repositories we need to use for this:

abd-vro
abd-vro-internal

You will need the correct permissions in order to create releases for both repositories.

THIS ASSUMES YOUR main BRANCH IS IN A STATE TO CUT A RELEASE FROM ‚Äî usually by merging develop into main ‚Äî such that it has all the latest and greatest items you want in your next release.

1) First, we want to be on abd-vro

2) [THIS IS NOW AUTOMATED] Next, we will want to create a new release

Click on either of these areas I've highlighted with red boxes:

3) [THIS IS NOW AUTOMATED] Draft a new release:

4) [THIS IS NOW AUTOMATED] Click on ""Choose a tag"":

5) [THIS IS NOW AUTOMATED] Our release hasn't been created yet, so we need to manually type it in

In this example, we'll be bumping the patch iteration of our semantic version number up by one ‚Äî going from release-1.0.94 to release-1.0.95. Then we will need to click on ""Create new tag"".

6) [THIS IS NOW AUTOMATED] Select the branch you want to cut the release from

In this example, we're doing so from main (this is most likely going to be the only branch we cut releases off of from now on)

6) [THIS IS NOW AUTOMATED] Type in the name of the release in the ""Release title"" text field. We've been using a format like so: v1.0.95

You don't need to type in a description. When everything looks correct, click ""Publish release"".

7) Once published, this is what you should see:
8) Go to abd-vro's Actions tab and make sure the ""Mirroring"" job runs and goes green

You might have to wait a minute for the job to queue up, but this should happen quickly. This job does what it says in the name and tries to mirror abd-vro to abd-vro-internal. In our case, it pushes the tag over to abd-vro-internal so we don't have to create the tag over there, though we will still have to create the release.

More info on tags versus releases.

9) Now we switch to the abd-vro-internal repository

Note that this repo's most current release is still the old one. In this example, it's still release-1.0.94

10) [THIS IS NOW AUTOMATED] REPEAT STEPS 2-7 FOR THIS REPO

The only difference will be that this time, the release-1.0.95 tag we created in abd-vro was ""mirrored"" over ‚Äî just the tag, not the actual release, which we will have to create (the same way) in this repo.

NOTE: Selecting the new release created in abd-vro will default your branch to main

11) Go to abd-vro-internal's Actions tab and make sure you see a job with the release tag as the job name

It may take a minute to queue up but is generally quick to show up. The workflow job itself, however, takes a while to run ‚Äî approx 20 mins. We need to wait and make sure it turns green. Once complete, if successful, you should be ready to deploy now (next step)

12) In your Actions Workflow pane to the left, click on ""1. (Internal) Update Deployment"" and then ""Run workflow""

13) [OBSOLETE] Select your branch and target environment you wish to deploy from

The following is obsolete -- refer to https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deploying-VRO#deploy-images-with-the-release-tag instead.

Default settings for all else will be used in most cases. The deployment workflow itself shouldn't take anywhere near as long as the previous SecRel step. It averages 4-5 minutes.

You're done!

That's all there is to it. Provided there are no problems (usually SecRel-related), this should be a straight-forward process.

If you don't wait between manual steps, then the whole thing should usually take around 30-40 minutes."
Pull Requests ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Pull-Requests,"Procedure for submitting and merging PRs (pull requests).

Create a pull request
The title of the PR becomes the commit message in the main branch, so be as concise and specific as possible. Refer to How to Write a Git Commit Message.
Try to limit to 300 lines of code changes; smaller logical chunks of code is better; https://blog.logrocket.com/using-stacked-pull-requests-in-github/
https://opensource.com/article/18/6/anatomy-perfect-pull-request
https://developers.google.com/blockly/guides/modify/contribute/write_a_good_pr
Add a minimum of 2 reviewers or post in Slack channel #benefits-vro-engineering for anyone to review
Exception: Dependabot PRs are authored by Dependabot. This means that the two reviewers are the Dependabot Lead for the sprint, and another teammate. See Dependabot for more information.
Look for successful testing steps as GitHub actions for your changes. Note that some actions such as Container Health Checks will be skipped on some PRs, skipped actions are acceptable but failed actions should be addressed.
Add comments to your code under the ""Files Changed"" tab to explain complex logic or code
https://betterprogramming.pub/how-to-make-a-perfect-pull-request-3578fb4c112
Reviewers provide feedback, clearly marking blocking and non-blocking suggestions
https://github.blog/2015-01-21-how-to-write-the-perfect-pull-request/
Address feedback and repeat review step until PR is approved
Squash and merge PR, which will trigger Github-Actions#mirror among other things
Code Reviews

Code reviews are intended to help all of us grow as engineers and improve the quality of what we ship. These guidelines are meant to reinforce those two goals.

CODEOWNERS

PRs are currently auto assigned to Github department-of-veterans-affairs/benefits-vro-engineers team, this can be tweaked and changed when additional code based rules apply based on the documentation here: https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners

For reviewers

Aim to respond to code reviews within 1-2 business day.

Remember to highlight things that you like and appreciate while reading through the changes, and to make any other feedback clearly actionable by indicating if it is optional preference, an important consideration, or an error.

Don't be afraid to comment with a question, or to ask for clarification, or provide a suggestion, whenever you don‚Äôt understand what is going on at first glance ‚Äî or if you think an approach or decision can be improved. Code reviews give us a chance to learn from one another, and to reflect, iterate on, and document why certain decisions are made.

Once you're ready to approve or request changes, err on the side of trust. Send a vote of approval if the PR looks ready except for trivial changes. Use ""Request changes"" when there's a blocking issue or major refactors that should be done.

For authors or requesters
Draft PRs

It is highly recommended that you create a Draft Pull Request initially, then when ready, mark it as Ready for Review.

Your PR should be small enough that a reviewer can reasonably respond within 1-2 business days. For larger changes, break them down into a series of PRs. If refactors are included in your changes, try to split them out as recommended below.

As a PR writer, you should consider your description and comments as documentation; current and future team members will refer to it to understand your design decisions. Include relevant context and business requirements, and add preemptive comments (in code or PR) for sections of code that may be confusing or worth debate.

Re-requesting reviews after completing changes

After you make requested changes in response to code review feedback, please re-request reviews from the reviewers to notify them that the work is ready to be reviewed again."
Proposal to simplify abd_vro ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Proposal-to-simplify-abd_vro,"The Problem

The code collected under abd-vro serves a several purpose:

To act as a parent for all subprojects relating to claim processing

To host the Glue code (JPA, REST, and Camel) that connects the individual sub-services

To host the CI and deployment scripts

It's multi-purpose nature already makes this repository pretty complex. To simplify its structure, we should aim to concentrate the source code and, especially, the configurations, in a few files as possible.

In my opinion the fact that the project structure is spread over several sub-modules is problematic as it involves several unnecessary features:

Figure out which module depends on which and try to enforce those dependencies
Require a build script for each module
Require spring configuration for every module, especially for testings
Makes it very hard to run integration tests because spring requires a full configuration for @SpringBootTest
The Solution

In my opinion, the solution is to simplify the project as much as possible, and this can be accomplished by combining all the sub-modules of ""app"" into a single module. This module will include the following sub-modules:

api
app
controller
service (and all sub-modules of service)
persistence (and all sub-modules of persistence)

There will be one build script and one spring boot configuration (two if you count the tests) for all these modules. That's it.

Architectural structure

Whereas it is important that the layering of the architecture should be preserved, the layering does not need to rely on submodules. In the current structure, persistence, services, controllers, etc reside in separate gradle module. It suffices that they reside in different packages within the same module. We can leverage archunit, which we are already using, to enforce the dependencies between layers.

Final input from Dimitri:

Overall, I don‚Äôt think this application is a good candidate for breaking up into even more microservices. When refactoring into microservices, we accept the price of additional complexity in order to gain flexibility advantages. These advantages include the ability to develop parts of the application independently as well as scale different pieces independently. If no such advantage can be gained, then we just pay a complexity price for no reason.

My reasoning for not recommending microservices can be summarized by the following points:

This application is already an orchestration layer which coordinates the actions of several microservices into a cohesive workflow.
The same external APIs are used at every step of the workflow: Lighthouse, MAS, BIP, and the python services. This means that there is no particular component which is responsible for communicating exclusively with some type of API.
Everytime we interface we an external service, the probability that something will go wrong increases. If we inject additional microservices, there will be two levels of indirection before calling, say, the MAS API thus augmenting the probability of error and the difficulty of debugging
There is actually not that much code in this application.

Nevertheless, if we really want to extract microservices, the key is to identify components that can act virtually independently of one another. I can think of the following components that satisfy this criterion:

The database/auditing framework can become a downstream service that receives claim updates and persists them in the database and generates alerts as needed
The v1 and v2 processing streams have their own controllers and their own camel routes. This makes them good candidates for independent services. The downside of separating them is that they share a lot of the same code, so some common libraries will need to be extracted as well.
Input from Rajesh on Refactoring the MAS Integration:

To refactor the interaction with MAS into self contained microservice, it could include the following

Authorization flow support components in the ""app folder"" (props, config)
MAS API service components(auth, service, mapper) in the ""service folder"" (mas)
Abstract the call to get the MAS collection status and annotations into a single call with options to return the response from MAS asis and mapped to a common model(abdevidence)
Abstract the call to MAS order exam"
Partner Team Deploy Process ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Partner-Team-Deploy-Process,"Separate Domain Branch

To support a more independent deployment process for a Partner Team, a temporary domain-... branch (diverging from the develop branch) can be used during domain development. While this may add marginal complexity in git branch management, it offers the following advantages:

Separate commit histories isolate changes so that changes in one branch doesn't interfere with the other branch.
While the domain code is being developed, the domain-... branch is based on a stable/unchanging develop commit, which offers a consistent VRO Platform and minimizes any delays in non-prod deployments.
Changes to domain-specific Docker images can deploy to non-prod environments, independent of changes to the develop branch.
The domain-... branch can sync up with the develop branch as frequently as desired. Any significant changes to develop that affect domain(s) are communicated to domain developers, allowing Partner Teams to make updates according to their timeline/schedule (i.e., Partner Teams don't have to address the Platform changes immediately, and the changes don't immediately block their work).
Encourage separation of concerns, where domain-specific changes should be committed to the domain-... branch, while VRO platform changes should be committed to the develop branch. This will also mitigate future branch-merging conflicts.

When the domain code is ready for deployment to prod, the VRO Platform Team will update the develop branch using the domain-... branch, update the main branch, and (once testing is successful) deploy to prod.

Once major domain development is complete, the domain-... branch can be deleted and minor domain-specific changes are committed to the develop branch.

LHDI Deployment requires only Docker images

Deployment to LHDI requires only Docker images. Git branches help to track and manage the codebase as it's being developed. Once Docker images are created, git branches are irrelevant -- the images are associated with git via the commit hash uses as the image tag.

To deploy domain images:

Create and publish domain images (created from the domain-... folder) to GHCR (as dev or non-dev images) and have SecRel sign them.
Deploy the domain images to LHDI, leaving other images unchanged (i.e., helm --reuse-values).

(When we have multiple domains being deployed, we can leverage separate ""Helm releases"" (not to be confused with GitHub releases or git release tags). For now, let's ""Keep It Simple, Silly"" and have only one Helm release currently called vro.)

Git Branches and Commit History

To explain how a separate domain-... branch will be used in the context of deploying domain images, the following sections describes the basic deployment steps (see Deploying VRO for details), followed by sections that use those basic steps in relation to git branches.

Domain branch

The VRO Platform Team creates a protected domain-* branch for the Partner Team to develop code and deploy to non-prod LHDI environments independently of the develop branch (and its changes). Take for example a ""CC"" domain:

Create (or reset) the domain-cc branch to the latest develop commit
Partner Team commits to domain-cc branch (with PR reviews from the VRO Platform Team)
Non-prod Deploy of Domain branch

(Similiar steps as Non-prod deploy of Develop except they're applied to the domain-cc branch)

Create a release tag on domain-cc by selecting the domain-cc branch for ""Use workflow from"" and using tag format v*.*.*-domaincc-N where N is an integer counter. This tag format helps to distinguish domain release tags from those created from the develop branch.
Use Update CC Deployment action to deploy the created release to the desired non-prod LHDI environment (dev, qa, sandbox, or prod-test). Deployments to prod are performed by only the VRO Platform Team.
Update Domain branch with the Develop branch

After some development, the develop and domain-cc can have different commits:

To update the domain-cc branch, rebase the domain-cc branch against the latest (or desired) develop commit:

Partner teams are encouraged to rebase against develop frequently to minimize the number of accumulated merge conflicts and to stay up-to-date with the develop branch.

Rebasing will enable the develop branch to easily be updated (in the next section) without any merge conflicts and maintain a cleaner git commit history (i.e., without merge commits).

Update Develop with the Domain branch

Prerequisite: The Partner Team has completed Update Domain branch with the Develop branch and tested its deployment.

To update the develop branch, ask the VRO Platform Team to fast-forward merge the develop branch to the latest (or desired) domain-... commit. After which, more commits can be added to the domain-... branch:

Then, Create a release tag on develop using the latest release version number. This is to ensure the latest develop commit is used as the last release version in case releases have been created on both develop and domain-cc.

Prod Deploy of Domain branch
Update Develop with the Domain branch
Ask the VRO Platform Team to do Prod deploy of Main"
Possible Errors on Windows Machines ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Possible-Errors-on-Windows-Machines,"Windows users could run into some common issues when trying to build or run VRO. If you are a windows user and run into any issues that are related to using windows, please feel free to update this page with the error and your solution.

CRLF -> LF

When trying to run the app on docker, I get this error message: Error creating bean with name 'bipCERestTemplate' defined in class path resource [gov/va/vro/config/BipApiConfig.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.web.client.RestTemplate]: Factory method 'getHttpsRestTemplate' threw exception; nested exception is [gov.va](http://gov.va/).vro.service.provider.bip.BipException: Failed to create SSL context. Caused by: java.io.IOException: Keystore was tampered with, or password was incorrect

This is because in your abd-vro-dev-secrets repo the file endings are CRLF, when they need to be LF to work. To fix this:

Open the abd-vro-dev-secrets project into an IDE. I use intelliJ for abd-vro, but for abd-vro-dev-secrets I use VS code to keep them separate.
In the bottom right corner of the screen, you will see that it says CRLF. Click on this and change it to LF. You will need to repeat this process for every file.
Make sure you get the files under abd-vro-dev-secrets/local as these are the files causing the issue.
Make sure to save every file that you made the change on and then open abd-vro
Now set your env variables using source scripts/setenv.sh
Run the app and see that the app now starts up as expected."
New Domain Setup ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/New-Domain-Setup,"To set up a new domain folder and deployment configuration for a Partner Team, this page enumerates the tasks, which are organized into the following stages:

Gradle domain subproject for all domain code
Docker configurations to containerize applications
Helm configuration for LHDI Deployment
Miscellaneous Setup

Unless otherwise stated, any team member (including those in the Partner Team) can perform the described tasks.

These tasks set up and configure the resources needed for eventual deployment. For deployment to LHDI, see Partner Team Deploy Process.

References

PRs that add a new domain:

domain-cc Feature - run contention classification api in docker #1646
Setup a deployment process/plan for domain-ee in nonprod #1922
By 8/21: Setup a deployment process/plan for domain-ee in nonprod #1932

PRs that add a new VRO microservices:

Docker: add svc-bie-kafka container image #1794
Helm: Add config for svc-bie-kafka #1796
1. Gradle subproject for all domain code

Create a new domain-XYZ folder (replacing XYZ with a name or acronym associated with the Partner Team). In that folder, add build.gradle and README.md with build and test instructions for the domain. Refer to other domain-* folders.

Add a domain application project

In the domain-XYZ folder, create Gradle subproject(s) for your application. The subprojects depend on your application.

Populate the subprojects with code -- refer to Software Conventions and other domain-* folders; for example:
For Java projects, refer to domain-xample.
For non-Java projects, the XYZ-app subproject should have build.gradle and gradle.properties files in the domain-XYZ/XYZ-app folder. There can be multiple *-app subprojects in the domain folder.
Add each subproject to settings.gradle in the project root folder.
Add unit tests to the subprojects and ensure ./gradlew :domain-XYZ:test executes those tests. Then in .github/workflows/test-code.yml, update and add job steps to run unit tests in the domain by running ./gradlew :domain-XYZ:test.
2. Docker configurations to containerize applications

For end-to-end and integration tests, add a docker-compose.yml in the domain-XYZ folder (see Docker Compose). It should set up any new containers added in the domain-XYZ folder. Not all subprojects are manifested as Docker container, i.e., some subprojects are used as libraries -- those projects don't need to be added to docker-compose.yml.

Add container to scripts/image-names.sh the list of IMAGES environment variable. Run ./scripts/image-names.sh to update scripts/image_vars.src, which is used by GH Action workflows (e.g., to build and publish the Docker image to GHCR). Commit both files.

Add container healthcheck
For each new container, add a healthcheck.
Refer to the HEALTHCHECK command in Dockerfile-java or Dockerfile-python within the gradle-plugins/src/main/resources/docker/ folder.
Specify the healthcheck_port in domain-XYZ/XYZ-app/gradle.properties. For port numbering convention, see Software-Conventions#port-numbers.
Add a job step to .github/workflows/container-healthchecks.yml that exercises the healthcheck.
Add integration tests
Add end-to-end and/or integration tests for the domain.
Add a new GitHub Action workflow that runs those tests. For examples, refer to *-integration-test.yml and *-end2end-test.yml.
Add the new workflow as a job in .github/workflows/continuous-integration.yml.
Add mock service

If a new mock service is needed, add it to the mocks/ folder. Note that mocks/ is a Gradle composite build (not a Gradle subproject) to isolate it from the typical VRO codebase because the mocks are not built for LHDI deployment. See PR Gradle: refactor mocks into composite build #1704 for details.

Update CI/CD GitHub Action workflows
For each Python subproject, add the requirements.txt files to .github/actions/setup-vro/action.yml and .github/secrel/config.yml.
If the domain application is ready for daily deployments to LHDI dev as part of [our CI/CD](CI CD Workflows), then add a job to .github/workflows/continuous-delivery.yml.
3. Helm configuration for LHDI Deployment
Add a new helm/domain-XYZ/ folder and populate it. Refer to Helm Charts and helm/domain-*/.
Ensure that helm/domain-XYZ/templates/named_templates is a symbolic link.
Add an entry to helmChartAppVersions in build.gradle in the project root folder so that Helm chart version number are updated when a release is created.
Update Deployment GitHub Action workflows
Add the new helm chart folder as a helm_chart option in .github/workflows/update-deployment.yml.
Optionally, create .github/workflows/update-deployment-XYZ.yml that deploys to all LHDI environments except prod for Partner Teams to use. Tip: copy and modify existing workflows, e.g. update-deployment-cc.yml and update-deployment-ee.yml.
In the rare situation where different versions of containers in the same helm chart needs to be deployed, create a new workflow (refer to update-deployment-app.yml) and update deploy.sh (to pass the version numbers to the helm command).
4. Miscellaneous Setup
Optional: domain-XYZ branch

If the Partner Team wants to work on a separate git branch (see Partner-Team-Deploy-Process#separate-domain-branch):

Ask the VRO Team to create a domain-XYZ branch in the repo. It will be a protected branch and is configurable by repo admins at https://github.com/department-of-veterans-affairs/abd-vro/settings/branches.
Add the branch as a domain_branch option to .github/workflows/fast-forward-develop.yml.
Add entry to API Gateway

To have the OpenAPI spec available via Swagger UI, update configurations for the API Gateway -- refer to API-Gateway#adding-a-new-domain-api-for-lhdi-deployments-and-swagger-ui which:

configures the VirtualService
updates Swagger UI configuration"
Mock Slack Server ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-Slack-Server,"Mock Slack Server is mainly developed to validate calls to Slack channels in automated end-to-end tests.

Implementation

Mock Slack Server is a Spring Boot based REST API.

It is implemented as a Gradle subproject mocks:mock-slack in abd-vro project.

Slack Message Store

The primary functionality of Mock Slack Server is to detect Slack channel posts. To facilitate end-to-end tests, this Mock Server stores all received Slack messages in a Java HashMap with collection id's as keys. The values are the content of the slack message as stored in a SlackMessage object. The collection id is extracted from the message using a regex expression. The regex can be found in the Spring Boot Application configuration.

Spring Boot Application configuration initializes the HashMap as part of initialization of the SlackMessageStore bean.

End Points

End-points are defined in an interface and implemented in a controller.

The end-points are:

POST /slack-messages: posts a new slack message.
GET /slack-messages/{collectionId}: retrieves a previously posted slack message for the collection.
DELETE /slack-messages/{collectionId}: deletes any stored slack message for the collection.

The main POST end-point mock Slack API basic message POST end-point. This end-point accepts a basic json object with text property similar to the Slack end-point. The collection id is extracted from the text property using the regex available in Spring Boot Application configuration.

The GET end-point retrieves the slack message posted for a collection. It is used in automated end-to-end tests to verify a slack message is posted.

The DELETE end-point removes the slack message for a collection if it has been posted. It is used before any end-to-end test to make sure a previous run of the end-end-test does not affect the current run."
Mock MAS API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-MAS-API,"Mock MAS API is mainly developed to validate MAS related functionality in automated end-to-end tests.

Implementation

Mock MAS API is a Spring Boot based REST API.

It is implemented as a Gradle subproject mocks:mock-mas-api in abd-vro project.

Mock Collections

The primary functionality of Mock Mas API is to provide mock collections. All mock collections are specified in a resource directory called annotations as json files. A naming convention collection-ddd.json is followed where ddd stands for the collection id.

These collections are modelled after actual collection data receieved from MAS before and during end-to-end testing.

These mock collections are stored in a Java HashMap with collection id's as keys. Spring Boot Application configuration initializes the HashMap as part of initialization of the Collection Store bean.

Exam Ordered Store

To facilitate end-to-end tests, this mock server keeps tracks of the collections for which exams are ordered. This information is stored in a Java HashMap with collection id's as keys. The values are Boolean objects. Spring Boot Application configuration initializes the HashMap as part of initialization of the Exam Order Store bean.

End Points

End-points are directly defined and implemented in a controller.

Three end points mock MAS functionality:

POST /token: retrieves a jwt.
POST /pcQueryCollectionAnnots: retrieves a collection.
POST /pcCheckCollectionStatus: retrieves status of a collection.
POST /pcOrderExam: orders an exam.

The remaining end points provides information about the state of the server to facilitate automated end-to-end tests:

GET /checkExamOrdered/{collectionsId}: checks if an exam ordered for a collection.
DELETE /checkExamOrdered/{collectionsId}: resets exam ordered status for a collection.
Token Retrieval

The end-point /token retrieves jwt. This end-point is a pass-through; it simply calls to MAS development server for token and returns the retrieved token.

Note that this mock server itself does not validate the token when it is attached as the Authorization HTTP header in calls to other end-points. This has been left to future development primarily due to other priorities.

Collection Retrieval

The end-point /pcQueryCollectionAnnots retrieves a collection. This end-points accepts a simple json object that specifies the collection's id.

Currently MAS development server provides only one collection id'ed 350. For this collection id, this end-point retrieves the collection from MAS development server and returns the the caller. For all collection id's that identify one of the mock collections, the mock collection is return. If the collection id is not 350 or does not identify one of the mock collections, Not Found (404) status code is returned.

Collection Status Retrieval

The end-point /pcCheckCollectionStatus currently simply returns VRONOTIFIED status. Since MAS is now preparing the collection before calling to VRO, we do not expect any other status.

Exam Orders

The end-point /pcOrderExam mocks ordering an exam. This end-point currently always returns success. In addition the collection id for which the exam is ordered for is stored in the Exam Ordered Store.

Exam Ordered Status

The end-point GET /checkExamOrdered returns if an exam is ordered for a collection. This information is useful in end-to-end tests. A DELETE end point is also provided so that the mock server can be reset for a particular collection id. This mackes it possible to run end-to-end multiple times after this mock api is starred.

Environment Variables

This mock API uses the same MAS related environment variables that VRO application uses. You can find these environment variables in the application.yml mock-mas properties."
Mock Services ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-Services,"Mock Services are primarily used to simulate external systems VRO integrates with in End to End Tests.

There are five mock services, one for each external system

Mock BIP Claim Evidence API
Mock BIP Claims API
Mock Lighthouse Health API
Mock MAS API
Mock Slack Server"
Mock Lighthouse Health API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-Lighthouse-Health-API,"Mock Lighthouse Health API is mainly developed to validate LightHouse Health API related functionality in the automated end-to-end tests.

Implementation

Mock Lighthouse Health API is a Spring Boot based REST API.

It is implemented as a Gradle subproject named mocks:mock-lighthouse-api in abd-vro project.

Mock Fhir Bundles

The primary functionality of Mock Lighthouse Health API is to provide Mock Fhir Bundles. All Mock Bundles are in a resource directory called mock-bundles in patient specific directories that are named after the Mock ICN for the patient.

Each patient specific directory contains the actual Mock Bundles named after the resource types. Currently there are three types bundles all in json format

Observation.json
MedicationRequest.json
Condition.json

These Mock Bundles are modeled after the actual Fhir Bundles received either from Lighthouse Sandbox environment or from Lighthouse Production environment before and during end-to-end testing.

The Mock Fhir Bundles are stored in a MockBundles object which are themselves stored in a Java HashMap with the patient ICN's as the keys. Spring Boot Application configuration initializes the HashMap as part of initialization of the MockBundleStore bean.

End Points

All end-points are defined in an interface and implemented in a controller.

All end-points mock Lighthouse Health API functionality. One is for token retrieval and others are for mock Fhir Search End-points to retrieve resources:

POST /token
GET /Observation
GET /Condition
GET /MedicationRequest

The end-point /token retrieves jwt. This end-point is a pass-through; it simply calls to Lighthouse Health API Sandbox server for the token and returns the retrieved.

All other end-points identify the patient ICN from the patient query parameter. If the ICN is one of those for which Mock Fhir Bundles are available, the Mock Fhir Bundle is returned. Otherwise the Bundle is retrieved from Lighthouse Health API Sandbox server and returned.

Note that this Mock Server itself does not validate the token when returning the Mock Bundles. For Sandbox Bundles, Mock Server passes the token to the Lighthouse server.

Environment Variables

This Mock API uses the same Lighthouse Health API related environment variables that VRO application uses. You can find these environment variables in the application.yml mock-lh properties."
Mock BIP Claims API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-BIP-Claims-API,"The BIP APIs are not available outside of VA firewall for local development. There is a UAT BIP Claims API server within VA firewall but using it currently requires

Developing on GFE's which have typically not enough resources for projects of VRO size
Installing and maintaining of Java requires special permissions
Other security requirements makes it difficult to test https
Since the server is not controlled by the VRO team creating and maintaining claims for test cases is difficult

Due to these difficulties, a Mock BIP Claims API has been developed. This mock is now an integral part of the End to End Tests.

HTTPS Server

Mock Claims API is an HTTPS enabled Spring Boot server. It uses self-signed certificates generated by the build-certificates.sh. In particular the PKCS#12 files client_truststore.p12 and server_keystore.p12 are placed in resources directory and pointed directly in application.yml server.ssl property to set up an https port 8097.

Note that if you generate a new set of self-signed certificates using build-certificates.sh and change it in Mock BIP Claims API, the corresponding certificates in Application and Mock BIP Claim Evidence API subprojects must change as well.

Health Check

The standard Spring Boot Health Check is provided in http port 8080 and configured in application.yml management property.

HTTP Server

One primary use of the Mock Claims API is the End to End Tests. Since these are black-box tests that do not have access to https setup within the VRO application, it is involved to have these tests to use the https port. To simplify the End to End Tests, an http port 8099 is made available. The full set of end-points are available through this port.

The implementation of opening the http port 8099 can be found here.

Open API Specification

Open API Specification of the Mock BIP Claims API is available from the Swagger page.

The primary purpose of the Mock BIP Claims API is to mock the end points that are used by the VRO application. Thus all the end-points listed in BIP Claims API API Calls section are implemented. The initial version of the api specification and model classes are generated using the Open API Generator using the Open API Specification of the BIP Claims API

npm install @openapitools/openapi-generator-cli -g 
npx @openapitools/openapi-generator-cli generate -i bipclaim_3.1.1.json -g spring -o code

The Open API Specification bipclaim_3.1.1.json is available from the Swagger page by clicking on the small /v3/api-docs link under the title.

The initial version of the api specification has been changed very little. The model classes has been updated primarily to use Lombok instead of verbose getters and setters.

Future Work

The plan was to merge the model classes with the VRO proper model classes. But since the mock has been developed late, we abandoned the idea to reduce risk.

Security Requirements

This mock implements a JWT based security configuration. At this time not all the claims are checked. However signature is validated in a JWT Request Filter.

A JWT generator is included together with an interface to specify claims. This interface is implemented in this class whose property values is read from application.yml mock-bip-claims-api.jwt setting.

For testing purpose you can update the application-test.yml with development, prod or production JWT claim values and run a test such as Claims Test and JWT will be available in the logs. In principle you can use that JWT in curl or other tools to make API calls if you specify the certificates as well.

Future Work

The JWT generator implementation is mainly used in unit tests. It could be merged with the VRO implementation since it is a bit cleaner and more modular.

Test Cases

This mock uses a simple Map store to hold all the Mock Claims. This Claim Store is initialized as a bean in Application Configuration. It reads all the Mock Claims from a json file. If you need new test cases you can add to this json file. The mock server does not watch the file updates so you will need to recompile and deploy.

Updates

To simplify to get insight into the state of claim objects an Updates Store has been created. This store is again initialized as a bean in Application Configuration and keeps track of calls to update contentions and lifecycle statuses per claim.

The controller for PUT /claims/{claimId}/contentions directly updates the state for contentions per claim and the controller for PUT /claims/{claimId}/lifecycle_status directly updates the state for lifecycle statuses per claim. The states are stored in a simple Set object that holds the updated claim ids.

To make the states accessible to End to End Tests which are black-box tests that run outside of the docker compose, this mock server provides two end-points

GET /updates/{claimId}/contentions
GET /updates/{claimId}/lifecycle_status

These end-points return a simple json objects of type ContentionUpdatesResponse and LifecycleUpdatesResponse. Both types provides a boolean property found which can be used to check if updates to contentions or lifecycle statuses per claim has happened. Contentions and Lifecycle status objects are also provided if found is true.

The updates are stored in a simple Set for claim ids. There are two consequences for this

Same claim ids should not be used for multiple End to End Tests
During local development if you run an End to End Test without deployment multiple times, you might get false positives

To remedy the first issue, End to End Tests include a self-check test to make sure the same claim id is not used by multiple tests. The second issue is remedied by a DELETE /updates/{claimId} end point which resets the update flags per claim id. This end point is called during the start of each End to End Test.

As with the actual mock end-points, updates end-points are available to End to End Tests and to local development through http port 8099."
Mock BIP Claim Evidence API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Mock-BIP-Claim-Evidence-API,"The BIP APIs are not available outside of VA firewall for local development. There is a UAT BIP Claim Evidence API server within VA firewall but using it currently requires

Developing on GFE's which have typically not enough resources for projects of VRO size
Installing and maintaining of Java requires special permissions
Other security requirements makes it difficult to test https
Since the server is not controlled by the VRO team creating and maintaining claims for test cases is difficult

Due to these difficulties, a Mock BIP Claim Evidence API has been developed. This mock server was instrumental both testing of mTLS functionality and upload functionality. It is also now an integral part of the End to End Tests.

HTTPS Server

Mock Claim Evidence API is an HTTPS enabled Spring Boot server. It uses self-signed certificates generated by the build-certificates.sh. In particular the PKCS#12 files client_truststore.p12 and server_keystore.p12 are placed in resources directory and pointed directly in the application.yml server.ssl property to set up an https port 8094.

Note that if you generate a new set of self-signed certificates using build-certificates.sh and change it in Mock BIP Claim Evidence API, the corresponding certificates in Application and Mock BIP Claims API subprojects must change as well.

Health Check

The standard Spring Boot Health Check is provided in http port 8080 and configured in application.yml management property.

HTTP Server

One of the primary use of the Mock Claim Evidence API server is the End to End Tests. Since these are black-box tests that do not have access to https setup within the VRO application, it is involved to have these tests to use the https port. To simplify the End to End Tests an http port 8096 is made available. The full set of end-points are available through this port.

The implementation of opening the http port 8096 can be found here.

Open API Specification

Mock BIP Claim Evidence API mocks the BIP Claim Evidence API end points that are used by the VRO application. These end-points are listed in BIP Claim Evidence API API Calls section and and also defined in Mock BIP Claim Evidence API specification. In fact the initial version of the api specification is generated using the Open API Generator

npm install @openapitools/openapi-generator-cli -g 
npx @openapitools/openapi-generator-cli generate -i claimevidence_1.1.1_openapi.json -g spring -o code

The Open API Specification claimevidence_1.1.1_openapi.json is available from the BIP Claim Evidence API Swagger page by clicking on the small /v3/api-docs link under the title.

The initial version of the api specification has been changed very little for the end-points used by the VRO.

Security Requirements

This mock implements a JWT based security configuration. At this time not all the claims are checked. However signature is validated in a JWT Request Filter.

A JWT generator is included together with an interface to specify claims. This interface is implemented in this class whose property values is read from application.properties mock-bip-ce-api.jwt setting.

For testing purpose you can update the application-test.yml with development, prod or production JWT claim values and run a test such as Files Test and JWT will be available in the logs. In principle you can use that JWT in curl or other tools to make API calls if you specify the certificates as well.

Future Work

The JWT generator implementation is mainly used in unit tests. It could be merged with the VRO implementation since it is a bit cleaner and more modular.

Storage

This mock uses a simple Map based store to hold the file content and meta data from request and UUID from response. This Basic Store is initialized as a bean in Application Configuration.

Received Files

To simplify to get access to the generated files from End to End Tests which are black-box tests that run outside of the docker compose, this mock server provides an additional end-point GET /received-files/{fileNumber} to download the generated files.

Note that the storage Map uses Filenumber as a key so if you call the /files twice, the previous file will be overwritten."
Local Setup ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Local-Setup,"If you are on a Windows machine, take a look at Setup on Windows first.

IDE
Required Dependencies
Directory setup
Software Packages
GitHub Permissions
Running the Application
Verifying the Application is Running
Running app container locally
IDE

VRO is architected to be language agnostic though you should always confirm with maintainers that a language is supported before using it.

If a language is supported, you should feel free to use your preferred IDE for your language. However, it is recommended to use an IDE which supports an integration with Gradle (VRO's build system), like IntelliJ (Community Edition):

https://www.jetbrains.com/help/idea/installation-guide.html
https://www.jetbrains.com/products/compare/?product=idea-ce&product=idea.
Required Dependencies
Directory setup

Make sure you have both abd-vro and abd-vro-dev-secrets cloned in sibling directories.

path/to/code/
         |-----abd-vro/
         |-----abd-vro-dev-secrets/

Software Packages

See Ubuntu VM Setup for detailed software setup instructions on Ubuntu.

Before you run this application locally, you will need to make sure you have all the following required dependencies available in your local environment:

Java 17 (Mac Guide | Other OS Guide)
Gradle 8.1.1. Better yet, use ./gradlew
docker (ensure docker-compose version 2.X)

Note for Apple Silicon Macs: export DOCKER_DEFAULT_PLATFORM=linux/amd64

hadolint
spectral
shellcheck

LHDI has additional setup documentation for Mac OS and Other Operating Systems.

GitHub Permissions

This should no longer be needed:

Create a GitHub personal access token with read:packages permission.
Add the newly created token to your shell
# add this to your shell profile (e.g. ~/.zprofile)
export GITHUB_ACCESS_TOKEN=<replace-with-token-from-github>
This is required to in order to download application artifacts that are published to the VA GitHub Package Registry.

You should also be added to the VA-ABD-RRD GitHub team.

Running the Application

You'll need port 5432 (default for postgres) available/not-in-use when running the code.

Also, be sure that you have a docker daemon running on your machine. If you use Docker Desktop, having the application running will run the daemon. However, if you are using a different Docker runtime like Colima, you will need to manually start the daemon process configure it to always run at boot time.

Once you have all the required dependencies, you can start the application in your local environment by navigating to the root of your application directory and running the following command:

./gradlew clean
./gradlew build check docker

This will build all the application artifacts and docker images.

Note: Due to the way Gradle computes dependencies, the clean command must always be separate from build commands

You can then start the application by running (more details at Docker Compose):

./gradlew :dockerComposeUp
./gradlew :app:dockerComposeUp

Note: Make sure you have completed the instructions in the abd-vro-dev-secrets README

This should bring up a docker container with the app running at http://localhost:8111

Verifying the Application is Running

You can verify that the application is up and running by issuing the following commands in your terminal:

curl http://localhost:8111/actuator/health
curl http://localhost:8111/actuator/info

You should get back responses similar to the following:

curl http://localhost:8111/actuator/health

{
    ""status"":""UP"",
    ""components"":{
        ""db"":{
            ""status"":""UP"",
            ""details"":{
                ""database"":""PostgreSQL"",
                ""validationQuery"":""isValid()""
            }
        },
        ""diskSpace"":{
            ""status"":""UP"",
            ""details"":{
                ""total"":62725623808,
                ""free"":53279326208,
                ""threshold"":10485760,
                ""exists"":true
            }
        },
        ""livenessState"":{
            ""status"":""UP""
        },
        ""ping"":{
            ""status"":""UP""
        },
        ""readinessState"":{
            ""status"":""UP""
        }
    },
    ""groups"":[
        ""liveness"",
        ""readiness""
    ]
}
curl http://localhost:8111/actuator/info

{
    ""app"": {
        ""description"": ""Java API Starter from Template"",
        ""name"": ""abd_vro""
    }
}
Running app container locally

The above instructions should give you enough to get all the necessary docker containers running to test new functionality in abd-vro. However, developers may wish to run the vro-app-1 container with a run configuration to more quickly iterate on changes being made as opposed to performing a full ./gradlew dockerComposeUp every time.

Refer to Docker Compose
Find instructions for setting up a jetbrains run configuration for vro-app-1 HERE.
Tips

Refer to Running Kafka Services/Tests Locally for information on how to run the bie-kafka microservice(s), specifically helpful for when there are issues with our messing system.

RabbitMq occasionally segfaults during startup if using apple silicon. Restarting the container will resolve this issue.
Check if the svc-bie-kafka docker container is running and causing failures in mock-bie-kafka:dockerComposeUp, if not restart it.
WIP

Breakdown of RabbitMq Messaging, Exchanges, queues, and routes"
Lightkeeper tool ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Lightkeeper-tool,"Prerequisite: You must be a member of the VA-ABD-RRD GitHub team

Followed Getting Started instructions and GETTING-STARTED page to install lightkeeper.

Development Environment Step:
Execute below command on a GFE (a machine within the VA network).
  lightkeeper create clusterconfig nonprod > kube_config
Transfer kube_config from GFE to development laptop (where kubectl is installed) and save it as ~/.kube/config.

NOTE:- this config is for the `nonprod` cluster (containing `dev`, `qa`, and `sandbox` only )

Execute below command for Prod cluster ( containing prod-test and prod),
lightkeeper create clusterconfig prod > kube_config` 
see Development environments#LHDI for cluster info.

NOTE:- If you're switching between the two clusters, remember to use the right config file.

Remember to specify the namespace (e.g., --namespace va-abd-rrd-dev) for all kubectl commands, e.g.:

`kubectl get pods --namespace va-abd-rrd-dev`

`helm list --namespace va-abd-rrd-dev`

Important

Kube config needs to be renewed every 90 days.

Be cautious when running commands as this may affect others using VRO in these environments, particularly in the prod-test and prod environments where PII/PHI and various secrets are visible.

Tip: For a nice GUI to monitor the status, try the Lens app.

Watch VRO logs

To follow the logs for the app container in the dev environment:

# Use the namespace corresponding to the dev environment
alias kc='kubectl -n va-abd-rrd-dev'

# List the pods; we want the vro-api pod with 7 containers in it
kc get pods
NAME                                 READY   STATUS    RESTARTS      AGE
vro-api-7ff6569c78-jj9zk             7/7     Running   2 (17m ago)   22m
vro-api-postgres-7776cbd54f-wkf8r    1/1     Running   0             22m
vro-api-rabbit-mq-7ff55bcb5f-6rg46   1/1     Running   0             22m
vro-api-redis-555446854-ksgjt        1/1     Running   0             22m

# Follow the logs for the abd-vro-api in the vro-api pod
kc logs -f vro-api-7ff6569c78-jj9zk -c abd-vro-api

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v2.7.4)

2022-11-18 18:47:11.835  INFO 1 --- [           main] gov.va.vro.VroApplication                : Starting VroApplication using Java 17.0.5 on vro-api-7ff6569c78-jj9zk with PID 1 (/project/vro-app.jar started by docker in /project)
2022-11-18 18:47:11.839  INFO 1 --- [           main] gov.va.vro.VroApplication                : The following 2 profiles are active: ""compose"", ""dev""
2022-11-18 18:47:15.525  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Multiple Spring Data modules found, entering strict repository configuration mode
Machine user login (DevOPS - Step only ):

IMPORTANT:- This step is only for Automated GH workflows and GH Actions. Not for Development use.

Use this step to generate kubernetes config for both prod and non-prod clusters

Execute .\lighkeeper login
Follow the OTP Url generated by lightkeeper in a browser
Follow instruction provided in section OTP https://github.com/department-of-veterans-affairs/abd-vro/wiki/Machine-User-Account to log in to GitHub
Generate *_KUBE_CONFIG secrets for GitHub Action workflows by executing
lightkeeper create clusterconfig nonprod | base64 > DEV_KUBE_CONFIG
lightkeeper create clusterconfig prod | base64 > PROD_KUBE_CONFIG

Update DEV_KUBE_CONFIG and PROD_KUBE_CONFIG in GitHub secrets in git GH Action secrets

Then test deploying to LHDI.

IMPORTANT: Since the config expires in 90 days, the DEV_KUBE_CONFIG and PROD_KUBE_CONFIG in git GH Action secrets need to be updated regularly so that GitHub Action workflows that interact with LHDI continue to work."
Machine User Account ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Machine-User-Account,"The machine-user account for VRO is named abd-vro-machine. This machine-user account has been added to the VA's GitHub organization. (The only concern is the VA may kick this account out since there's no va.gov email address associated with it. The account is still active.)

This username will be seen in some GH Action workflow runs, e.g., for SecRel. This account is also used for generating kube configs.

PATs

The account as several PATs (personal access tokens):

to comment on ticket 28 to keep the machine-user account active -- ticket #28 and PR #762
for Dependabot PR checks so that the code can be built by Dependabot's PRs
for pulling and deleting GHCR images
Email account

The account is associated with an outlook.com email address with username abd-vro. The credentials for the email and GitHub accounts is stored in LHDI's HashiCorp Vault -- see Secrets Vault.

OTP (One-Time Password)

In the github/abd-vro-machine-user folder of Secrets Vault, there is a 2FA code secret. Use that secret in an OTP Generator like https://it-tools.tech/otp-generator to generate the OTP number required to log into the GitHub abd-vro-machine-user account."
Lighthouse APIs ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Lighthouse-APIs,"We prefer to use Lighthouse APIs when possible. It usually provides the quickest path to production since approvals are minimal, it builds on existing working code, and there are fewer unknowns.

Some APIs need CCG (machine-to-machine auth) to be enabled. Such features should be requested sooner rather than later.

Of particular relevance to VRO:

Patient Health API (FHIR) to get health evidence to support fast-tracking a claim
Benefits Claims API to get claim data; details
Veteran Verification APIs doesn't return veteran information like SSN, name, DOB. It will need to be expanded so VRO can get veteran information to generate a PDF; additionally, VRO needs the veteran's ICN -- see the MPI section below

Useful resources

API Backend Systems - backing VA system(s) for each Lighthouse API
Patient Health API

Pertinent Secrets

VRO_SECRETS_LH

LH_ACCESS_CLIENT_ID
LH_PRIVATE_KEY_BASE64"
LHDI's Boilerplate Instructions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/LHDI's-Boilerplate-Instructions,"VRO software is deployed on the Lighthouse Delivery Infrastructure (LHDI) platform, which offers tools, services, and support team.

LHDI's Java Starter Kit was used to populate this codebase (see PR #8) using Java (AdoptOpenJDK) 17 and Gradle 7.4.

LHDI's Boilerplate Setup Instructions
About
CI/CD Pipeline
Secrets
Default Pipeline Behavior
Dependencies
Deploying the Application
What's Next
About

This is a Java Spring Boot application that has been created using the Lighthouse DI Java 17 Starterkit. It is intended to be used as a starting point for building Java APIs and should be customized to deliver whatever functionality is required. If no other changes have been made, this application will have these features included by default.

CI/CD Pipeline

This project comes with a skeleton Github Actions CI/CD pipeline out of the box. You can always choose to rewrite the pipeline using a different CI/CD tool; this pipeline serves as an example that you can use and run with minimal setup.

Secrets

In order to run the pipeline, you will need to create a personal access token and add it to your repository's secrets in Github. The access token should have write:packages scope.

The secrets you need to configure are

ACCESS_TOKEN: the personal access token
USERNAME: the Github username of the user who owns the access token
Default Pipeline Behavior

The default pipeline has 3 jobs, which do the following things:

Runs CIS benchmark tests against the application Docker image using docker-bench-security
Builds and tests application
Publishes Docker image to VA GHCR repository
Dependencies

The pipeline runs on Github's ubuntu-latest runner, which is currently Ubuntu 20.04. The Github Actions Ubuntu 20.04 documentation lists the software installed by default. To learn more about choosing a Github runner and Github-hosted runner types, see the job.<job-id>.runs-on documentation.

Software required for the pipeline but not installed by default, such as Java 17, hadolint, and spectral, is installed in the pipeline. The installation for app build dependencies is implemented as an action in <./.github/actions/setup-pipeline/action.yml>.

Deploying the Application

The pipeline does not currently deploy the application to the DI Kubernetes clusters out of the box, although this setup will be coming in the future. To learn how to deploy your applications, see the DI ArgoCD docs.

Common Errors

Error: Cannot find plugin

Error Message:

* What went wrong:
Plugin [id: 'gov.va.starter.plugin.cookiecutter', version: '0.1.20', apply: false] was not found in any of the following sources:

- Gradle Core Plugins (plugin is not in 'org.gradle' namespace)
- Plugin Repositories (could not resolve plugin artifact 'gov.va.starter.plugin.cookiecutter:gov.va.starter.plugin.cookiecutter.gradle.plugin:0.1.20')
Searched in the following repositories:
    MavenLocal(file:/Users/aasare/.m2/repository/)
    Gradle Central Plugin Repository
    MavenRepo
    BintrayJCenter
    maven(https://palantir.bintray.com/releases)
    maven2(https://dl.bintray.com/adesso/junit-insights)
    starterBootPkgs(https://maven.pkg.github.com/department-of-veterans-affairs/lighthouse-di-starter-boot)
    nexus(https://tools.health.dev-developer.va.gov/nexus)


Fix: Set your Github token as per the instructions in the Required Dependencies section above.

Error: Failed to get resource

Error Message:

Failed to get resource: GET. [HTTP HTTP/1.1 401 Unauthorized: https://maven.pkg.github.com/department-of-veterans-affairs/lighthouse-di-starter-boot/starter/java/build-utils-property-conventions/starter.java.build-utils-property-conventions.gradle.plugin/0.1.32/starter.java.build-utils-property-conventions.gradle.plugin-0.1.32.pom)]


Fix: Set your Github token as per the instructions in the Required Dependencies section above, ensure that read:packages is true.

What's Next

Once you have verified that you are able to run the application successfully, you can now start customizing the application to deliver the functionality you would like.

By default, this application assumes the use of a build, test, release cycle as defined in this development guide. Take a look at that guide to see how you can make changes, test them and get them deployed to a target environment.

The application itself is organized into the following three tiers of functionality:

API
Service (business logic)
Persistence

To see how each of these tiers is used by default, take a look at the Project Structure documentation."
Kubernetes clusters ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Kubernetes-clusters,"EKS/Kubernetes clusters

From LHDI kubernetes docs, clusters are:

ldx-nonprod-1 (DEV, SANDBOX, QA)
ldx-prod-1 (PROD)
ldx-preview-1 (PREVIEW)

Associated gateways

ldx-nonprod (NONPROD)
ldx-nonprod-1 (NONPROD1)
ldx-dev (DEV)
ldx-prod-1 (PROD1)
ldx-preview-1 (PREVIEW1) (no gateway)
ldx-mapi-1 (MAPI-1)

DNS:

dev.lighthouse.va.gov
sandbox.lighthouse.va.gov
qa.lighthouse.va.gov
(PROD) api.lighthouse.va.gov

Output from lightkeeper list team va-abd-rrd:

{
  ""name"": ""va-abd-rrd"",
  ""id"": ""..."",
  ""description"": ""va-abd-rrd description"",
  ""namespaces"": [
    {
      ""name"": ""va-abd-rrd-prod"",
      ""cluster"": ""ldx-prod-1"",
      ""cpu"": 128,
      ""ram"": ""256G"",
      ""provided"": true,
      ""imageSigningEnabled"": true
    },
    {
      ""name"": ""va-abd-rrd-qa"",
      ""cluster"": ""ldx-nonprod-1"",
      ""cpu"": 128,
      ""ram"": ""256G"",
      ""provided"": true,
      ""imageSigningEnabled"": false
    },
    {
      ""name"": ""va-abd-rrd-sandbox"",
      ""cluster"": ""ldx-nonprod-1"",
      ""cpu"": 128,
      ""ram"": ""256G"",
      ""provided"": true,
      ""imageSigningEnabled"": true
    },
    {
      ""name"": ""va-abd-rrd-dev"",
      ""cluster"": ""ldx-nonprod-1"",
      ""cpu"": 128,
      ""ram"": ""256G"",
      ""provided"": true,
      ""imageSigningEnabled"": false
    }
  ],
  ""scm"": null,
  ""artifact_store"": null,
  ""pipeline"": null,
  ""registry"": null,
  ""reporting"": null,
  ""observer"": null
}
S3, RDS, Gateways

See:

LHDI's AWS Infrastructure as a Service
LHDI - Managed Customer Resources"
Jetbrains springBoot run configuration setup ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Jetbrains-springBoot-run-configuration-setup,"Run Configuration

WIP

What?

abd-vro system involves running numerous docker containers, which can be costly in terms of build time as well as system memory.

This guide serves as a set of instructions to run the vro-app-1 container as a run configuration for IntelliJ ide.

This allows developers to more quickly spin down and spin up new changes when testing the code.

Setup
create this directory on your system (via Terminal.app or in the terminal inside your IDE)
PERSIST_TRACKING_FOLDER=/tmp/persist/tracking
mkdir -p $PERSIST_TRACKING_FOLDER

In IntelliJ ide, navigate to top right -> edit configurations
add new run configuration for spring boot
name it whatever you want
under environment variables set the following (comma separated)
PERSIST_TRACKING_FOLDER=/tmp/persist/tracking
POSTGRES_URL=jdbc:postgresql://localhost:5432/vro
REDIS_PLACEHOLDERS_HOST=localhost
RABBITMQ_PLACEHOLDERS_HOST=localhost
BIP_CLAIM_URL=localhost:8097
BIP_EVIDENCE_URL=localhost:8094
SLACK_EXCEPTION_WEBHOOK=http://localhost:9008/slack-messages

Configuration should look something like this
make sure you have all the docker containers running already (or build and run ./gradlew :app:dockerComposeUp now)
more details on running locally can be found HERE.
manually stop the vro-app-1 container, which we will run through our IntelliJ configuration
docker stop vro-app-1

now run it from the IDE w/ the big green play button"
iMVP claim and contention update scenarios ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/iMVP-claim-and-contention-update-scenarios,"Scope

Single issue NEW and INCREASE hypertension claims

Updates handled by MAS

Before VRO is notified of an automation eligible claim, MAS makes the following automation-specific updates to the claim/contention:

Claim:

Temporary Station of Jurisdiction = 398

Contention:

Special Issue = Rating Decision Review - Level 1 (RDR1)
Special Issue = RRD
Automation Indicator = False
VRO scenarios that require updating the claim/contention

Happy path

In order for an automated claim to be routed and tracked appropriately after processing has completed in VRO, the claim/contention needs to be updated:

Field (object)	Temporary Station of Jurisdiction (claim)	Lifecycle Status (claim)	Automation Indicator (contention)	Lifecycle Status (contention)	Special Issues (contention)
Field value when claim is RFD	398	RFD	True	RFD	RRD (remove RDR1)
Field value when exam is ordered	398	Open	True	Open	RRD (remove RDR1)

Offramp (for any reason)

In order for an automation eligible claim to be processed manually when offramped by VRO, updates made to the claim/contention as part of automation processing need to be reverted:

Field (object)	Temporary Station of Jurisdiction (claim)	Lifecycle Status (claim)	Automation Indicator (contention)	Lifecycle Status (contention)	Special Issues (contention)
Field value for any offramped claim	398	Open	False	Open	Do NOT include Rating Decision Review - Level 1 (RDR1) or RRD

"
Helm Charts ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Helm-Charts,"VRO uses Helm Charts to specify a deployment to LHDI. The charts are located in the helm folder.

GitHub Action workflows call helm to deploy to various LHDI environments -- see CI CD Workflows.

Configurable Settings

The values.yaml contains the default settings for the dev environment. Other values-for-*.yaml files override some settings in the values.yaml. For example, values-for-prod.yaml overrides the size of the persistent volumes for the prod environment.

Before adding a Helm configuration, read Configuration-settings#guidelines-for-placing-settings.

The values are referenced in charts and the _helper.tpl, which is used by charts to reduce repetition and consolidate common settings into a single location. All containers share a common set of configuration settings, so changes can be consistently and easily applied to all containers.

Additionally, each chart has it's own values.yaml file, which can be overridden by using the setting global.<chartName>.someSetting, where <chartName> corresponds to the name within the corresponding Chart.yaml file. For example, overriding the value of global.rabbitmq-chart.imageTag would change the imageTag value used in the rabbitmq subchart.

See the Update deployment GitHub Action workflow for an example of how settings are overridden. Using this mechanism, sets of containers can be enabled and disabled.

Persistent Volumes

LHDI offers 2 types of persistent volumes (PVs):

EFS
pros: allows mounting from any container; any container user can write to it but all files are owned by the same user (as determined by LHDI when the PV is provisioned)
cons: unmodifiable folder ownership (Slack)
EBS
pros: allows changing folder ownership
cons: can only be mounted by containers in the same Kubernetes node; only supports ReadWriteOnce, which implies:

""the volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access the volume when the pods are running on the same node.""

VRO creates 2 PVs for different purposes:

tracking: EFS volume mounted by the rabbitmq and app containers to tracking incoming requests for diagnostics and recovery
postgres data (pgdata): EBS volume mounted by the postgres container to retain DB data between redeploys. The postgres container requires that the data folder be owned by the postgres user, hence EFS cannot be used.

The VRO Console mounts these PVs for diagnostics.

We do not have visibility into the PVs themselves -- LHDI manages the PVs. We must specify persistent volume claims (PVCs) in order to get PVs.

Resources

https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
https://www.datree.io/resources/kubernetes-troubleshooting-fixing-persistentvolumeclaims-error
StatefulSet

Since the pgdata is an EBS volume, the postgres and console containers must run in the same node as the EBS. To satisfy this constraint, a StatefulSet is created for the postgres container and a podAffinity is applied to the console container.

When updating a deployment (using helm upgrade), StatefulSets causes an error. A workaround is implemented to avoid this error.

Secrets

See Secrets Vault"
Health data assessment containers ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Health-data-assessment-containers,"Health data assessment containers

The code for health data assessment and analysis is in the python-service folder. Each folder is designed to run inside its own docker container and communicate with other parts of the application using RabbitMQ. In general, the assessment services loop through a Veterans health data (medications, observation, conditions, procedures etc) and pull out anything relevant to a claimed contention (""relevant"" in this case means anything that may help an RVSR rate a claim). The assessment services also provide summary statistics as meta-data that gets stored in the DB. Refer to Plan to Deploy for more general information on ""Queue-Processor"" architecture.

Assessment of health data for hypertension

This container handles all claims with contentions for hypertension (7101 VASRD). There are two queues set up by this container, health-assess.hypertension for v1 functionality and health-sufficiency-assess.hypertension for v2. The queue is named this way because the v2 queue will return a flag to describe the sufficiency of evidence for a given claim (the logic is described in 905 ). The health-sufficiency-assess.hypertension queue leads to a function that is designed to handle combined data from MAS and Lighthouse in a robust fashion. The assessment service does formatting of dates for the PDF, since it is already parsing a date as part of its core functionality. Medications, blood pressure readings and conditions are sorted by date before being returned as an evidence object. Blood pressure readings without a parse-able date are not considered algorithmically because there is no feasible way of determining whether they meet the date requirements (without a lot of error-prone custom string parsing, and the vast majority of data that will be flowing through the VRO will have parse-able dates). Medications and conditions without a date are attached at the end of the sorted list of objects with dates.

Health Data validation requirements

Some ""keys"" in the request message are validated before any algorithms are run to alleviate the need to constantly catch errors. Some data cleaning is assumed by the upstream services for data collection. To accommodate MAS and Lighthouse as data sources, most dates are not assumed to always be in the correct format. For example some primary dates from MAS could be missing from OCR data and instead a secondary, or ""partial"" date will be used. For the hypertension queues, ""bp_readings"" is required to be present and for each blood pressure reading, a diastolic and systolic value must be present. The cerberus Python package is used as a lightweight validator.

Sufficiency to set RRD status

To determine sufficiency a collection of algorithms analysis the patient data to determine a few decision points. The patient health data will include diagnosis information as code-able concepts in ICD or SNOMED. The ICD 10 codes {""I10"", ""401.0"", ""401.1"", ""401.9""} are used to filter the complete patient record down to just the relevant objects. In addition, blood pressure measurements are filtered by date and value.

More information on the logic to determine sufficiency can be found here 905 )

Assessment of health data for asthma

This container handles claims with contentions for asthma (6602 VASRD). On startup it creates a queue, health-assess.asthma which is only used by v1 endpoints.

All other assessment folders

The other assessment folders are included in a gradle build if it is given a special property and the docker-compose profile is set to prototype. Use the following commands to build and run those containers.

./gradlew -PenablePrototype build check docker

export COMPOSE_PROFILES=prototype

Testing and Development

There is unit testing for each assessment service in service-python/tests/assessclaim.

To test additional conditions, the VASRD code needs to be added to the dpToDomains object in svc-lighthouse-api/src/main/java/gov/va/vro/abddataaccess/service/FhirClient.java. The dpToDomains map determines which resources to pull from the Lighthouse Patient Health API and later sent to the assessment services."
Gradle ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Gradle,"Gradle is used to specify Java dependencies, lint code, compile code, build Docker images, etc.

There's lots of tutorials online. Here are some tips:

https://www.youtube.com/playlist?list=PL0UJI1nZ56yAHv9H9kZA6vat4N1kSRGis
https://www.youtube.com/watch?v=5v92GbC9cqo
Bash alias

Since it's used frequently, a Bash alias is useful (alias g='$PWD/gradlew'), which reduces typing g build. (The $PWD enables g to be run from any folder.)

Additional aliases can be created:

alias g=""$PWD/gradlew""
alias delint='g spotlessApply'
alias gbuild='delint && g build'
alias gdocker='g build && g docker'
alias gcdocker='g clean && gdocker'

alias gdcDown='g dockerComposeDown'
alias gdcUp='g dockerComposeUp'
alias gdcRestart='g dockerComposeDown dockerComposeUp'
Important Files to Review
settings.gradle: includes subdirectories as Gradle subprojects
build.gradle in the project root, api, and app subdirectories
buildSrc subdirectory: LHDI Starter Kit provides Gradle plugins; details in the README.md.
Gradle library dependencies

To see the current dependencies between subprojects, navigate to the root of the repo and run: ./gradlew :<GRADLE_PROJECT_NAME>:dependencies --configuration compileClasspath | grep ""project :""

To see all (including 3rd-party) of the dependencies for a gradle subproject run: ./gradlew :<GRADLE_PROJECT_NAME>:dependencies --configuration compileClasspath > deps.txt

This command will print the dependency tree for the supplied subproject to the deps.txt file.

Gradle tasks
./gradlew tasks
./gradlew tasks --all

https://gitlab.com/barfuin/gradle-taskinfo:

./gradlew tiTree assemble
./gradlew tiOrder docker
Jacoco Test coverage report

Jacoco is used to report test coverage for Java -- see https://rhamedy.medium.com/how-to-setup-jacoco-code-coverage-with-maven-gradle-76e0b2fca9fb.

At the root project folder:

./gradlew test jacocoTestReport


Then open build/reports/jacoco/jacocoAggregatedReport/html/index.html in a browser.

For a subproject, for example:

cd svc-bip-api/
./gradlew test jacocoTestReport


Then open build/jacoco/jacocoAggregatedReport/html/index.html.

buildSrc

If you're looking for a buildSrc folder, use the gradle-plugins folder instead -- it functions equivalently but is reusable and results in faster builds.

Composite Builds

The codebase use Composite Builds to partition and decouple Gradle projects, resulting in faster builds.

The current composite builds are:

gradle-plugins: only used for building the code; this is included by other Gradle projects
mocks: for projects that represent external APIs or services
(root): the main VRO codebase

A composite build is built independently unless it is referenced by an includeBuild in settings.gradle. For example, since gradle-plugins is included by other Gradle projects, it is re-built automatically when it changes. However, mocks will not be rebuilt unless the Gradle build task is explicitly run against mocks folder ./gradlew -p mocks build or:

cd mocks
./gradlew build


Since mocks rarely need to be rebuilt (and are not integral to the deployed VRO), this will reduce build time."
Github Actions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Github-Actions,"GitHub Actions are defined in the .github/workflows folder, and their executions are in the Actions tab.

Build and Publish

No longer required: GitHub actions that build VRO code or publish VRO images require credentials of someone in the VA-ABD-RRD Team.

This was replaced in #925: @yoomlam added his USERNAME and ACCESS_TOKEN secrets to the repo's Secrets settings
When publishing, resulting packages (i.e., images for Docker containers) are tagged using the first 7 characters of the PR's commit hash, which are needed for deployment to LHDI's Kubernetes clusters
CodeQL

The VA GitHub.com team requested that Advanced Security Code Scanning be enabled via codeql.yml. See PR #116 for details.

This is a compliment to any tools and security procedures your team is already performing rather than a replacement. ... this feature will identify potential security issues on any new pull requests. ... For more information, check out the GitHub Handbook.

(If needed, an admin for this repo can bypass any identified issues.)

In commit ea4be65, we limit this action from running to certain PR event types (e.g., ready_for_review, review_requested) and for pushes to special branches since this action is slow. To manually run the action, do any of the following:

Request a review from someone.
Convert it to a draft PR, then click the ""Ready for Review"" button.
Go to the CodeQL Action and click ""Run workflow"" on the desired branch.
Merge the PR to a special branch (develop).
Mirror

The Lighthouse SecRel (Secure Release) process operates only on non-public repos. To satisfy that requirement, a abd-vro-internal repo was created as a mirror of this repo. The internal repo will only be used by the Lighthouse SecRel team to enable deployments to prod -- no one should be committing to or creating PRs directly in that repo. The mirror.yml GitHub Action updates the internal repo whenever a branch in this repo is created, updated, or deleted.

@yoomlam generated a password-less ssh key pairs (using bogus email mirror@abd-vro.va.gov) and added a SSH_PRIVATE_KEY secret to the repo's Secrets settings
In order for the GitHub Action to access the abd-vro-internal repo via ssh, the public key was added to Deploy keys using the abd-vro-machine account, which will attribute triggered actions (e.g., SecRel) in the abd-vro-internal repo to the machine-user account.

PR #680 makes the following unnecessary, but keeping it for reference:

PR #250 limits this action from running to certain PR event types (e.g., ready_for_review, review_requested) and for pushes to special branches since this action is causes extraneous checks to run in the internal repo. To manually run the action, do any of the following:

Request a review from someone.
Convert it to a draft PR, then click the ""Ready for Review"" button.
Go to the Mirror Action and click ""Run workflow"" on any branch (all branches will be mirrored)
or run curl -XPOST -u ""$GITHUB_USERNAME:$GITHUB_ACCESS_TOKEN"" -H ""Accept: application/vnd.github+json"" -H ""Content-Type: application/json"" https://api.github.com/repos/department-of-veterans-affairs/abd-vro/actions/workflows/mirror.yml/dispatches --data ""{\""ref\"": \""develop\""}""
or run gh workflow run mirror.yml --ref develop
Merge the PR to a special branch (develop).
SecRel

PR Enable SecRel workflow #235 adds new actions (secrel.yml and aqua-checker.yml) for VRO's code to go through Lighthouse's SecRel pipeline to be deployable to production -- see Secure Release GitHub Actions for details.

The ""SecRel workflow"" action will be automatically triggered in the internal repo by pushes to develop and main.

To manually trigger the SecRel workflow on a PR, see To test PRs in the SecRel pipeline."
End to End Tests ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/End-to-End-Tests,"End-to-end Tests are black-box integration tests.

External systems are primarily simulated using Mock Services. The main reason to use Mock Services instead of the test servers of the external systems are:

Data in test servers of the external systems do no line up. For example a claim id from MAS API development server cannot be found in the UAT server of BIP Claims API.
VRO team does not have control over test data on external test servers.
BIP Claims API and BIP Claim Evidence API do not have servers that are accessible outside of VA.
Interactions are more easily recorded in Mock Services for verification.
Implementation

End-to-end Tests require almost all VRO containers specified in the VRO docker-compose file to be running. Only those marked as prototype can be ignored.

An environment setting script is provided. You must run this script before building the project.

source setenv-e2e-test.sh

Once the environment is set you can build VRO and run the containers as usual.

End-to-end Tests are configured as Integration Tests as described here. After all the containers are run using ./gradlew :app:dockerComposeUp, you can run the end-to-end test task:

./gradlew :app:end2endTest

This will run all the end-to-end tests and report success or provide an html link for errors.

Action

End-to-tests are automatically run when a PR is merged to develop or when a PR is assigned a reviewer. Automation is achieved using a Github action. This PR: End-to-end test for v2 action can also be run manually.

MAS Integration (V2 - iMVP) Tests

All iMVP End-to-end Tests are in the Java file VroV2Tests.java. The tests are implemented as JUnit 5 tests.

End-to-end Tests for iMVP aims to verify most, if not all, of Road Map for iMVP. There are two types of end-to-end tests: positive and negative. Positive tests verify the paths in the Road Map for iMVP when they are arrive at completion as opposed to negative tests that verify error cases where any of the paths are interrupted due to errors.

Positive Tests

Each positive End-to-end Test is driven by a JSON file in the resource directory test-mas. Each JSON file is the mocked payload to the VRO /automatedClaim end-point which is the primary interface to MAS for iMVP.

Critical fields in each JSON file are

collectionId: the id of the collection for which annotations are retrieved. The collection can either exist in the MAS development server (currently 350 only) or be available from the Mock Mas API.
veteranIdentifiers.icn: the ICN for the patient. The patient can either exist in the Lighthouse Health API Sandbox server or be available from Mock Lighthouse Health API
claimDetail.benefitClaimId: the id for the claim. The claim has to be available from Mock Bip Claims API.
veteranIdentifiers.veteranFileId: the file id for the veteran. This id is used to generate and identify generated PDF's.

The tests calls to /automatedClaim end-point with the payload and verify the response and the expected events. The events that happen are queried using end-points in Mock Services or VRO Claim Metrics API.

The following are the primary verifications

Response code
PDF Generation: Mock BIP Claim Evidence API is queried to make sure the evidence PDF is generated when necessary. The generated PDF is downloaded from Mock BIP Claim Evidence API and existence of Veteran name and last name verified using a simple parse.
Exam Ordering: Mock MAS API is queried to make sure exam is ordered for the collection when needed.
Slack Messaging: Mock Slack Server is queried to make sure a slack message is sent when needed.
Claim Updates: Mock BIP Claims API is queried to make sure the claim and contention records are updated when necessary.
Database Updates: VRO Claim Metrics end-points are used to make sure database is populated as expected.

All positive End-to-end Tests resets the records in Mock Services for the collection under test to enable multiple running of the test during local development.

Negative Tests

Negative tests are for conditions that generate 4xx and 5xx status codes. All the negative tests are in VroV2Tests.java as well. The following are some of the examples:

Check if VRO is sanitizing the request by filtering disallowed characters such as those that are not printable.
Check if a non-existent claim id is validated.
API (V1) Tests

All API (V1) End-to-end Tests are in the Java file VroV1Tests.java. The tests are implemented as JUnit 5 tests.

End-to-end Tests for MVP aims to verify most, if not all, of Road Map for MVP. There are two types of end-to-end tests: positive and negative. Positive tests verify the paths in the Road Map for MVP when they are arrive at completion as opposed to negative tests that verify error cases where any of the paths are interrupted due to errors.

Positive Tests

There are two positive End-to-end Tests.

Both tests use existing records from Lighthouse Health API Sandbox server.

The first test is driven by the JSON files in the resource directory test-7101-01 in particular assessment.json which is the expected response from /full-health-data-assessment end-point for a hypertension case. The input payload for /full-health-data-assessment is prepared from assessment.json, the end-point is exercised, and the response is compared to assessment.json. The second step of the test exercises /evidence-pdf. The input payload is formed using data retrieved from the /full-health-data-assessment call and the file veteranInfo.json. The generated PDF file is sanity checked by comparing pdf data to fields in assessment.json and veteranInfo.json.

The second test is similar but verifies responses for an Asthma case. This test case is driven by the JSON file in the resource directory test-6602-01. Otherwise the methodolody is identical to the first test case and in fact both share majority of the test code.

Negative Tests

Negative tests are for conditions that generate 4xx and 5xx status codes. All the negative tests are in VroV1Tests.java as well. The following are some of the examples:

Check if authorization fails without an API key."
Entity Relationship Diagram (ERD) ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Entity-Relationship-Diagram-(ERD),
EE ‚Äê Build and Deploy Process ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/EE-%E2%80%90-Build-and-Deploy-Process,"Build and deploy EE to both prod and non-prod environments EE follows same CI/CD process as VRO. Detailed documentations can be found with below references

Reference: https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deployment-Overview https://github.com/department-of-veterans-affairs/abd-vro/wiki/CI-CD-Workflows https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deploying-VRO https://github.com/department-of-veterans-affairs/abd-vro/wiki/Container-Image-Versions

Deploy to Non-Prod

When the PR is merged to develop branch CI process gets triggered

Deployment to Non-Prod
Navigate to https://github.com/department-of-veterans-affairs/abd-vro-internal/actions/workflows/update-deployment-ee.yml
Select Domain-EE application, Select target env. Run the workflow
Deploy to Prod
- Prerequisite
Images has to be signed before deploying to Sandbox and higher environments
Secrel job has to succeed. If there is sec-rel issue then visit https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deploying-VRO#verify-and-fix-secrel, Follow steps listed in section Verify and Fix SecRel
Make sure you sync secrets from vault to cluster using https://github.com/department-of-veterans-affairs/abd-vro-internal/actions/workflows/deploy-secrets.yml
Follow Release process listed in https://github.com/department-of-veterans-affairs/abd-vro/wiki/Release-Process
Run workflow https://github.com/department-of-veterans-affairs/abd-vro-internal/actions/workflows/update-deployment-ee.yml, Select target environment and deploy
Verify Applications are up and running"
Docker containers ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Docker-containers,"Docker containers

The values.yaml file shows a list of container images that VRO creates and are ultimately deployed to Kubernetes.

Packages

(Packages in Github Container Registry)

Initially, there were 2 container images (a.k.a. ""packages"" in Github's lingo) set up by LHDI's Starterkit:

abd-vro/abd_vro-app
abd-vro/abd_vro-db-init
abd-vro/abd_vro-assessclaimdc7101
abd-vro/abd_vro-pdfgenerator
abd-vro/abd_vro-service-data-access

Currently, there are more packages associated with this repo. Packages are listed on the side panel of the VRO repo.

UPDATE: PR #65 makes it such that packages are automatically associated with this repo, but the package needs to be manually set to ""Inherit access from source repository"" as instructed by LHDI doc.

OBSOLETE: If more are created, go to the VA Organization's Packages page, search for the package, and manually connect them to this repo -- see LHDI Development Guide.
Docker.com rate limits

Due to Docker.com rate limits, PR #67 and PR #68 pulls container images (e.g., postgres and rabbitmq) from Docker.com (i.e., DockerHub, docker.io), then tags and pushes them to Github Container Registry (ghcr). LHDI has ideas to create mirrors of commonly used images, so that people can access those without getting rate limited (as well as being well-vetted images), however the timeline for that is unclear. Until then, we'll push them to ghcr so that LHDI can pull the images without any limit.

These packages (unchanged from Docker.com) need ""Inherit access from source repository"" to be set manually (instructions in LHDI Development Guide), and must be manually connected to this repo because we do not modify the Dockerfile LABEL (LABEL org.opencontainers.image.source=https://github.com/department-of-veterans-affairs/abd-vro), which would automatically associate the image with the repo.

Gradle Docker plugin

To build the container images, this project uses Palantir's Gradle Docker plugin -- https://github.com/palantir/gradle-docker.

Uses of container images

The following subsections describe uses of container images.

LHDI deployments

Used in LHDI's Kubernetes clusters, the images are retrieved and deployed to dev and separately deployed to production.

In the Kubernetes clusters, the app (or abd_vro-app) docker container depends on init containers (which run and exit before the app container is started):

container-init: init_pg.sql
db-init (or abd_vro-db-init): flyway DB migrations
opa-init: rego policies and permissions

As mentioned in the packages section above, the application-specific container images (abd_vro-app and abd_vro-db-init) are packaged, tagged, and pushed to Github Container Registry by VRO's Github Actions. The other (non-abd_vro) container images (such as pg-ready, istio-init, and istio-proxy) are provided by LHDI.

Pushed by Github Action

Used by publish.yml Github Action to push container images (""packages"") to the Github Container Registry -- see Github Actions.

Local development

Used to set up a local environment for code development and testing. The dockerComposeUp task in app/build.gradle starts Docker containers locally using your local code modifications. See Development process for details.

Approved images

When deploying an image from Docker, use Docker official images.

When defining a new image with a Dockerfile, use a base image from Docker official images.

Why? See LHDI docs. Either use a Docker official image, or have it scanned and signed in the Secure Release process by including it in .github/secrel/config.yml.

Adding a container for a new microservice
Add a new Gradle subproject
Include your new service inside of settings.gradle
Include your new service inside of build.gradle
For local development, add your container to app/docker-compose.yml so that your container is started when running VRO. This file has several helpers and placeholder vars to ensure consistency between services and containers (rabbitmq, redis, etc). Note the service and image names will be prefixed with svc and abd_vro respectively, e.g.:
svc-foo:
    image: va/abd_vro-foo:latest


After creating and updating the files above, run these commands:

./gradlew build check docker
# to start VRO
./gradlew :dockerComposeUp :app:dockerComposeUp


You should see Container docker-svc-foo-1 Started in the output of all containers starting up.

Adding a container for a new python microservice

To add a new python microservice with docker container, there are a few steps & files that need to be updated.

First, create the new microservice files inside of service-python. It's okay if these are stubbed to begin with, full functionality is not needed in order to create the container.

For a microservice named foo, you will need to add these files:

service-python/foo/__init__.py
service-python/foo/build.gradle
This Gradle file can be barebones, all that is needed is plugins { id 'local.python.container-service-conventions' } to re-use the common settings for all python services. Refer to the other service-python/*/build.gradle files.
service-python/foo/src/requirements.txt the dependency requirements for your service
service-python/foo/src/lib/ as the directory where your python code will actually live. This will have another __init__.py and likely a main.py, utils.py, settings.py, etc. as required by your services.
(Optional) service-python/foo/docker-compose.yml with the necessary services listed (rabbitmq, redis, etc)
service-python/Dockerfile will be used to create the container image for the service using files in the build/docker/ subfolder (populated after running ./gradlew service-python:foo:docker). Using a custom Dockerfile is possible but will require special Gradle and SecRel config.yml configurations.

Outside of your specific service, a few other files need to be updated:

Include your requirements in service-python/requirements.txt
Add your container to VRO configs -- see section above
Add your container to deployment configs -- see section below
Adding a container for deployment
Update scripts/image-names.sh:
add the folder's basename to IMAGES Bash array
add to various the bash functions's case statement if the folder is not at the project root-level
Run scripts/image-names.sh and review the changes
If changes look good, update the files manually or like so:
cp .github/secrel/config-updated.yml .github/secrel/config.yml
cp helmchart/values-updated.yaml helmchart/values.yaml

Add changes, including scripts/image_vars.src, and commit changes to git
Update helmchart/templates/api/deployment.yaml, which prescribes how VRO is to be deployed to LHDI
Commit all changes, push, and create a PR
Run the Deploy-Dev action on the PR's branch
Once the action completes:
Verify that the image (""package"") was pushed to GHCR: https://github.com/orgs/department-of-veterans-affairs/packages?tab=packages&q=dev_vro-. Click on your image and note there are 0 downloads.
The new container image needs to be manually set to ""Inherit access from source repository"" as instructed by LHDI doc.
Check for successful LHDI deployment to the DEV namespace by
Looking for non-zero downloads of your package. LHDI should have downloaded it.
Or connecting to LHDI's EKS cluster, e.g., using the Lens app"
Domain Applications in VRO ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Domain-Applications-in-VRO,"This page is a reference to pages for specific domains that are built on top of the VRO platform.

As new domains are onboarded to the VRO platform, application developers should create a new wiki page and link it here along with a short description. Code for a domain ABC can be found in the domain-ABC folder at the root of the repo.

xample - an example domain to illustrate proper folder and dependency structures
cc -"
Deprecated Pages ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deprecated-Pages,"VRO failure runbooks
(March 2022) Plan to Deploy to LHDI
CircleCI
MAS api spec (IBM hosted api)
MAS api spec (VRO hosted api)
PDF Generator
MAS Integration Camel Routes
External APIs (Partial)
Initial Roadmap for VRO's RRD Implementation
Amida Hand-off Resources
Problems being solved
POCs
Current Software State (Oct. 2022)"
Docker Compose ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Docker-Compose,"There are various other Docker setups depending on your needs, this page provides some guidance and typical setups to use during development. It also provides additional details for you to customize your own Docker setup, depending on your current development focus.

Docker Compose is used to define multi-container Docker setups for local development and integration tests. It sets up a Docker network, so that containers can readily connect using obvious hostnames rather than localhost. It also sets up shared volumes to simulate LHDI deployment environments.

Docker setups:

Platform Base setup (docker-compose.yml): includes Postgres, Redis, RabbitMQ, API Gateway. This creates Docker volumes and a network required by other setups.
App setup (app/docker-compose.yml): includes the Java-based VRO App and platform (domain-independent) microservices
Mocks setup (mocks/docker-compose.yml): defines containers that act as substitutes for External APIs and services; only used only for development and integration testing
Domain setups (domain-*/docker-compose.yml): each defines containers for their respective domain

For configurability, a docker-compose.yml file can use compose profiles to run a subset of defined containers. Examine the file to see what profiles are available for each Docker setup. The environment variable COMPOSE_PROFILES determines which containers are started by docker-compose (or docker compose). This allows developers to start only the containers they need for their current work.

For even further customization, create a local docker-compose.override.yml (docs) but do commit it to git.

The following instructions, assume the Docker container images have been built: ./gradlew docker

The Gradle tasks to start up and shut down the Docker setups are dockerComposeUp and dockerComposeDown. Alternatively, the equivalent docker compose command can be used instead.

Platform Base
Start up: ./gradlew :dockerComposeUp (Note the :, which will run the dockerComposeUp task in only the root Gradle project).
Equivalent to docker compose up -d
Check that all containers are healthy: docker ps -a
Manually test: visit the RabbitMQ Management UI at http://localhost:15672/
Shut down: ./gradlew :dockerComposeDown or ./gradlew :dockerComposeDown :dockerPruneVolume
Equivalent to docker compose down or docker compose down --volume (respectively)
Platform Base + API-Gateway

The Platform Base defines an API Gateway container but it is not needed for typical development, so it is not started by default. To start it, set COMPOSE_PROFILES:

Start up: COMPOSE_PROFILES=""gateway"" ./gradlew :dockerComposeUp
Note: Setting COMPOSE_PROFILES on the same command line as running ./gradlew will set the variable to the specified value for only that Gradle execution. In other words, if COMPOSE_PROFILES was set prior to running the above command, its value remains unchanged.
To check it out, visit http://localhost:8060/
Shut down: ./gradlew :dockerComposeDown
To shut down only the api-gateway container:
docker compose stop api-gateway
docker compose rm -f api-gateway

Platform Base + Java App
Prep: Start up the Platform Base setup (using instructions in the prior section)
Start up: ./gradlew :app:dockerComposeUp (Note the :app, which will run the dockerComposeUp task in only the app Gradle project.)
Equivalent to cd app; docker compose up -d
Equivalent to docker compose -f app/docker-compose.yml up -d
After a couple of minutes, visit http://localhost:8110/v3/api-docs or http://localhost:8110/swagger
Shut down: ./gradlew :app:dockerComposeDown
Equivalent to docker compose -f app/docker-compose.yml down
Optionally, shut down the Platform Base setup (using instructions in the prior section)
Platform Base + Java App with Platform Microservices
Prep: Start up the Platform Base setup (using instructions in the prior section)
To run only the Lighthouse microservice: COMPOSE_PROFILES=""lh"" ./gradlew :app:dockerComposeUp
Equivalent to cd app; COMPOSE_PROFILES=""lh"" docker compose up -d
Equivalent to COMPOSE_PROFILES=""lh"" docker compose -f app/docker-compose.yml up -d
To run all the Platform microservices: COMPOSE_PROFILES=""svc"" ./gradlew :app:dockerComposeUp
Equivalent to cd app; COMPOSE_PROFILES=""svc"" docker compose up -d
Equivalent to COMPOSE_PROFILES=""svc"" docker compose -f app/docker-compose.yml up -d
Optionally, shut down the Platform Base setup (using instructions in the prior section)
Mocks

Containers defined in the Mocks setup are not needed for typical development, so they are not started by default.

First, build the mock container images: ./gradlew -p mocks docker
To run the Lighthouse API and Slack mocks: COMPOSE_PROFILES=""lh,slack"" ./gradlew -p mocks :dockerComposeUp
Equivalent to cd mocks; COMPOSE_PROFILES=""lh,slack"" ./gradlew :dockerComposeUp
Equivalent to cd mocks; COMPOSE_PROFILES=""lh,slack"" docker compose up -d
Equivalent to COMPOSE_PROFILES=""lh,slack"" docker compose -f mocks/docker-compose.yml up -d
To run all the mocks: COMPOSE_PROFILES=""all"" ./gradlew -p mocks :dockerComposeUp

Examine the mocks/docker-compose.yml file to see what other profiles are available.

Without Docker Compose

If you want the mock to be available as localhost outside of the Docker Compose network, run it without Docker Compose -- for example:

docker run -d -p 20100:20100 --name mock-slack va/abd_vro-mock-slack
Run specific containers

To run a container without the Platform Base, don't use Docker Compose. Instead run using Gradle task dockerStart, for example to run the Domain-CC App: ./gradlew domain-cc:cc-app:dockerStart. Prior to running the container, this task will automatically build the container image (./gradlew domain-cc:cc-app:docker). To shut down the container:

./gradlew domain-cc:cc-app:dockerStop
./gradlew domain-cc:cc-app:dockerRemoveContainer # Optional

Run specific Platform Base container

To run only a subset of the Platform Base, specify the containers declared in the docker-compose.yml file and use the docker compose command directly -- don't use the Gradle tasks. For example:

To run only the Postgres DB and db-init containers: docker compose up -d postgres-service db-init.
To shut down, run docker compose down.
Start All Containers

All containers are included in the all COMPOSE_PROFILE, so use that profile to start all containers in a given Docker setup.

Set up: COMPOSE_PROFILES=""all"" ./gradlew dockerComposeUp (Note the lack of :, which will run dockerComposeUp in all relevant Gradle projects, starting from the root project).
Shut down: COMPOSE_PROFILES=""all"" ./gradlew dockerComposeDown
Individually Start All Containers

To start each setup individually, start the Platform Base first; the others can be started in any order:

export COMPOSE_PROFILES=""all""
./gradlew :dockerComposeUp   # start the Platform Base
./gradlew :app:dockerComposeUp
./gradlew :domain-xample:dockerComposeUp
./gradlew -p mocks :dockerComposeUp
...


To stop each setup individually, shut down the Platform Base last:

export COMPOSE_PROFILES=""all"" 
./gradlew :domain-xample:dockerComposeDown
./gradlew :app:dockerComposeDown
./gradlew -p mocks :dockerComposeDown
...
./gradlew :dockerComposeDown  # stop the Platform Base

Tip: set COMPOSE_PROFILES once

When a subset of containers across several setups is consistently used, set COMPOSE_PROFILES once and export it. The variable will be used for all subsequent commands, and any unknown profiles for a particular Docker setup is ignored.

For example, if development or testing involves only the API Gateway and Lighthouse API, then running:

export COMPOSE_PROFILES=""gateway,lh""
./gradlew :dockerComposeUp :app:dockerComposeUp 
./gradlew -p mocks :dockerComposeUp


This will start the Platform Base setup with the API Gateway (due to COMPOSE_PROFILES=""gateway"" and :dockerComposeUp), the Java App with Lighthouse microservice (due to COMPOSE_PROFILES=""lh"" and :app:dockerComposeUp), and the Lighthouse API mock (due to COMPOSE_PROFILES=""lh"" and -p mocks :dockerComposeUp).

Prune all

Because Docker resources (containers, images, and volumes) may be overridden and recreated during development, occasionally clean up:

To prune unreferenced Docker resources: ./gradlew :dcPruneAll
Equivalent to:
docker container prune
docker image prune
docker volume prune
"
Development process ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Development-process,"Check out LHDI's ""Building and Running"" Development Guide and LHDI's-Boilerplate-Instructions#running-the-application in addition to this page.

M1 Macs

For M1 Macs, set export DOCKER_DEFAULT_PLATFORM=linux/amd64 to avoid problems downloading Docker images (e.g., ERROR: no match for platform in manifest or no matching manifest for linux/arm64/v8 in the manifest list entries). Details: https://pythonspeed.com/articles/docker-build-problems-mac/

Secrets

Check out the abd-vro-dev-secrets repo and follow the instructions to set local development credentials and enable querying Lighthouse's development environment.

Pre-commit hook

Enable VRO's pre-commit hook by doing the following:

Install pre-commit (Mac: brew install pre-commit)
Run: pre-commit install. This creates a .git/hooks/pre-commit file.
You'll likely have to install go: brew install go

With this enabled, a series of checks and fixes (defined in .pre-commit-config.yaml) will be applied to changed files when you make a git commit.

To skip a specific check, see https://pre-commit.com/#temporarily-disabling-hooks
To disable pre-commit temporarily, pre-commit uninstall , and then pre-commit install to re-enable it

(Most of the pre-commit alerts have been addressed in PR #666.)

Modifying existing code

Before modifying application code, it's advantageous to determine in which Docker container (and hence Code structure) the target code in being run.

The abd_vro-app container results from the following Gradle subprojects/folders:
app
api
controller
persistence
service
while the abd_vro-db-init container results from the db-init folder;
and the abd_vro-service-ruby container results from the service-ruby folder.

Also examine the docker-compose.yml files in the various subprojects, i.e., app, db-init, and service-ruby, to identify the container names and dependencies.

With that understanding, you can start to modify code.

First start all the containers needed to run VRO: ./gradlew :dockerComposeUp :app:dockerComposeUp
To reset and rebuild container images: ./gradlew clean build docker
Next determine what code needs to be modified and in which container it is run.
Then follow one of the subsections below:
A. Modify Microservice

If modifying code in the service folder, run using Sprint Boot. Alternatively, the slower option is to rebuild container image.

If modifying code in the abd_vro-service-ruby container, stop the container (docker stop docker-service-ruby-1), and run cd service-ruby/src; ./entrypoint.sh (which calls ruby) to quickly run the modified code. Alternatively, the slower option is to rebuild container image but is consistent with running other containers.

B. Modify Camel Route

These currently reside in *Route(s) classes in the gov.va.vro.service.provider.camel package of the service/provider folder. Changes to this code will result in the abd_vro-app container, run using Sprint Boot or rebuild container image.

C. Modify API, Controller, or Model

Currently, the API, controller, and model code result in the abd_vro-app container, so run using Sprint Boot or rebuild container image.

Run using Spring Boot

Close the abd_vro-app container (docker stop docker-abd_vro-1) and run ./gradlew :app:bootRun to quickly run the modified code using Spring Boot.

Rebuild container image

The alternative (and slower option) for running the modified code is to run it in an updated container: ./gradlew :app:dockerComposeUp, which requires rebuilding the container image and deploying the container locally. This command will automatically stop the running container and start an updated container with the same name.

Tip: Portainer UI

For a visual Docker Container management UI, try Portainer -- example UI. Run docker run -d --name portainer -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v /opt/portainer:/data portainer/portainer-ce and go to http://localhost:9000 in a browser.

New API and microservice

When developing a new API endpoint with a new microservice, some folks like to start with the backend service; others prefer to start from the API; and still others like to start with a dead-simple end-to-end implementation and iterate on it until it satisfies the requirements.

The following sections describe the development steps used to add a new API endpoint and associated microservice. Each step can be done independently and in parallel, with some coordination to ensure consistent interfaces between Controller classes and Camel routes, and between route endpoints and microservices. For details, check out Routing API requests.

Build the microservice

To minimize software coupling and maximize single-responsibility, a microservice should be built independent of existing VRO code. It should be idempotent and testable without having to run any other VRO code. It can be placed in its own Docker container or into an existing container. See the service-ruby folder and PR #71 for an example of Ruby microservices that can be run and tested on its own, without Camel or a REST API -- standalone testing Ruby scripts are in the examples folder. The only requirement is interfacing with a message queue, specifically RabbitMQ.

To add a container in which the microservice will run, see Docker containers.

Test Camel routing to microservice

New backend Camel routing and microservices can be tested without having to implement API endpoint, controller, data model, request/response, and mapper classes. CamelRestConfiguration uses Camel to create a REST API for quick development and testing in a local development environment -- it is not for production. CamelRestConfiguration is only enabled when vro.camel_rest_api.enable = true (set in conf-camel.yml). This is achieved by using Camel's REST endpoint to provide quick API for testing via curl or Postman. The sample CamelApp uses this Camel feature to provide the API, whereas VRO uses Spring's Web MVC described in the next section.

Alternatively, Camel messages can be directly injected into Camel routes, which could be useful in unit tests. For examples, inspect the CamelEntrance class.

Build out the API and Controller

Implement API endpoint, controller, data model, request/response, and mapper classes.

Get agreement on the API endpoint specification, including examples and error codes. Check http://localhost:8080/swagger in a browser.
Implement controller methods using Request and Response classes, and a Mapper class to convert to service.spi.*.model classes.
Have the controller initiate a Camel route that eventually leads to a microservice, for example by calling a method in the CamelEntrance class."
Development environments ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Development-environments,"VRO Environments

Development (non-prod) and prod environments for the various systems, APIs, and data sources with which VRO interacts.

Following Environments are Available for VRO

DEV
The purpose of the development environment is to provide a controlled space for software developers to create, modify, and test new features, enhancements, or bug fixes. Developers have the freedom to experiment without affecting the stability of the production system. Developers use the DEV environment to write and test code, integrate new features, and collaborate on software development tasks.
Runs mainly on Develop Branch and is frequently Updated.
https://dev-api.va.gov/services/abd-vro/
QA
The QA environment is dedicated to testing and quality assurance activities.Teams use this environment to perform various types of testing, including functional testing, regression testing, performance testing, security testing, and user acceptance testing.More stable then DEV environment and is manually updated.
Can be used by team for manual testing
Internal:
https://qa.lighthouse.va.gov/abd-vro
External:
https://staging-api.va.gov/services/abd-vro
Sandbox
A sandbox environment used for testing. It provides a safe space for trying out integrations without impacting the production or other critical systems. Previously it was used for testing v1
Internal:
https://sandbox.lighthouse.va.gov/abd-vro/
External:
https://sandbox-api.va.gov/services/abd-vro/
Prod-test
Used as staging env to test prod changes before pushing to prod.
Read Access to Lighthouse Prod API
Production data, contains PII
Internal:
https://prod-test.lighthouse.va.gov/abd-vro/
External
https://api.va.gov/services/abd-vro-prod-test
Prod
Release process deploys code from main branch
Internal
https://prod-test.lighthouse.va.gov/abd-vro/
External
https://api.va.gov/services/abd-vro/
See Also:
Environment Connections
VRO should use Lighthouse API (as an intermediary) when possible -- this can simplify getting access and staying up-to-date.
VRO Environment Crosswalk Spreadsheet
Prior RRD Testing

Testing the RRD prototype involved these environments:

va.gov staging user
Lighthouse sandbox
VBMS uat
EVSS pint and VBMS uat interact with the linktest (fake) data.

We'll likely do something similar for testing VRO.

EVSS preprod corresponds with VBMS prodtest
VRO v1 Testing
va.gov staging
Lighthouse sandbox
VRO sandbox (sandbox.lighthouse.va.gov/abd-vro)
VRO v1 connects to only Lighthouse (not VBMS or EVSS)
VBMS uat
EVSS pint and VBMS uat interact with the linktest (fake) data.
VRO v2 Testing
va.gov none - no prod-test env exists
MAS prodtest
Lighthouse prod
VRO prodtest
VBMS prodtest
LHDI

On the nonprod cluster:

DEV - https://dev.lighthouse.va.gov
example: accessible on the VA network at https://dev.lighthouse.va.gov/abd-vro/v1/example/claimsubmissions or https://dev.lighthouse.va.gov/abd-vro/swagger
QA - https://qa.lighthouse.va.gov
https://qa.lighthouse.va.gov/abd-vro
SANDBOX (used as pre-prod) - https://sandbox.lighthouse.va.gov
https://sandbox.lighthouse.va.gov/abd-vro
The environment goes through the same release gates that deployments in the production environment.

On the prod cluster:

PRODTEST - ?
PROD - https://api.lighthouse.va.gov
https://api.lighthouse.va.gov/abd-vro

See Kubernetes clusters for details

va.gov

va.gov (a.k.a. vets-api)

From VA Platform doc and devops repo

sandbox
utility
dev - https://dev.va.gov/
staging - https://staging.va.gov/
Test User Dashboard ""helps teams to find acceptable high quality test users that are available for use on staging"" -- also mentions ""requesting a new test account""
prod - https://www.va.gov/
Lighthouse API

From Lighthouse doc

sandbox (https://sandbox-api.va.gov)
https://sandbox-api.va.gov/oauth2/health/system/v1/token (Validate Token)
https://deptva-eval.okta.com/oauth2/aus8nm1q0f7VQ0a482p7/v1/token (FHIR CCG Assertion URL)
https://sandbox-api.va.gov/services/fhir/v0/r4 (FHIR URL)
https://deptva-eval.okta.com/oauth2/ausj1sd1wiZE9S6BR2p7/v1/token (VRO CCG Assertion URL)
prodtest & prod (https://api.va.gov)
https://api.va.gov/oauth2/health/system/v1/token (Validate Token)
https://va.okta.com/oauth2/aus8evxtl123l7Td3297/v1/token (FHIR CCG Assertion URL)
https://api.va.gov/services/fhir/v0/r4 (FHIR URL)
https://va.okta.com/oauth2/ausgbozn6qGzmjAFh297/v1/token (VRO CCG Assertion URL)
Lighthouse Sandbox Test Patient Data Management
There's 2 ways to submit requests to the Health API team:
Email api@va.gov (consumer support team's preferred method)
Fill out the contact form at developer.va.gov/support/contact-us
A few reminders:
It's okay to send one email that includes the creation of 3 health test patients, for example, but if subsequent changes are needed to the health data for those 3 test patients, a new email should be sent
Keep api@va.gov in the cc line of the email when responding to requests and questions
Specify that the request is for health test patients
Indicate which team the request is coming from
Request does not need to be in a specific format, but be as specific as possible
Estimated turn around time: ~1 week, depending on the intricacies of the change
VBMS

VBMS Overview

Logins

In order to login you need:

Station IDs: the 3-digit number that corresponds to your Regional Office. We usually use 283 for Developers. ARC is 397, and Florida is 317.
User ID: is the Active Directory account name under which you will authorize access.
PIV card

For VBMS Prod access

Fill out this form, sign it, and send it to Zach securely
Once approved, use your PIV to log in

For VBMS ProdTest access

you must have access to VBMS Prod
email E_VBMS_4mopsleads@bah.com to be added to the ProdTest URL mailing list, which changes daily.

Local Testing of VBMS and BGS

dev
bep.dev.vbms.aide.oit.va.gov
bepdev.vba.va.gov
uat (a.k.a. devtest) - Core: https://www.uat.vbms.aide.oit.va.gov/vbmsp2
to set up the test Veterans in VBMS UAT, see Notion page - involves logging in to VBMS Core, VBMS Rating, VBMS Awards.
preprod (a.k.a. prodtest) - Core: https://www.pre.vbms.vba.va.gov/vbmsp2
duplicate of production data updated twice a day
prod
EVSS
INT
PINT (aka SQA or staging)
PreProd
Prod
CMP/MAS

CMP (a.k.a. MAS - Mail Automation Systems)

dev - https://iam-dev.ibm-intelligent-automation.com
test - https://iam-test.ibm-intelligent-automation.com
prod

MAS tests its OCR function in VBMS prodtest

Old related ticket

MPI

MPI (Master Person Index, a component of Master Veteran Index (MVI) or formerly known as MVI or Master Patient Index)

MPI can be used to get the Integration Control Number (ICN) for a given veteran.

Lighthouse Veteran Verification API v2 could provide this lookup capability. API-17363 will look into the feasibility of this (Slack).

Resources

MPI Testing refers to existing Ruby code
VA MPI Playbook list service functions
MPI Service Description
Contacts"
Deployment Overview ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deployment-Overview,"VRO is deployed to the LHDI platform. To deploy to LHDI, Docker images for deployment must exist in the GHCR (GitHub Container Registry).

As mentioned in Uses of container images, a GH Action pushes packages to GHCR so that the packages can be retrieved by EKS in the LHDI platform, initiated by calls to helm and kubectl.

For VRO releases, see Release Process and Deploying VRO.

GHCR images

GHCR images are associated with the abd-vro-internal repo

abd-vro-internal packages
dev_* images: added as part of the SecRel workflow when the dev_ prefix is selected
mirror-* images: added when the Publish 3rd-party image workflow is manually run in the abd-vro-internal repo; these images are unmodified mirrors of the original
vro-* images: added as part of the SecRel workflow when no prefix is selected or when a release-* is created in the abd-vro-internal repo (see Quick Deploy Instructions)

The SecRel workflow has an option to sign the images, which is needed for certain deployment environments.

Deployment environments (LHDI clusters) pull images from GHCR

DEV and QA do NOT require signed images
SANDBOX, PROD-TEST, and PROD require signed images. For these namespaces:

DI does not enforce any specific usage of these namespaces other than enforcing resource quotas and image signing for prod and sandbox environments.

In the internal repo, when a GitHub Release is created. the SecRel workflow is triggered, which will sign the image in GHCR if it passes SecRel checks. Then these signed images can be deployed to the LHDI environments that require signed images. See more details at Deploy to Prod.

Why not use the signed images for all environments so that there are fewer images to manage? Because if SecRel fails (for various reasons), the images are not signed. To allow testing to continue despite SecRel failures, the unsigned images are useful to have for deployment to certain environments for testing.

Special branches
The main branch is intended to reflect the code deployed to the PROD environment of LHDI.
Only branch main goes to PROD, but only after main is tested in PROD-TEST.
Any branch can be deployed to PROD-TEST.
All code changes should be pushed initially to the develop branch. When ready to deploy to PROD, the main will be updated (git reset) to the desired commit on the develop branch.
The only exception are hotfixes (including fixes for security vulnerabilities), which can be merged directly into main for PROD deployment. The hotfixes should also be applied to the develop branch so that when main is updated, the hotfixes are retained. See Partner Team Deploy Process for details.
domain-* branches facilitates deployment of domain-specific code, independent of the develop branch. See Partner Team Deploy Process for details.
Process diagram

Legend:

rounded boxes: activity performed by a person
rectangular boxes: automation or object
solid line: causal connection or trigger
dotted line: ordering association"
Deploying VRO ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deploying-VRO,"NOTE: This should only be done by VRO Engineers

See More of a overview of the deployment process here

Deploy to Prod using main
Create a release tag on develop
Run these commands on the public repo to sync main with develop
git checkout ""develop""
git pull
# Checkout branch           
git checkout ""main""     
git pull
# Hard Reset main to point to the desired develop
git reset --hard <CANDIDATE_COMMIT_HASH>
git push --force-with-lease


This should trigger the Mirror GitHub Action to copy the branches from the public repo to the internal repo's branches.

Verify and Fix SecRel
Deploy images with the release tag to prod-test for testing.

Passing SecRel is required to deploy to prod-test

Once final testing is successful: NOTE: How do they have pre-release tags?
Convert the GitHub release in the public repo from a Pre-release to a Release to prevent it from being automatically deleted. Nothing needs to be done in the internal repo.
Optionally, create a GitHub release with a new git tag using the format release-..* so that it will never be deleted. (Do not use the create-release-tag GitHub Action workflow.) Deploy images with the release tag to prod LHDI env
Deploy to non-prod using develop
Merge a PR with changes into develop branch
Create a release tag on develop
Deploy images with the release tag to a non-prod LHDI environment (dev, qa, sandbox, or prod-test)

Note that automated deploys of the develop branch to the dev LHDI environment occur daily -- CI-CD Workflows.

Prod hotfixes

If it's acceptable to deploy the latest develop commit, then:

apply the hotfix to develop
and do Prod deploy of Main

Otherwise (there are develop commits that shouldn't be deployed):

apply the hotfix to main (test changes)
Create a release tag on main (using a tag with a -hotfix suffix, like v3.0.2-hotfix1 or release-3.0.2-hotfix1 representing the first hotfix for the previously released 3.0.2)
and do Prod deploy of Main and also:
If relevant (for instance, if the hotfix should be included in future releases), then:
apply the hotfix to develop so that the hotfix is included for the next Prod deploy of Main (where main is reset to a develop commit).
Create a release tag on develop (using a tag like v3.0.<next consecutive integer>)
Create a Release Tag (Learn more about release tags)
Use the create-release-tag GitHub Action workflow to create a new release Tag
Run a new workflow and follow the Semantic Versioning guidelines to choose the next version number for the release tag to provide into the workflow.
Deploy images with using the release tag

For the VRO Platform Team, click the ""Run workflow"" here for one of the ""Update Deployment"" GitHub Action workflows:

Please use the following ""Update Deployment"" workflows based on the env you are deploying:

Update Deployment is used to deploy various Helm charts, including api-gateway, platform microservices, and domain-*.
Update Deployment - App is only needed to update the vro-app Helm chart, which exposes Java-based REST API and updates the DB schema (via the db-init container).
Update Deployment - Platform - Used to deploy changes to Postgres, RabbitMQ, Redis, CLI (rarely needs to be used)

For the ""Update Deployment"" GitHub Action workflow inputs, run bash scripts/image_vars.src imageVersions to show the latest image tag values for each container image. Details at Container Image Versions.

Verify and Fix SecRel
Navigate to the this Github workflow page and confirm/wait for the workflow to complete for your release version. Navigate to the analogous page in the internal repo and again confirm/wait for the workflow to complete for your release version.
Check for a post to the #benefits-vro-devops Slack channel that SecRel is running. Monitor the SecRel workflow by navigating to the internal GitHub repo and checking the GitHub actions for an action with the name of the release or by clicking the link in the Slack message.
Any SecRel alerts that have either an expired acknowledgement (from prior releases) or is new to the changes in the release will require remediation. If SecRel alerts can be addressed without compromising the release date, they should be addressed and this process will have to be repeated. Otherwise, the two engineers will need to collaborate to weigh the severity of alerts and the harms of delaying the release date to determine next steps. If acknowledging alerts is chosen, engineers should be able to clearly articulate their argument for acknowledgement to the SecRel assessor if needed and file an issue summarizing this argument.
Once all SecRel alerts have been acknowledged or addressed, restart the SecRel GitHub workflow.
Repeat steps 6-8 until SecRel GitHub workflow completes successfully."
Deploy to Prod ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Deploy-to-Prod,"From this Slack post:

As of right now, deployments to production environments on LHDI are fulfilled by going through the Secrel pipeline. team-tornado is the team that manages that flow and can work with you on onboarding there. They have a support channel in Lighthouse slack at vaapi-secure-release-pipeline-support-channel.

LHDI validates that you have gone through the SecRel pipeline through signed image enforcement, which you can read more about here: https://animated-carnival-57b3e7f5.pages.github.io/container-platform/signed-images/

The SecRel workflow GitHub Action implements the SecRel pipeline.

Review Deploying VRO for the bigger picture. For exact instructions, see Deploying-VRO#prod-deploy-of-main.

For visual walkthrough, see Quick Deploy Instructions."
Dependabot ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Dependabot,"Dependabot has been enabled, and VRO dependencies (for Java, Python, Docker, and GH Actions) are currently up-to-date. On a daily basis or when we manually run it, it will automatically create PRs to upgrade library versions (like these). Having this will help keep VRO code patched against vulnerabilities and will reduce the work in resolving SecRel alerts, required for deploying to production.

Known problems
Checks (GitHub Actions) fail on Dependabot-generated PR

UPDATE: Based on https://docs.github.com/en/code-security/dependabot/working-with-dependabot/managing-encrypted-secrets-for-dependabot and https://docs.github.com/en/code-security/dependabot/working-with-dependabot/automating-dependabot-with-github-actions#accessing-secrets, PR #454 fixes this issue by:

Adding a ACCESS_TOKEN secret to Dependabot's secrets
Updating Dependabot config to use the secret to retrieve jars from the VA's repo (starterBootPkgs https://maven.pkg.github.com/department-of-veterans-affairs/lighthouse-di-starter-boot) in order to build

Original problem

Certain checks fail because Dependabot doesn't have access to the GitHub Actions secrets -- see https://github.com/dependabot/dependabot-core/issues/3253#issuecomment-795101596 and in https://docs.github.com/en/code-security/dependabot/working-with-dependabot/automating-dependabot-with-github-actions#responding-to-events (""GitHub Actions secrets are not available."").

A workaround is to merging the develop branch into the PR or manually committing to the PR branch. See example resolution in PR #386.

Error during Dependabot update

Error is shown in logs on the Dependabot tab. Example PR that resolves the error. Once the PR was merged, Dependabot created the PR, which is no longer relevant. Manually re-running Dependabot shows no errors.

Dependabot Processing Guide
Checking for new Dependabot PRs

Dependabot-generated PRs can be found under the Pull Requests tab by searching on open PRs with the dependencies label

Also check for and address Code scanning and Secret scanning alerts.

Deciding to accept the update

If the proposed update seems relatively minor or trivial, then

Navigate to the mirrored dependabot PR in the internal repo, and check the PR actions for a SecRel run. Address any SecRel issues and update the PR in the public repo if necessary.
In the dependabot PR in the public repo, be sure that all testing actions along with container health checks both run and pass.
Optionally, perform additional manual API testing as needed depending on the complexity of the change.

Sometimes, the generated PR does not have all the changes necessary to truly update the references successfully. Once functional testing passes the branch, you should push up your changes and run it through SecRel again to make sure you haven't introduced unforeseen security issues.

When possible, update the constraints section in shared.java.vro-dep-constraints.gradle so that other projects dependencies can be updated.

If all looks well with the generated (or new) PR, then

Comment on the PR that all testing passes, and tag the Eng Lead (or some other relevant developer) for an additional approval.
Dependabot authors the PR, so only one additional reviewer other than you is required.
Once approved, merge the PR into the develop branch
Deciding to skip the update

If the proposed update seems like a relatively major change that could take non-trivial testing and/or refactoring, you may decide to delay it for a future date (as with these PRs). If that's the case, then

Close the PR with a detailed comment explaining why, tagging the Eng Lead (and/or some other relevant developers)
Open an issue so that it can be tracked and updated at a future date (see below)
Opening an issue
Goto the Issues tab, and click New Issue
Click ‚ÄúOpen a blank issue‚Äù
Fill in ticket, perhaps using this as a model (Description + A.C. sections). Be sure to link to specific PRs when applicable, so that they will be linked to the issue in their comments
Add the Engineer and vro-issue labels under Labels section
Add ABD VRO Project under the Projects section"
DataDog monitoring ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/DataDog-monitoring,"DataDog

Log in using LHDI's Okta sign-in page is https://ablevets-dots-va.okta.com/

VRO's dashboard
LHDI dashboards

Our deprecated DataDog account:

lasershark's dashboard - one of the Alpha customers"
Data Visibility MVP Tech spec ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Data-Visibility-MVP-Tech-spec,"Data Visibility MVP Tech spec
Background

Currently VA.gov activity data, including disability benefits claim submission data, is functionally inaccessible to Benefits Portfolio product teams, with the exception of a handful of engineers with command line access to query the production postgres database in vets-api. OCTO wants to develop a safer, more accessible, and more user-friendly way for teams to access this data.

Thus, VRO as a platform as an MVP will be resposible for safely and securly providing VRO partner teams within the Benefits portfolio the cliams data submitted via 526EZ forms through va.gov.

In-order to make this happen, the VRO team is responsible for coordinating this effort via collaboration across the Benefits Portfolio, in particular with the Disability Benefits Experience team(s) who are familiar with the va.gov Postgres database and the needs of engineers working on va.gov benefits products.

Pain points
Disability benefits claim submission data is only avaiable via rails console in prod.
Cannot use any BI/dashboarding tools to view metrics.
MVP goals and assumptions:
Focused on the 526EZ form benefits claims submission data
Data dump from production vets.gov postgres db happens daily into a s3 bucket through a another process.
Data at rest is decrypted before being dumped into the bucket
S3 bucket is already setup, encyrpted and secured via SSE-KMS or other AWS provided options
Benefits claims data is available via sql initially from the VRO postgres db
Solution
Utilize Kubernetes cron job to run a python script daily
Use Pandas or another dataframe python library to read the csv file, sanitize the data, filter any unwanted data, standardize datetime if nessessary.
Keep track of processed csv file s3 bucket file names in a database.
Store the processed claims data into the database using transactions.
Re-try mechanism for errors when they happen during cron-job.
Slack notification when a dump has been processed or failed.
Monitor
Create Datadog dashboard to monitor the cron jobs
Local development
s3 bucket and csv file
Generate fake data csv file without any PII to simulate daily dumps
To emulate s3 bucket functionality locally, use local stack
Rather than using docker-compose.yaml files for the container, use the kubernetes deployment files used for LHDI env locally and leverage minikube to run the container locally and it can be ingrained into the existing Gradle tasks. Added step will be that VRO will installing minikube.
Questions and comments
How do we handle storage of PII data because of possible ATO restrictions?
What exactly does current data look like? This can help us design exception handling and job-retry mechanisms.
Have a backup mechanism in place for the data in case of any failures or data loss"
Culture and Norms ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Culture-and-Norms,"Our teams core values
Veteran's first - always consider if what you are working on provides value to Veterans.
Help others - how can you elevate others?
Be a thoughtful and trusted partner to the VA and their contractors - treat others as you would like to be treated.
Look to make small, incremental improvement - fail (learn) fast if needed.
Enjoy your work - be proactive, flexible, resourceful and curious.
Make the team be a safe place - demonstrate kindness, empathy, respect, trust, care and assume best intentions.
Group norms
Be proactive - don't wait for someone to tell you what to do.
Pair on tasks often - offer and ask to pair.
Follow Slack protocols - use threads, tag people as needed and change your status if you are away for more than a couple of hours.
Include summaries of links when adding links in tickets.
Keep documentation in Wiki - the Wiki should be the source of truth.
Create user stories for tasks that are more than a half day of work.
Create a ticket for each sprint where small ad-hoc tasks can be added for tracking and visibility.
Start with Slack threads and move to huddles if there is a lot of back and forth.
How we measure success
We produce quality deliverables and are proud of our work.
We have good balance.
We are always learning - we experiment and learn from each other.
We have delivered value to Veteran's - we have delivered capabilities to OCTO that other teams can benefit from.
We collaborate - we build good relationships within and outside of our team.
We work as a team towards a common goal.
VRO is deployed and is providing value.
How we ensure quality
The deliverable provides value.
The Product Manager has signed off on the deliverable after all acceptance criteria have been completed and demonstrated.
PRs have been reviewed by XXXX people and revisions based on feedback completed.
Be proud of your work - self QA/test.
Documentation is updated and can easily be followed by others.
Proposed Norms

Starting some proposals for consensus:

Document Coding Patterns that should be followed, especially if it follows best practices. The LHDI Starter Kit provides some initial code but we don't have to follow it if it causes undue friction.
Document DevOps How-To's for developers, such as Deploy to Dev and Deploy to Prod. Application developers don't need to know all the details; provide a ""Quickstart"" guide with references to relevant code, script, and/or configuration files. Document it once and reference it so you don't have to re-explain.
Document DevOps details that may be useful for others or demonstrate how to get the details, such as Kubernetes clusters.
Document available tools, such as DataDog monitoring, for diagnostics, monitoring, or reporting.
Document learnings or tips that takes a while to figure out, such as Docker containers (because it is involved in development and deployment) or your Development process, to help your teammates learn and include notes to remind your future self.
Radiate intent because we want to get things done quickly and openly."
Code structure ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Code-structure,"This page does not seek to explain testing and how it is performed or the runtime/deployment infrastructure for the VRO project.

This page also does not seek to explain the details of any domain-specific code that is accessible within the VRO app. Each domain should include a link to a page here which provides details about its logic and components.

Initial things to review
Application Entrypoint
Domain-Specific Logic
Camel Routes and Microservices
Shared libraries
Putting It All Together: Project Folder Structure
Initial things to review
Software Conventions
initial files provided by LHDI's Java Starter Kit in PR #8
software changes in Current Software State
Gradle Build System
Configuration Settings and Environment Variables
Application Entrypoint

VRO exposes a single API through a Java Spring Boot Application within the app subproject. The exact structure of the API endpoints along with the expected form of requests and responses can be subject to frequent changes. VRO uses OpenAPI to allow for dynamic creation of documentation for API endpoints. LHDI Docs for using OpenAPI for applications (requires membership in abd-vro Github team)

To see the most recent VRO API spec, run the software and browse to http://localhost:8080/swagger.

The Spring MVC framework is used to define the implementation of the Spring Boot Application, giving the following key components for defining the application logic:

api defining VRO's API
controller for the API
persistence:model defining DB Entity classes
service:db for DB interactions
service:spi containing Java interface and model definitions
service:provider containing Apache Camel configurations

The following is a basic graphic depiction of the application

Domain-Specific Logic

As VRO is a host of domain-specific logic, the Spring Boot Application defined within the app Gradle subproject takes a dependency on Gradle subprojects of the form domain-* which are responsible for defining their own domain-specific API routes and controllers which can then be exposed through the base VRO Spring Boot Application.

For instance, logic specific to some ABC functionality would have code following the structure:

domain-ABC: domain-specific code with subfolders:
ABC-api-controller: module that defines domain API endpoints and controller
ABC-workflows: defines domain workflows (i.e., Camel Routes)
svc-*: domain microservices supporting the workflows
Camel Routes and Microservices

The Spring Boot web app processes a request by delegating to a workflow defined by Apache Camel routes. More information on Apache Camel and how it is used in this project can be found here.

Once a request is received by the Spring Boot application entrypoint, it can be routed by Camel through a number of different steps including to a microservice (discrete component which is responsible for completely handling a smaller piece of computation) via a message bus or to some data sink.

Current list of microservices (each has an associated Gradle subproject):

2 Python assessors
1 Python pdf generator
1 Java client for LH API svc-lighthouse-api
1 Java client for MAS API svc-mas-api (in development)

Note that clients to external APIs are also modeled as microservices for compatibility with Camel routes.

Since the Python microservices have common code, it is extracted into a shared library:

service-python/main-consumer.py

RabbitMQ is used as our message bus for transporting messages to microservices while Redis and Postgres are used as data sinks depending on the use case. Postgres is deployed as a containerized application through the help of a separate init container called db-init. Both definitions for Postgres and this init container are found in the top-level directory of the project.

Shared libraries

VRO offers utilities and DB classes as shared libraries for use by domain-specific classes.

shared/api folder: (Java) general API models
shared/controller folder: (Java) Controller utilities, including InputSanitizerAdvice
shared/lib-camel-connector folder: (Java) Camel utilities, including a Spring Configuration for Camel and CamelEntry
persistence folder: DB utilities, including Java Entities
Python MQ utilities to facilitate interaction with the MQ
domain (PR Move domain folder to new shared folder #551)
Putting It All Together: Project Folder Structure
app: VRO entrypoint; pulls in domain-specific api-controller modules
db-init: initializes and updates the database schema
postgres: defines a customized database container
shared: utility and library code shared across domains (Shared libraries)
svc-*: domain-independent microservices, typically for integrations (Shared microservices)
domain-ABC: domain-specific code with subfolders:
ABC-api-controller: module that defines domain API endpoints and controller
ABC-workflows: defines domain workflows (i.e., Camel Routes)
svc-*: domain microservices supporting the workflows"
Container Image Versions ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Container-Image-Versions,"Container images are built and published to GHCR (GitHub Container Repository) -- see Deploying VRO. The images are tagged with an image tag, representing the version of the image. The image tag (or ""image version"" or ""version"") is:

the first 7 of commit hash when the image is built as part of Continuous Integration (CI CD Workflows),
or a semantic version string formatted as v1.2.3 when a release tag is created, typically for testing in LHDI environments,
or a semantic version string formatted as release-1.2.3 when a manual release tag is created, typically for deployment to prod LHDI.

See different image versions in the GitHub Container Repository. The latest image tag always refers to the last published image, which may or may not be signed by SecRel.

Previously: A single image tag

Previously, there was a single image tag for all container images and all containers were SecRel-scanned and deployed, regardless of whether a container had changed or not. With this constraint, as the number of images increases, more time is spent on SecRel scans, addressing SecRel alerts for unchanged images, and redeploying unchanged images. This coupling of container images to a single image cersion imposes an unnecessary constraint.

As described in Allow deployed containers to have different release versions #1725, we want a way to be able to deploy only the containers that have been updated, with minimal additional manual maintenance overhead. The remaining sections describe how this is achieved.

Decoupling image versions

The image_vars.src and image_versions.src files determine which versions of each container is used during deployment. The image_vars.src file should not be modified manually. The versions can be overridden by manually-specified versions in GitHub Action workflows (e.g., Update deployment).

To see the latest release versions for each image, run bash scripts/image_vars.src imageVersions as described in Deploying-VRO#deploy-images-with-the-release-tag.

Terminology and Benefits
pinning an image version = set the version for a particular container image so that:
the image is not republished to GHCR and SecRel does not rescan the (same) image
the pinned version is used during LHDI deployments
(The version number has the format v*.*.* or release-*.*.*)
unpinning an image version = unset the version for a particular container image so that:
a new image is built and SecRel scans the new image
the new image is deployed to LHDI
(The version number is the first 7 of commit hash)
Pinning versions

When a release is created (create-release-tag.yml), image versions that are unpinned will become pinned to that specified release version.

Unpinning versions

When the codebase is changed (e.g., a PR is merged or develop is changed) and new images will be published to GHCR, if the container image has changed for image versions that are pinned, then it will be unpinned.

image-version.sh automation script

Pinning and unpinning is performed by image-version.sh, which updates image_versions.src and is automatically called when certain GitHub events trigger GitHub Action workflows. The result of the script is that images that haven't changed will stay at their pinned versions, while change images will be unpinned. By looking at image_versions.src, it is apparent which images have not changed.

Manually pin image versions

To manually pin images so that they don't get automatically updated, use the export myimage_VER=""v1.2.3"" syntax in the image_versions.src file so that they will be ignored by the image-version.sh script.

Detecting changes to images

Image versions are unpinned when the image has changed. There are many reasons for an image to change:

we updated our code used in the image (to add features or fix bugs)
application dependencies or OS libraries have been upgraded (to address SecRel alerts)
the base container image was updated (to get updated features or address SecRel alerts)

The image-version.sh script detects changes to images using the container-diff utility, which identifies every tiny difference. Comparison is done using the ""size"" and ""history"" analyzers. However, there are docker-build-time differences (e.g., random file identifiers, installing OS packages and libraries, and file creation timestamps) that cause two images that are practically the same to be considered different. The script's isImageSame() function tries to account for some trivial differences but it's not perfect -- e.g., any image size difference cannot be discounted and so the images must be considered different. Also in rare cases where images with identical sizes aren't necessarily the same, it also checks the ""history"" (of Dockerfile commands) to mitigate this risk. (Further improvement for better image-change detection is welcome.)

Workaround: In those rare cases where a change is not detected and to ensure that a new image is published, simply unpin the image version by removing the corresponding line in the image_versions.src file.

Known issues

The container-diff utility reports differences for the following images, even when there may be no apparent changes:

vro-app - has it's own Dockerfile; uses Java Spring
vro-cc-app - has it's own entrypoint.sh; Python image; container-diff reports differences in pip-related files
vro-svc-bgs-api - has it's own Dockerfile; Ruby image

Possible causes:

OS packages are not set to specific version (search for hadolint ignore=DL3018 in Dockerfiles). As a result, new files are indeed being saved to the image.
Application libraries (e.g., pip packages) are being downloaded and cached in the image. The cache may have an updated catalog of available packages or the cache may be generating random filenames.

Further work is needed to improve the change detection mechanism.

Force all images to be SecRel-scanned

To have SecRel scan all images, manually run SecRel with publish mode all, which will publish and then scan all images. This is useful to identify newly discovered vulnerabilities in unchanged/pinned images.

To help ensure new vulnerabilities are addressed and to express a desire for a new version of the image, simply unpin the image version by removing the corresponding line in the image_versions.src file."
Configuration settings ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Configuration-settings,"Configuration settings are needed for different use cases (unit tests) and environments (dev, qa, prod, ...).

Use Cases

VRO is run the following ways, where environment variables are set for the containers:

locally via Docker (and docker-compose), usually as a result of running ./gradlew app:dockerComposeUp
deployed to LHDI's EKS
Guidelines for placing settings
If the setting is a secret, it must be put in the Secrets Vault. The secrets will be made available as environment variables.
Prefer container-scoped settings or configuration files rather than exposing those settings outside the container in Helm configurations.
Prefer to add them to application*.yml (for Java) or settings*.py (for Python). Those files allow different setting values per deployment environment. For example, the application-dev.yml is used for deployments to LHDI dev.
If the setting needs to be overridden, allow it to be overridden by an environment variable. This should rarely be needed.
Minimize changes to Helm configurations (under the helm folder). This facilitates diagnosing issues in LHDI that originate from LHDI changes outside our control by allowing us to say: ""VRO hasn't change any Helm configurations but things stopped working today"". Settings that must be in Helm configurations:
VRO_SECRETS_* environment variables -- see Secrets Vault
Environment variables that refer to a Helm value or variable, e.g., pv.tracking.mountPath
Settings that are shared across containers, i.e., when the setting value must be the same for those containers. For example, connection URLs to VRO's DB, MQ, and Redis.

Also check out Secrets-Vault#configuration-setting-vs-environment-variable and comments in scripts/setenv.sh.

Environment Variables

Before using an environment variable, use a configuration setting that is scoped to only the relevant container -- read Secrets-Vault#configuration-setting-vs-environment-variable and see section ""Spring's application.yml"" below.

Use environment variables in code so that they can be overridden for deployment. For example:

build.gradle: in dockerRun settings block, set the default values so that it works in your local development environment -- example
docker-compose.yml: in environment settings block -- example
Dockerfile: using the ENV command -- example

In addition to the above, environment variables are initialized by:

setenv.sh (in the abd-vro-dev-secrets repo) - sets environment variables for local builds
helmchart/values.yaml (for LHDI's EKS deployments) -- Helm chart configurations

Environment variables are used:

in app/src/main/resources/application.yml (Java Spring Framework) - default values are set for environment variables that don't exist
app/src/test/resources/application.yml (for unit tests)
app/src/main/resources/application-compose.yml (used when running docker-compose)
app/src/main/resources/application-docker.yml (used when running docker)
in settings.py (for Python code)
by VRO containers (when run by Docker or EKS) - the variables are referenced by application.yml (for Java containers) and settings.py (for Python containers)

Java code in the VRO application should prefer to use configurations set in application.yml via Spring's @Value annotation -- environment variables should not be referenced directly. This encourages consistency in how VRO is configured.

For Python code, environment variables should be referenced in the settings.py file only (and not in the other files). The settings.py file functions like Java/Spring's application.yml.

Spring's application.yml

The ENV environment variable is used to set spring.profiles.active.

Similarly, environment variable SPRING_PROFILES_ACTIVE (Spring Profile doc) determines which other application-<PROFILE>.yml is loaded, in addition to application.yml -- see this StackOverflow answer. Several application-<PROFILE>.yml files can be loaded by delimiting the profile names with a comma, where properties in the later profiles will override those in earlier profiles if there is overlapping settings. -- see Profiles.

Spring property spring.profiles.include is used to load additional application-<PROFILE>.yml files regardless of the active profile. However, consider these guidelines when using this property.

Associated with profiles, Spring's @Profile(<PROFILE>) can be used to enable certain Spring Beans, Components, Services, etc. -- however, use it sparingly and responsibly.

@ActiveProfiles can be used for a @SpringBootTest that requires special profile(s), e.g., with settings that are different from application-test.yml.

VRO's use of Spring Profiles

The value of ENV can be any of LHDI's EKS environments (dev, qa, sandbox, prodtest, prod) or local (your local docker-compose environment). spring.profiles.active is set to the value of ENV, which will cause Spring to load application-$ENV.yml, in addition to the default application.yml.

Following this suggestion, VRO has:

application.yml - shared properties across all environments; always loaded by Spring
application-prodtest.yml - non-shared properties for the prod-test environment
application-prod.yml - non-shared properties for the prod environment. Secrets should reference environment variables, which will be set in Kubernetes.
application-nonprod.yml - shared properties for non-prod environments (local, dev, qa, sandbox). Spring Profile Groups are set up (in application.yml under spring.profiles.group) so that application-nonprod.yml will be loaded, followed by one of the following (if they exist):
application-local.yml - for your local docker-compose environment
(Optionally) application-dev.yml, application-qa.yml, application-sandbox.yml - for the corresponding LHDI EKS environment. These don't need to exist, esp. if their configuration is no different than application-nonprod.yml

When running VRO, the loaded Spring profile names are shown in one the first few lines of the app's log output, e.g., gov.va.vro.VroApplication: The following 2 profiles are active: ""nonprod"", ""dev"".

To override the active profile (spring.profiles.active), set SPRING_PROFILES_ACTIVE. This may be useful to set properties for mock services for example.

Feature flags

Feature flags help developers gate which code should be enabled or not. The microservice for feature flags exists inside of service-python/featuretoggle and [java location TBD].

Feature flag status is stored in Redis and checked by the main app, which attaches the data to routes as needed. The app queries the microservice which updates the timestamp of the last check in order to ensure flag data is up-to-date.

Update the features.yml file in the featuretoggle service with the feature flag name and status."
CI CD Workflows ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/CI-CD-Workflows,"Quoting from this CI/CD page:

Continuous integration (CI) is practice that involves developers making small changes and checks to their code. ... this process is automated to ensure that teams can build, test, and package their applications in a reliable and repeatable way.
Continuous delivery (CD) is the automated delivery of completed code to environments like testing and development. CD provides an automated and consistent way for code to be delivered to these environments.
Continuous deployment is the next step of continuous delivery. Every change that passes the automated tests is automatically placed in production, resulting in many production deployments.
1. Continuous Integration (CI)

When the develop branch is updated (e.g., due to a PR being merged), it triggers two major GH Action workflows:

Continuous Integration: runs various tests, including linters, unit tests, end-to-end tests, and CodeQL vulnerability check.
SecRel: builds and publishes Docker images and has SecRel sign them if the images pass SecRel checks. These images are published to GHCR (GitHub Container Registry) so they can be used in deployments to LHDI's Kubernetes environment.

At this stage, the published Dev images have the prefix dev_ and can be deployed to the dev and qa LHDI environments. Other environments have different requirements, so non-dev images (i.e., those without the dev_ prefix) are used to clearly distinguish them to prevent accidental deletion.

2. Continuous Delivery (CD)

Every afternoon, an automated GH Action workflow called Continuous Delivery updates the deployment in the dev LHDI environment with the latest Dev images published (by the SecRel workflow). There is a small chance that the latest published Dev images may not reflect the latest develop commit since it can take 15 minutes for the images to be built and published.

FAQ 1

Q1: Why not deploy with every update to the develop branch? A: If someone is testing in the dev environment, an unexpected deployment would interfere and possibly produce unexpected behavior. Predictable deployments mitigate the likelihood of this scenario.

Q2: Does that mean we have to wait until VRO is automatically deployed to do testing? A: Nope -- see the ""Manual deployments"" section below.

Q3: Can I deploy and test a specific version of the develop branch? A: Yup! See the ""Manual deployments"" section below.

Q4: Won't GHCR be filled with Dev images? A: We have a GitHub Action workflow for that! Delete old published DEV images runs twice a month to delete Dev images older than 30 days but keeping at least 10 of the latest versions (regardless of age) for each image. This can also be run manually with different parameters.

Manual deployment to Dev

To manually deploy to the dev environment, use the Update Deployment workflow to update containers that have changed (the DB, MQ, and Redis containers do not typically need redeployment). Click the ""Run workflow"" button, then:

(Leave ""Use workflow from"" as develop)
Ensure the dev is the target LHDI environment
Choose a specific version to deploy or leave it as latest
Choose which sets of containers to update in the deployment -- the default selections are usually fine. Helm (our deployment tool) will redeploy only images that have changed (usually only the VRO App and domain services) -- this makes deployments efficient. Unselecting an option will undeploy the associated containers -- this is only needed if there is some deployment problem. The following Helm Charts can be enabled:
Enable & update VRO Application API
Enable & update RRD domain services
Enable MQ
Enable DB
Enable Redis
Enable VRO Console
For ""If deploy fails, rollback to previous? (Always true for prod)"", leave it unchecked so that if it fails, the failure state will be available for diagnostic investigation. Let VRO DevOps know if a deploy unexpected fails. Since this is a non-production environment, it's okay that VRO is not operational so a rollback is not needed.
""Shut down deployment completely, then redeploy"" can be useful to completely reset and restart everything; however this is not typical, so keep it unselected (false). Use this option only if deployments continue to fail and diagnostics is unfruitful.

Reminders when using this workflow:

Make sure no one is actively testing VRO in the target environment (i.e., dev).
Automated deployments will still run every afternoon (as part of Continuous Delivery). So if a manual deploy is done immediately before the automated deploy, the deployment may be overridden with the latest version.
3. Continuous Deployment

We currently do not automatically deploy changes to the production environment (prod). Rigorous manual QA testing is first conducted in the qa and prod-test environments, with fake and realistic data respectively.

Before describing deployments to the sandbox, prod-test, and prod environments, let's talk about GHCR packages and signed images.

GHCR Packages

The published images are called ""packages"" in GHCR, as seen here:

mirror-* images are copied from external sources. These images are unmodified mirrors of the original and provide services used by VRO. They are added when the Publish 3rd-party image workflow is manually run.
vro-* images are created from the VRO codebase.
These images are typically used for deployments to the sandbox, prod-test, and prod environments.
New tagged versions are added as part of the SecRel workflow when run on the main branch or when a release-* is created in the abd-vro-internal repo.
New versions of images are tagged using the first 7 characters of the commit hash or the release version (if a release was created).
These non-dev images should rarely be deleted.
dev_vro-* images are the Dev counterparts to the vro-* images.
These images are typically used for deployments to the dev and qa LHDI environments
New tagged versions are added as part of the SecRel workflow when run on the develop and qa branches.
This allows you to address vulnerabilities quickly; from changes to the image or newly discovered vulnerabilities
This does not delay you from testing images while secrel is failing, as unsigned images are pushed to dev and qa
New versions of images are tagged using the first 7 characters of the commit hash.
A dev_ prefix is used to distinguish from images used in prod, so that these Dev images can be readily deleted.

The tags for an image in GHCR can be seen by clicking on the image, then ""View and manage all versions"". To determine if an image is signed, compare the image ""Digest"" with the sha256-*.sig tag -- see these instructions.

Signed images

LHDI environments (clusters) pull images from GHCR. LHDI enforces the following policies for deployment to each environment:

dev and qa do NOT require signed images
sandbox, prod-test, and prod require signed images

Images are signed by the SecRel (Secure Release) pipeline, which runs in the the abd-vro-internal repo. VRO's SecRel GitHub Action workflow publishes the images to GHCR and runs the SecRel pipeline on the images, which signs them if the images pass SecRel checks). If SecRel checks fail (for various reasons), the unsigned images are still available for deployment to dev and qa for testing.

Deploying to sandbox, prod-test, and prod

Deployments to the sandbox, prod-test, and prod environments are manual and use signed non-Dev images, which are created in 2 ways:

Create a GitHub release in the abd-vro-internal repo (Note that a release needs to be created in the abd-vro repo first to prevent the abd-vro-internal's release from being removed due to repo mirroring). The images will be tagged with the release name (e.g., release-1.2.3).
Update the main branch. The images will be tagged with first several characters of the git commit hash.

These actions will trigger SecRel to publish and hopefully sign the images. If SecRel fails and cannot be remedied, identify the associated ""SecRel scan failed"" Slack message and notify @Tom to investigate.

If the SecRel workflow succeeds, run the Update Deployment workflow with the desired image tag to deploy to the desired environment -- see the ""Manual deployment to Dev"" above."
Change Management Plan ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Change-Management-Plan,"Branching Strategy

This strategy is taken and modified from Microsoft.

Use feature branches for all new features and fixes
Merge feature branches into the develop branch using pull requests
When ready to deploy to production, merge develop into the main branch, conduct QA testing, then deploy when ready.
Deploying VRO describes how branches are used for deploying specific versions of the code.
See Repository Branch Protection Settings for more configuration settings
Use feature branches for your work

Develop your features and fix bugs in feature branches based off your develop branch. These branches are also known as topic branches. Feature branches isolate work in progress from the completed work in the develop branch. Even small fixes and changes should have their own feature branch.

Creating feature branches for all your changes makes reviewing history simple. Look at the commits made in the branch and look at the pull request that merged the branch.

Review and merge code with pull requests

The review that takes place in a pull request is critical for improving code quality. Only merge branches through pull requests that pass your review process. Avoid merging branches to the develop branch without a pull request.

Pull Requests guidelines
Development process

Pull Request Policy:

All pull request reviews will require at least 2 approvals
Draft pull requests should be created immediately when work on a branch begins. This is facilitate conversations about in-progress tasks and ensure no work gets forgotten in an orphaned branch.
All pull requests will use Squash and Merge

Certain Github Actions are triggered for pull requests to scan the changes to meet security requirements and coding style conventions.

PR Templates

A PR template is a way to automatically populate the body of a PR, to ensure developers fill in useful information for the team to share context and have a historical record of changes.

The PR template is https://github.com/department-of-veterans-affairs/abd-vro/blob/develop/.github/pull_request_template.md.

Release Strategy

Triggering a new release is currently a deliberate decision and is detailed in Deploying VRO. All of the pipeline releases will be created from tags off main. The team decides when to release a new version following best practices. We should encourage following the semver definition of:

patch versions including minor updates or changes
minor versions including updates and added functionality that is backwards compatible
major versions including breaking changes and added functionality

Each release will contain Release Notes specifying bug fixes, breaking changes, and new features.

Deploy to Prod describes details about deploying to releases to production.

Repository Branch Protection Settings

The following branch protection rules should be applied against main and develop:

Require a pull request before merging
Require Approvals: 2 approvers
Require status checks to pass before merging
Require branches to be up to date before merging"
BIP Claims API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/BIP-Claims-API,"VRO uses BIP Claims API to get information about and do updates to VBA Benefit Claims.

currently it exposes two endpoints via RabbitMQ. To connect, use exchange name bipApiExchange. The end points are following.

queue name: getClaimDetailsQueue. Expects a long collectionId as input. Returns a BipClaim object.
queue name: setClaimToRfdStatusQueue. Expects a long collectionId as input. Returns a BipUpdateClaimResp object.
queue name: updateClaimStatusQueue. Expects a RequestForUpdateClaimStatus object as input. Returns a BipUpdateClaimResp object.
queue name: getClaimContentionsQueue. Expects a long collectionId as input. Returns a BipUpdateClaimResp object.
queue name: updateClaimContentionQueue. Expects an UpdateContentionModel object as input. Returns a BipUpdateClaimResp object.

Access to BIP Claims API is available only within VA firewall.

BIP Claims API uses mTLS. VRO's mTLS implementation is detailed in BIP-APIs.

Integration Requirements

The following benefit claim data is retrieved

Temporary station of jurisdiction
Existence of contentions' special issues ""Rapid Decision Rating Version 1"" (RDR1) and ""Rapid Ready Decision"" (RRD)

The following are the updates to claim data

Removal of contentions' special issue RDR1
Update of lifecycle status to ""Ready for Decision"" (RFD)
Open API Specification

BIP Claim API's Open API Specification is available from the Swagger page.

All the end-points that are used within the VRO application is also available from the Mock Bip Claim Api Swagger page in the local development environment.

Code Walkthrough
Security Requirements

BIP requires a Bearer JWT for access. Following claims are used

Subject (sub)
User Id (userID): Custom - VRO system user
Issuer (iss)
Station Id (stationID): Custom - VRO system user facility (?)
Application Id (applicationID): Custom - VRO application name
Expiration (exp)
Issued At (iat)

The JWT is created before each API call in BipApiService createJwt method.

Subject claim is hard-coded in createJwt. Expiration and Issued At claims are dynamicaly created in createJwt. The other claims are made available to the application with environment variables through application.properties.

User Id: BIP_CLAIM_USERID through bip.claimClientId
Issuer: BIP_CLAIM_ISS through bip.claimIssuer
Station Id: BIP_STATION_ID through bip.stationId
Application Id: BIP_APPLICATION_ID through bip.applicationId

JWT is signed by a secret provided by the BIP API team. The secret is made available to the application with the environment variable BIP_CLAIM_SECRET through application.properties bip.claimSecret setting.

In the VRO Kubernetes environment the related Kubernetes secrets for the BIP environment variables are

BIP_CLAIM_USERID: bip.bipClaimUserId
BIP_CLAIM_ISS: bip.bipApplicationIss
BIP_STATION_ID: bip.bipStationId
BIP_APPLICATION_ID: bip.bipApplicationId
BIP_CLAIM_SECRET: bip.bipClaimSecret

Note that due to an issue in the application (BIP_CLAIM_ISS is not specied in Helm charts) bip.bipApplicationIss is not functional. The hard coded value in application.properties bip.claimIssuer setting is used. This is in the list of issues for future fixes.

A set of BIP environment variables are available for local development by sourcing the setenv.sh script. There were attempts to move these to application-local.properties but failed. Please see the note in setenv.sh script.

API Hostnames
Environment	Hostname
dev	https://claims-dev.dev.bip.va.gov
test	https://claims-test.dev.bip.va.gov
int	https://claims-int.dev.bip.va.gov
ivv	https://claims-ivv.stage.bip.va.gov
uat	https://claims-uat.stage.bip.va.gov
pat	https://claims-pat.stage.bip.va.gov
pdt	https://claims-pdt.stage.bip.va.gov
demo	https://claims-demo.stage.bip.va.gov
perf	https://claims-perf.prod.bip.va.gov
preprod	https://claims-preprod.prod.bip.va.gov
prodtest	https://claims-prodtest.prod.bip.va.gov
cola	https://claims-cola.prod.bip.va.gov
prod	https://claims-prod.prod.bip.va.gov
API Calls

The following end points are used

GET /claims/{claimId}
GET /claims/{claimId}/contentions
PUT /claims/{claimId}/contentions
PUT /claims/{claimId}/lifecycle_status

The base URL is made available to the application with the environment variable BIP_CLAIM_URL through application.properties bip.claimBaseURL setting. The corresponding Kubernetes secret is bip.bipClaimUrl.

For local development and testing a Mock Server is available in docker compose with host name mock-bip-claims-api.

BIP API Service

All API calls are implemented in Bip API Service. Bip API Service uses the custom RestTemplate bean (qualifier: bipCERestTemplate) described in BIP-APIs.

Bip Api Service is available to rest of the application as a Spring service. The only current customer is Bip Claim Service which uses it through Bip Api Service Interface. This is mostly for historical progression of the implementation but also makes it possible to unit test Bip Claim Service more easily as Bip Api Service Interface is overridden in a test configuration.

BIP Claim Service

BIP Claims API related functionality is provided to the rest of the application through BIP Claim Service. The public methods hasAnchors, removeSpecialIssue, and markAsRFD should be recognizable from the VRO-v2-Roadmap. The public method completeProcessing is used to double check temporary station of jurisdiction as specified in requirements.

VBMS

VBMS (provided by VA's BIP (Benefits Integration Platform) team) has the following APIs:

Claim Service - specs for Release 23.1 (version 35.0, 08/23/2021) are in a 0003AE_5.2.2.d_Claims v4.0 SCD 23.1.pdf
update claims, contentions
dependent on BEP (Business Enterprise Platform), BGS (Benefits Gateway Service, where claims and contentions are persisted), and Oracle 11G DB
Claims API Swagger spec has more accurate info; code base
Claim Evidence API (see prior section)
BAM Shared Services

VRO will need to mark a claim as eligible for being assessed for fast-tracking (via a VBMS/BGS special issue?) so that users don't work on the claim.

We should investigate and ask if Lighthouse's Benefits Claim API can be updated to include the capability.

VRO will need to mark a claim or contention as ready-for-decision (RFD) so it can be adjudicated and fast-tracked.

MPI

When VRO checks for fast-track-ability, VRO queries an API (probably MPI) to map a given veteran identifier into an ICN, which is needed to query Lighthouse for health data.

Lighthouse has a connection to MPI, so VRO should leverage it
Benefits Claims API v2 provides a way get a veteran's ICN using MPI, but the veteran's SSN, name, and DOB is needed. Working with Kayla and Derek Brown (Lighthouse) to enable lookup by file number (a.k.a., BIRLS ID)

Note from a Slack post:

MPI queries straight from vets-api are mocked on localhost. This is necessary because MPI can only be accessed on the VA network, so our local machines don‚Äôt have direct access, even to the MPI test stacks."
BIP Claim Evidence API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/BIP-Claim-Evidence-API,"VRO uses BIP Claim Evidence API to upload evidence PDFs to E-Folder.

Access to BIP Claims API is available only within VA firewall.

BIP Claim Evidence API uses mTLS. VRO's mTLS implementation is detailed in BIP-APIs.

Integration Requirements
VRO uploads PDF documents to E-Folder
VRO specifies meta data about the document
Open API Specification

BIP Claim Evidence API's Open API Specification is available from the Swagger page.

The only end point used from BIP Claim Evidence API is the /files end-point.

Future Work

It should be possible to activate the Swagger page for the Mock BIP Claim Evidence API similar to Mock BIP Claims API easily. But that has not been yet done.

Code Walkthrough
Security Requirements

BIP requires a Bearer JWT for access. Following claims are used

Subject (sub)
User Id (userID): Custom - VRO system user
Issuer (iss)
Station Id (stationID): Custom - VRO system user facility (?)
Application Id (applicationID): Custom - must be equivalent to Issuer per documentation
Expiration (exp)
Issued At (iat)

The JWT is created before each API call in BipCEApiService createJwt method.

Subject claim is hard-coded in createJwt. Expiration and Issued At claims are dynamicaly created in createJwt. The other claims are made available to the application with environment variables through application.properties.

User Id: BIP_EVIDENCE_USERID through bip.evidenceClientId
Issuer: BIP_EVIDENCE_ISS through bip.evidenceIssuer
Station Id: BIP_STATION_ID through bip.stationId
Application Id: BIP_APPLICATION_ID through bip.applicationId

JWT is signed by a secret provided by the BIP API team. The secret is made available to the application with the environment variable BIP_EVIDENCE_SECRET through application.properties bip.evidenceSecret setting.

In the VRO Kubernetes environment the related Kubernetes secrets for the BIP environment variables are

BIP_EVIDENCE_USERID: bip.bipEvidenceUserId
BIP_STATION_ID: bip.bipStationId
BIP_APPLICATION_ID: bip.bipApplicationId
BIP_EVIDENCE_SECRET: bip.bipEvidenceSecret

A set of BIP environment variables are available for local development by sourcing the setenv.sh script. There were attempts to move these to application-local.properties but failed. Please see the note in setenv.sh script.

API hostnames
Environment	Hostname
ivv	https://vefs-claimevidence-ivv.stage.bip.va.gov
stage	https://vefs-claimevidence-pat.stage.bip.va.gov
pdt	https://vefs-claimevidence-pdt.stage.bip.va.gov
uat	https://vefs-claimevidence-uat.stage.bip.va.gov
prodtest	https://vefs-claimevidence-prodtest.prod.bip.va.gov
prod	https://vefs-claimevidence.prod.bip.va.gov
API Calls

The only end-point that is used is

POST /files

The base URL is made available to the application with the environment variable BIP_EVIDENCE_URL through application.properties bip.evidenceBaseURL setting. The corresponding Kubernetes secret is bip.bipEvidenceUrl.

For local development and testing a Mock Server is available in docker compose with host name mock-bip-ce-api.

BIP Claim Evidence API Service

The API call is implemented in Bip Claim Evidence API Service. Bip Claim Evidence API Service uses the custom RestTemplate bean (qualifier: bipCERestTemplate) described in BIP-APIs.

Bip Claim Evidence Api Service is available to rest of the application as a Spring service. The only current customer is Bip Claim Service which uses it through Bip Claim Evidence Api Service Interface. This is mostly for historical progression of the implementation but also makes it possible to unit test Bip Claim Service more easily as Bip Api Service Interface is overridden in a test configuration.

BIP Claim Service

BIP Claim Evidence API related functionality is provided to the rest of the application through BIP Claim Service. The public method uploadPdf is self explanatory."
BIE Kafka Client ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/BIE-Kafka-Client,"Intro

Status: Currently, this page is a Work-In-Progress Tech Spec. Once implementation is completed, update and transition this page to become documentation for the BIE Kafka Client microservice.

Epic and tickets: Consume BIP contention event streams #1642

Goal: Implement a BIE Kafka Client microservice that other VRO components can use to subscribe to and handle BIE's Event Stream

Diagram
Dotted arrows represent many events flowing, as a result of the topic subscription.
There may be one kafkaEvent-handler to handle multiple Kafka event exchanges; or one kafkaEvent-handlers per exchange.

When VRO is deployed to LHDI, VRO uses the respective BIE env (TBD in Establish connectivity between VRO and BIE kafka service environments #1674). When VRO is run locally or as part of integration tests, the mock-kafka-server will be used instead.

Verify connection to Kafka clusters using kcat (a.k.a kafkacat)
visit Lightkeeper-tool and setup lighthouse and kubectl cli tool and get the config file, set an alias for dev env alias kcdev=va-abd-rrd-dev
run kcdev apply -f sleep-pod.yaml to deploy a k8s pod to dev namespace using a manifest file like this one (run kcdev get pods you should see the pod from the list)
apiVersion: v1
kind: Pod
metadata:
  name: sleep-pod
  namespace: va-abd-rrd-dev
spec:
  containers:
    - name: sleep
      image: debian:latest
      command: [""/bin/sleep"", ""3650d""]
      resources:
        limits:
          cpu: ""1""
          memory: ""1Gi""

run kcdev exec -it sleep-pod -- /bin/sh to access the pod
run apt-get update && apt-get install kafkacat to install kcat, then exit the pod
copy CA, private key, certificate from here (in VA network) and kafka_config.txt local files into the pod tmp folder e.g. kcdev cp /test.vro.bip.va.gov.pem sleep-pod:/tmp

kafka_config.txt content

bootstrap.servers=kafka.dev.bip.va.gov:443
security.protocol=SSL
ssl.key.location=test.vro.bip.va.gov.pem
ssl.certificate.location=test.vro.bip.va.gov.crt.pem
ssl.ca.location=VACACerts.pem

access the pod again and cd to tmp folder and run kcat -F kafkacat_config.txt -L
can access topic message by running kcat -F kafkacat_config.txt -L TST_CONTENTION_BIE_CONTENTION_ASSOCIATED_TO_CLAIM_V02 -C
Create keystore and truststore files

Read the instruction and run the script

If done correctly, load the truststore file in Keystore Explorer, the setting should look like this 

Kafka Topic Prefixes

Kafka Cluster Environments

NOTE: PreProd environment urls have been updated to kafka.preprod.bip.va.gov:443 and https://schemaregistry.preprod.bip.va.gov:443

BIE Kafka Schema Docs
svc-bie-kafka
When this microservice starts up, it will immediately subscribe to Kafka topics based on a configuration settings.
Authenticate to mock BIE's Kafka service; only this microservice holds the credentials and is responsible for authenticating (microservice clients don't need to provide BIE credentials).
When Kafka-topic events come in, send a RabbitMQ message into a RabbitMQ exchange with a payload as follows:
# exchange: 'bie-events-contention-associated'
  (async, one-way) payload = {
    ""topic"": ""TST_CONTENTION_BIE_CONTENTION_ASSOCIATED_TO_CLAIM_V02"",
    ""notifiedAt"": ""2023-06-12T19:29 UTC+00:00"", // when `svc-bie-kafka` received the event
    ""event"": { ...entire Kafka event object }
  }

# exchange: 'bie-events-contention-deleted'
  (async, one-way) payload = {
    ""topic"": ""TST_CONTENTION_BIE_CONTENTION_DELETED_V02"",
    ""notifiedAt"": ""2023-06-12T19:30 UTC+00:00"",
    ""event"": { ...entire Kafka event object }
  }

The RabbitMQ exchange name will be apriori constants, so the microservice and its clients know which exchange to use for each Kafka topic.
For the RabbitMQ exchange, create a topic exchange (or fanout exchange if desired), which will allow multiple svc-bie-kafka clients to subscribe to the topic.
Using routing keys was considered but we decided to implement a more straightforward 1-to-1 mapping between Kafka topic to RabbitMQ exchange.
xample-workflow
Implement an example VRO component that subscribes to RabbitMQ exchange(s) upon startup.
If using Java, it's encouraged to add a new Camel route to domain-xample/xample-workflows/ that does event-handling when a msg is available in the RabbitMQ exchange.
When a Kafka event comes through the RabbitMQ topic exchange, log the event and save it to the DB.
Initially, structure the DB entity generically. Iteratively add more specificity when we know what event fields should be stored as DB columns.
-- table: 'bie_event' columns: 
- uuid: ""PK: Primary key used to reference this DB record""
- created_at: ""Creation time of this DB record""
- notified_at: ""When the event was received by VRO""
- event_type (indexed): ""Enum: contention-associated, contention-deleted, ...""
- event_details: ""JSON text with PII removed""
- event_at: ""Event timestamp extracted from event_details""
- benefit_claim_id (indexed): ""Benefit Claim ID extracted from event_details""
- contention_id (indexed): ""Contention ID extracted from event_details""
- classification (indexed): ""Classification of contention extracted from event_details""

-- table: 'diagnostic_code'
- code: ""PK: Diagnostic Codes extracted from event_details""

-- many-to-many association table: 'bie_event_to_diagnostic_code'
- bie_event_uuid: ""FK: Primary key used to reference this DB record""
- diagnostic_code: ""FK: Diagnostic Codes extracted from event_details""
# Follow guidance at https://stackoverflow.com/a/9790225

Timestamps without a time zone should be in GMT
Create an integration test that registers a Kafka topic and publishes events to the topic, then checks if expected DB entry exists.
Future Improvements

The following possible improvements are not currently part of the Consume BIP contention event streams #1642 epic unless there is sufficient reason to include one or more of them.

Resilience: Store Kafka subscription state in Redis for observability.
Correctness when scaling: When multiple svc-bie-kafka instances, ensure there are no duplicate notification events in the RabbitMQ exchange."
BIP APIs ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/BIP-APIs,"VRO uses two BIP APIs.

BIP Claims API
BIP Claim Evidence API

Information specific to one of the individual APIs can be found in the above pages. This page, in addition to providing these links, describes the details of VRO Mutual TLS implementation.

The secrets that are pertinent to these integrations can be found in the HashiCorp vault under `vro-secrets/deploy/{ENV_NAME}/VRO_SECRETS_BIP and include:

BIP_CLAIM_SECRET
BIP_CLAIM_USERID
BIP_EVIDENCE_SECRET
BIP_EVIDENCE_USERID
BIP_KEYSTORE_BASE64
BIP_PASSWORD
BIP_TRUSTSTORE_BASE64
Mutual TLS (mTLS)

BIP APIs require mTLS. It is necesssary for VRO to use a client certificate signed by a VA recognized Certificate Authority (CA) during https handshake. In addition VRO needs to use a VA recognized CA to validate BIP certificates during https handshake.

Certificates
Generated Certificates

Venabi is the VA's new Internal Management System which can be used to manage the PKI certificates. Within VRO Kubernetes cluster the process is further simplified and it is possible to generate the certificates within one of VRO Kubernetes namespaces. The client certificates that are being used currently by VRO has been generated using these instructions.

The certificates tls.key, tls.crt, and va.crt has been generated in va-abd-rrd-dev namespace and can be found in va-abd-rrd-dev-va-gov-tls secrets.

The private key tls.key is not encrypted so there are no passwords needed for steps described later in this document.

Details for tls.crt can be displayed using

openssl x509 -in tls.crt -text -noout

The certificate tls.crt expires in January 6th, 2024.

The certificate tls.crt contains both the client public key and an intermediate CA. Only the client public key is used and will be referred as tls_bip.crt.

The certificate va.crt is not currently used.

By default, certificates generated are valid for one year, and are set to auto-renew ‚Öîrds of the way through their lifecycle.

Production Use

A second set of certificates are generated in production. All the discussion in this document is identical for production certificates. The production certificate expires in March 17th, 2024.

Future Work

As discussed later in this document we do not yet know a way to use these certificates directly in the Java code and there are manual steps to make them usable. More discussions are needed with LHDI around the process on how to update them.

The certificates will be auto-renewed. What is the trigger to perform manual steps when the switch is made?
Should LHDI make these certificates available to containers without additional steps?
Can we forego container level https and move that complexity to API Gateway so that it is managed by LHDI?

At this time Java cert renewal is manual and steps in this document should be followed. We have time until January 6th, 2024 to make improvement.

Downloaded Certificates

During VRO implementation of mTLS we were not successful using the VA CA public keys in va.crt and tls.crt.

Instead we downloaded the public keys that were used by BIP APIs from Chrome on a GFE:

Open Chrome on your GFE
Open one of the end points in BIP Claims API or BIP Claim Evidence API. If you use a GET end point you should typically get a jwt missing error
Once you get the error click on the lock (View site information) in the url bar to bring up a pop-up window
Select ""Connection is secure"" menu item to bring up a second pop-up window. From this pop-up you can select ""Certificate is valid"" to bring up the ""Certificate viewer"".
Goto ""Details"" tab, select the certificate for root CA and ""Export"". Repeat for the intermediate CA

Two certificates downloaded are

VA-Internal-S2-RCA1-v1.crt (root)
VA-Internal-S2-ICA4.crt (intermediate)

These public keys are concatenated in a file named va_all.crt for later use.

Future Work

We have not retried to use the VA CA public keys in va.crt and tls.crt after we successfully used va_all.crt. This should be revisited in the future since it is possible that the steps we eventually ended up with could be succesful for public keys in va.crt and tls.crt as well.

Self-Signed Certificates

VRO generates a set of self-signed certificates using a script called build-certificates.sh. These certificates are used for local development and end-to-end tests to mock the actual certificates and mTLS based https handshake.

More details and what is being generated are documented in the script itself.

VRO mTLS Implementation
PKCS #12 Files

As of now we do not know a simple way to consume the certificates tls.key, tls_bip.crt, and va_all.crt directly in the Java code to utilize them in https calls. Java uses either JKS files or PKCS #12 files. Since PKCS #12 files are not Java specific, we prefer them over JKS files in VRO.

To generate the PKCS #12 file keystore.p12 for client certificates we use openssl

openssl pkcs12 -export -in tls_bip.crt -out keystore.p12 -name keystore -nodes -inkey tls.key

This commands asks for an ""export password"". This password is to be recorded as it needs to be later used as a secret.

To generate the PKCS #12 file truststore.p12 for VA CA certificates we use keytool

keytool -import -file va_all.crt -alias all_cas -keystore truststore.p12

This command asks for a ""keystore password"". For simplicity VRO uses the same the value of ""export password"".

Future Work

It should be possible either to use keystore or openssl exclusively here. But these PKCS #12 files worked and we did not try either route any further.

Kubernetes Secrets

VRO uses three environment variables to store the content of the [PKCS #12] files and the ""export password"".

BIP_KEYSTORE
BIP_TRUSTSTORE
BIP_PASSWORD

Since PKCS #12 files are binary we convert them to Base 64 using openssl

openssl base64 -in keystore.p12 -out keystore.b64
openssl base64 -in truststore.p12 -out truststore.b64

What is stored in the environment variables BIP_KEYSTORE and BIP_TRUSTSTORE are the content of the Base 64 files.

In our environments these environment variables are made available to the app container using the following secrets

bip.bipKeystore
bip.bipTruststore
bip.bipPassword
Manual Steps

In summary the manual steps to make the certificates available to the Java code are

Convert certificates to PKCS #12 files
Convert PKCS #12 files to Base 64 files
Store content of the Base 64 files in Kubernetes secrets
Restart the pod to make secret changes effective

If https handshake implementation remains in the Java code (as opposed to say moved to API Gateway) these manual steps need to be automated if certificate renewal is automated.

Java implementation and Code Walkthrough

Java uses Keystore objects to store certificate information in PKCS #12 files. VRO

Reads in the content of the PKCS #12 files and the password from the environment variables BIP_KEYSTORE, BIP_TRUSTSTORE, and BIP_PASSWORD through application.yml
Converts Base 64 content to binary content
Creates the keystore and trusstore objects as Keystore instances from the binary content and the password
Creates a custom RestTemplate bean that can be used to make the https requests

The properties in application.yml that corresponds to the environment variables are

keystore for BIP_KEYSTORE
truststore for BIP_TRUSTSTORE
truststore_password for BIP_PASSWORD

The RestTemplate bean that is used to make the https requests is implemented in BipApiConfig. This file also includes the generation of the Keystore objects from the application.yml properties. This RestTemplate bean is autowired in the rest of the code using the Qualifier bipCERestTemplate.

Curl Validation of Certificates

Validation of the certificates using the Java code has been problematic since the BIP APIs are not available outside of VA Firewall

Our GFE's typically do not have Java installed
Extra permissions are necessary to install and maintain Java on the GFE's
Additional security features in GFE's makes it difficult to make https calls from Java

curl has been an invaluable tool to test the validity of the certificates. In principle it should be possible to run curl in your GFE but running it from the app container in one of our environment was easier. We copied the certificates tls.key, tls_bip.crt, and va_all.crt to the container using cat <<EOF and copy&paste in the /tmp directory. You should be able to use kubectl cp with enough permissions. Once the files are in the container you can run

curl -X POST <https://......> --cacert va_all.crt --cert tls_bip.crt --key tls.key --verbose

To see the https handshake to verify the validity of the certificates. The request can be made to any end point of interest."
BIE contention event queues User Guide ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/BIE-contention-event-queues-User-Guide,"Receive Contention Event data
Subscribe to the following RabbitMQ Queues
bie-events-contention-associated-to-claim (Kafka topic CONTENTION_BIE_CONTENTION_ASSOCIATED_TO_CLAIM_V02)
bie-events-contention-updated (Kafka topic CONTENTION_BIE_CONTENTION_UPDATED_V02)
bie-events-contention-classified (Kafka topic CONTENTION_BIE_CONTENTION_CLASSIFIED_V02)
bie-events-contention-completed (Kafka topic CONTENTION_BIE_CONTENTION_COMPLETED_V02)
bie-events-contention-deleted (Kafka topic CONTENTION_BIE_CONTENTION_DELETED_V02)

Note: VRO Kafka Service Design Diagram

RabbitMQ Sample Message Payload
{
  ""status"": 200,
  ""eventType"": ""CONTENTION_UPDATED"",
  ""claimId"": 12345678,
  ""contentionId"": 12345678,
  ""contentionTypeCode"": ""NEW"",
  ""contentionClassificationName"": ""Musculoskeletal - Wrist"",
  ""notifiedAt"": 1694451084874,
  ""occurredAt"": 1694451062000
}

eventType - There are total of 5 different event types:
CONTENTION_ASSOCIATED_TO_CLAIM
CONTENTION_UPDATED
CONTENTION_CLASSIFIED
CONTENTION_COMPLETED
CONTENTION_DELETED
Local Development Setup

Make sure you have the latest version of abd-vro and abd-vro-dev-secrets cloned in sibling directories. Run these commands

export COMPOSE_PROFILES='kafka'
source scripts/setenv.sh

./gradlew docker

./gradlew :dockerComposeUp

./gradlew -p mocks docker
./gradlew -p mocks :dockerComposeUp

./gradlew :domain-xample:dockerComposeUp
./gradlew :app:dockerComposeUp

pushd svc-bie-kafka && { source ./docker-entryprep.sh; popd; }

./gradlew :svc-bie-kafka:integrationTest


NOTE: The BIE team does not provide any mechanism to test or send Kafka payloads in non-local environments. So, testing in these environments is very limited since we need to wait for Kafka payloads in order to see the data in RabbitMQ queues.

Sample code for connecting to RabbitMQ Queues

Python using hoppy

Javascript Sample Code"
BGS API ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/BGS-API,"BGS Web Services documentation

API spec: Benefits Gateway Services (BGS) Web Services v120.doc

Clyde cautions about going around VBMS to do thing directly with BGS:

The reason is that anything that just goes to BGS has to be sync‚Äôd with VBMS for functionality in things like NWQ and other places to work. If you can do it through VBMS it would be a clearer architecture and reduce the need to sync more things.

The manageContention service from BGS could be used for updating special issues."
API Gateway ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/API-Gateway,"To support examination and testing of APIs implemented for particular domains and in various programming languages, VRO offers an API Gateway that provides a Swagger UI for manual exploration of APIs offered by VRO.

Refer to PR #1591 and PR #1578 for details.

To expose APIs for each domain, an API Gateway offers a Swagger UI to inspect API offerings.

In a local (docker-compose) environment, it is available at http://localhost:8060/.
In the Dev (LHDI) environment, it is available at https://dev.lighthouse.va.gov/abd-vro/. It's also available in other LHDI Development Environments.

When the user selects a ""Destination"" in the Swagger UI, the API Gateway queries domain-specific containers to retrieve the OpenAPI spec and presents them in a Swagger UI.

Adding a new domain API for LHDI deployments and Swagger UI

Pre-requisite: a domain container serves up an OpenAPI spec on some exposed port in LHDI

First, update Helm configurations to route domain URLs to the desired domain container:

In the VirtualService, add a URL rewrite, route destination, and header X-Forwarded-Prefix configuration
The URL rewrite makes the URL compatible with the route destination in the domain container.
The X-Forwarded-Prefix header is needed to prepend the prefix to URLs generated by the domain container.
Deploy to LHDI and test to make sure the URL rewrite and X-Forwarded-Prefix works.
In particular, ensure that the URL to the domain's OpenAPI spec works.

Then, update Swagger UI of the API Gateway:

Add an entry with the URL to the domain's OpenAPI spec to api-gateways application.yml
Test the API Gateway locally (see section below) and update application-local.yml with URL rewrite configuration for the new domain container.
Deploy to LHDI and test VRO's Swagger UI. For the dev LHDI env, go to https://dev.lighthouse.va.gov/abd-vro.
Testing the API Gateway locally
Run the API Gateway: ENV=local ./gradlew  :api-gateway:bootRun
ENV=local causes api-gateway/src/main/resources/application-local.yml to be loaded. In this file, the spring.cloud.gateway.routes configurations cause URLs to be rewritten, similar to the VirtualService settings in the Helm configuration.
Alternatively, to use the api-gateway defined in docker-compose.yml, run docker compose up -d api-gateway.
Run the domain APIs:
# Start VRO's App API
./gradlew :app:dockerComposeUp
# This API's Swagger UI is available at http://localhost:8080/swagger

# Optionally start a domain API, such as Team CC's API -- see domain-cc/README.md

Browse to http://localhost:8060/, read instructions, and click Swagger UI
Note the definition ""0. Gateway API"" is selected and its basic API is shown. Expand the GET /hello endpoint and click ""Try it out"", then ""Execute"". The response should be a code 200 with body Hi!.
Select the definition ""App API"". To ""Try it out"", make sure to select the ""/vro-app"" server on the left dropdown. Selecting this server will cause requests to go to http://localhost:8060/vro-app/<endpoint>, which will get re-written as specified in application-local.yml (i.e., requests will be rerouted to the vro-app container at http://localhost:8110/<endpoint> by default).
Implementation Details
It queries URLs specified by springdoc.swagger-ui.urls. The same URLs are routed differently depending on the environment:
In the local environment, the URLs are rerouted using spring.cloud.gateway.routes
In the LHDI environments, the URLs are rerouted using Istio's VirtualService in Helm
To have the Swagger UI offer the correct URL prefix to access the API endpoints, the server URL environment variable is set in Helm configurations.
The source API can be accessed directly (without going through the API Gateway). For example, to access the VRO App API:
In the local environment, http://localhost:8110/swagger-ui/index.html and http://localhost:8110/v3/api-docs
In the Dev (LHDI) environments, https://dev.lighthouse.va.gov/vro-app/swagger-ui/index.html and https://dev.lighthouse.va.gov/vro-app/v3/api-docs (based on vro-app's serviceUriPrefix)
For this to work, the Spring's OpenAPI implementation requires the X-Forwarded-Prefix header to be set correctly.
The API Gateway serves up it's own API in api-gateway/src/main/java/gov/va/vro/ApiGatewayRestController.java. It can be extended to include domain-independent deployment info, health of containers, and stats.
Decision History

An API Gateway provides a single location to access all APIs provided by VRO, regardless of implementation language -- see ticket #1512. As a quick solution, it was implemented using Spring Cloud Gateway. It could be replaced with another API Gateway solution (e.g., Kong, which is used for other VA products) if the current implementation is insufficient or as new use cases arise."
Apache Camel ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki/Apache-Camel,"Apache Camel 3.11

VRO uses Apache Camel 3.11.0, which uses Spring Boot 2.5.3. Apache Camel 3.13 through 3.15 are not compatible with gov.va.starter's current Spring Boot version (2.5.2).

Camel-related code is limited to a very small portion of the codebase but is important for routing requests through desired workflows and to microservices.

Why Apache Camel?

Apache Camel provides a well-tested and stable implementation of Enterprise Integration Patterns (EIP) so that we can focus on VRO functionality and less on ""glue code"".

We use it to define and quickly update business workflows that connect VRO functionalities (a.k.a. microservices).
Camel provides many core components and non-core components to connect to message queues, databases, email services, AWS services, etc.
Using Camel encourages VRO functionalities to be loosely-coupled and single-responsibility with the intent of improving software agility.
CamelApp

To demonstrate how a Spring-based app uses Apache Camel to implement a microservice architecture, a sample CamelApp project is available for you to fork. Use it to:

familiarize yourself with Java, Gradle, Spring, and Camel tools
experiment with the code

For a description of specific uses in VRO, see Routing API requests.

Camel Concepts

Design concepts as they are applied to VRO: Camel Components, Camel Routes, and microservices

Most VRO features are implemented into Camel Components (think Lego bricks). These components are reusable and replicable, and should be idempotent and robust to failures. Camel provides many components (mostly to integrate with other tools), and we can implement our own to process claims, generate PDFs, assess health data, etc.

A processing workflow is implemented by connecting these components with Camel Routes. These routes can include actions that perform basic logic, data transformations, and filtering to map and send a payload (i.e., a JSON data object) for consumption by the next component.

When there is a client to interface with some external system (e.g., Lighthouse API), it should typically be implemented as a microservice that listens on and responds to a message queue (e.g., RabbitMQ) so that anything (e.g., Camel component or route) can use the client. These microservices should also be idempotent and robust to failures. In a Camel route, a microservice can be treated like a Camel component."
Home ¬∑ department-of-veterans-affairs/abd-vro Wiki ¬∑ GitHub,https://github.com/department-of-veterans-affairs/abd-vro/wiki,"Virtual Regional Office (VRO) Overview
VRO Architecture Diagram
VRO Team working docs
Culture and Norms
Team Processes
VRO Engineer Onboarding
Active efforts
BIE Kafka Client
BIE Contention Events User Guide
VA.gov Data Visibility Initiative
Problem Overview
Welcome VRO Developers!
Software Conventions
VRO RabbitMQ Strategy
LHDI's Boilerplate Instructions
Local Setup
Jetbrains SpringBoot Run Configuration Setup
Code structure
Routing API requests
Apache Camel defines processing workflows
Configuration settings
Domain Applications in VRO
Docker Compose
Docker containers
Development process
Gradle
Pull Requests guidelines
Change Management Plan
CI CD Workflows
Deploying VRO
Container Image Versions
API Gateway
External APIs to interact with other systems
BIP APIs
Lighthouse APIs
BGS API
BIE Kafka Event Stream
VRO Database
Testing
Testing using Swagger UI
Development environments
End to End Tests
Mock Services
Partner Teams
New Domain Setup
Partner Team Deploy Process
Welcome Contention Classification Developers!

Updating Contention Classification DC Lookup Table üìã

Welcome Employee Experience Developers!
Welcome DevOps!
Deploying VRO
Lightkeeper tool
Kubernetes clusters
Helm Charts
Deploy to Prod
Quick Deploy Instructions
Github Actions
Machine User Account
Tokens and Secrets
VRO Secrets
Secrets Vault
Maintenance
DataDog monitoring
Dependabot
Welcome Q/A!
VRO Test Cases
Deeper topics
Lighthouse DI Documentation repo - including diagrams
Dive into RabbitMQ/Microservice reliability
Support Model
Support Model ( Draft )"
